<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="深度学习之吴恩达课程作业4, 统计学 数据挖掘 机器学习 计算广告">
    <meta name="description" content="吴恩达深度学习课程作业L2W5
Improving Deep Neural Networks
加强深度网络性能的一些技巧
参数初始化技巧
正则化技巧，l2, dropout
梯度检验

HW参考
视频链接
作业链接

注意使用numpy的问">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>深度学习之吴恩达课程作业4 | Myhaa&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    
    <script src="/libs/jquery/jquery.min.js"></script>
    
<link rel="alternate" href="/atom.xml" title="Myhaa's Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Myhaa's Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Myhaa's Blog</div>
        <div class="logo-desc">
            
            要么孤独，要么庸俗
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/myhaa" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/myhaa" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        深度学习之吴恩达课程作业4
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                <span class="chip bg-color">神经网络</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-11-03
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    294
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    1 分
                </div>
                
				
                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="吴恩达深度学习课程作业L2W5"><a href="#吴恩达深度学习课程作业L2W5" class="headerlink" title="吴恩达深度学习课程作业L2W5"></a>吴恩达深度学习课程作业L2W5</h1><ul>
<li>Improving Deep Neural Networks</li>
<li>加强深度网络性能的一些技巧</li>
<li>参数初始化技巧</li>
<li>正则化技巧，l2, dropout</li>
<li>梯度检验</li>
</ul>
<h2 id="HW参考"><a href="#HW参考" class="headerlink" title="HW参考"></a>HW参考</h2><ol>
<li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li>
<li><a href="https://github.com/suqi/deeplearning_andrewng/tree/master/Course2-DL-tuning/week5" target="_blank" rel="noopener">作业链接</a></li>
</ol>
<h2 id="注意使用numpy的问题："><a href="#注意使用numpy的问题：" class="headerlink" title="注意使用numpy的问题："></a>注意使用numpy的问题：</h2><ol>
<li><a href="https://stackoverflow.com/questions/21610198/runtimewarning-divide-by-zero-encountered-in-log" target="_blank" rel="noopener">RuntimeWarning: divide by zero encountered in log</a></li>
<li><a href="https://stackoverflow.com/questions/27842884/numpy-invalid-value-encountered-in-true-divide" target="_blank" rel="noopener">numpy: Invalid value encountered in true_divide</a></li>
</ol>
<h2 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
import sklearn
import sklearn.datasets
import copy

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0)
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="神经网络结构封装"><a href="#神经网络结构封装" class="headerlink" title="神经网络结构封装"></a>神经网络结构封装</h2><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_sigmoid(Z):
    """
    Implements the sigmoid activation in numpy

    Arguments:
    Z -- numpy array of any shape

    Returns:
    A -- output of sigmoid(z), same shape as Z
    cache -- returns Z as well, useful during backpropagation
    """

    A = 1/(1+np.exp(-Z))
    cache = Z

    return A, cache

def func_relu(Z):
    """
    Implement the RELU function.

    Arguments:
    Z -- Output of the linear layer, of any shape

    Returns:
    A -- Post-activation parameter, of the same shape as Z
    cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently
    """

    A = np.maximum(0,Z)

    assert(A.shape == Z.shape)

    cache = Z 
    return A, cache


def func_relu_backward(dA, cache):
    """
    Implement the backward propagation for a single RELU unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    Z = cache
    dZ = np.array(dA, copy=True) # just converting dz to a correct object.

    # When z <= 0, you should set dz to 0 as well. 
    dZ[Z <= 0] = 0

    assert (dZ.shape == Z.shape)

    return dZ

def func_sigmoid_backward(dA, cache):
    """
    Implement the backward propagation for a single SIGMOID unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    Z = cache

    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)

    assert (dZ.shape == Z.shape)

    return dZ
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layers_initialize_parameters(layer_dims, seed=1):
    """
    L层神经网络参数初始化
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param seed: 随机种子
    :return: 
        parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
        Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
        bl -- bias vector of shape (layer_dims[l], 1)
    """
    np.random.seed(seed)
    parameters = {}
    L = len(layer_dims)

    # 这里/ np.sqrt(layer_dims[layer-1])很重要，如果还是*0.01，会导致模型cost降不下去
    for layer in range(1, L):
        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1]) / np.sqrt(layer_dims[layer-1]) # * 0.01
        parameters['b' + str(layer)] = np.zeros((layer_dims[layer], 1))

        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))
        assert(parameters['b' + str(layer)].shape == (layer_dims[layer], 1))
    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_forward(A, W, b):
    """
    linear forward
    :param A:
    :param W:
    :param b:
    :return Z,chche:
        Z -- the input of the activation function, also called pre-activation parameter 
        cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """
    Z = np.dot(W, A) + b

    assert(Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b)
    return Z, cache

def func_linear_activation_forward(A_prev, W, b, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)
    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)
    :param b: bias vector, numpy array of shape (size of the current layer, 1)
    :param activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
    :return:
        A -- the output of the activation function, also called the post-activation value 
        cache -- a python dictionary containing "linear_cache" and "activation_cache";
                 stored for computing the backward pass efficiently
    """
    Z, linear_cache = func_linear_forward(A_prev, W, b)
    if activation == 'sigmoid':
        A, activation_cache = func_sigmoid(Z)
    elif activation == 'relu':
        A, activation_cache = func_relu(Z)
    else:
        raise ValueError('activation param')

    assert(A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache)
    return A, cache

def func_L_model_forward(X, parameters):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation

    :param X -- data, numpy array of shape (input size, number of examples)
    :param parameters -- output of initialize_parameters_deep()

    :return:
        AL -- last post-activation value
        caches -- list of caches containing:
                    every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                    the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """
    caches = []
    A = X
    L = len(parameters) // 2

    for layer in range(1, L):
        A_prev = A
        W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]
        A, cache = func_linear_activation_forward(A_prev, W, b, 'relu')
        caches.append(cache)

    A_prev = A
    layer = L
    W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]
    A, cache = func_linear_activation_forward(A_prev, W, b, 'sigmoid')
    caches.append(cache)

    assert(A.shape == (1, X.shape[1]))
    return A, caches
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="compute-cost"><a href="#compute-cost" class="headerlink" title="compute cost"></a>compute cost</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_compute_cost(AL, Y):
    """
    Implement the cost function defined by equation (7).

    :param AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    :param Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    :return:
        cost -- cross-entropy cost
    """
    m = Y.shape[1]

    # todo RuntimeWarning: divide by zero encountered in log
    AL = np.clip(AL, 1e-10, 1-1e-10)

#     Compute loss from aL and y.

#     logprobs = np.multiply(-np.log(AL), Y) + np.multiply(-np.log(1 - AL), 1 - Y)
#     cost = 1./m * np.nansum(logprobs)

#     cost = -1 / m * np.sum(np.multiply(Y, np.log(AL))+np.multiply(1-Y, np.log(1-AL)))
    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))

    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).

    assert(cost.shape == ())
    return cost
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_backward(dZ, cache):
    """
    Implement the linear portion of backward propagation for a single layer (layer l)

    :param dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    :param cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1. / m * np.dot(dZ, A_prev.T)
    db = 1. / m * np.sum(dZ, axis=1, keepdims=True)  # axis=1是行记录求和
    dA_prev = np.dot(W.T, dZ)

    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)
    assert(dA_prev.shape == A_prev.shape)
    return dA_prev, dW, db

def func_linear_activation_backward(dA, cache, activation):
    """
    Implement the backward propagation for the LINEAR->ACTIVATION layer.

    :param dA -- post-activation gradient for current layer l 
    :param cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    :param activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    :returns
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache = cache

    if activation == 'relu':
        dZ = func_relu_backward(dA, activation_cache)
    elif activation == 'sigmoid':
        dZ = func_sigmoid_backward(dA, activation_cache)
    else:
        raise ValueError('activation param')
    dA_prev, dW, db = func_linear_backward(dZ, linear_cache)
    return dA_prev, dW, db

def func_L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group

    :param AL -- probability vector, output of the forward propagation (L_model_forward())
    :param Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
    :param caches -- list of caches containing:
                    every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                    the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])

    :return:
        grads -- A dictionary with the gradients
                 grads["dA" + str(l)] = ... 
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ... 
    """
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)

    # todo divide by zero encountered in true_divide
    dAL = - (np.divide(Y, AL, where=AL!=0) - np.divide(1-Y, 1-AL, where=(1-AL)!=0))
#     dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))

    cur_cache = caches[L-1]
    grads['dA' + str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = func_linear_activation_backward(dAL, cur_cache, activation='sigmoid')

    for layer in reversed(range(L-1)):
        cur_cache = caches[layer]
        dA_prev_tmp, dW_tmp, db_tmp = func_linear_activation_backward(grads["dA"+str(layer+1)], cur_cache, activation='relu')
        grads['dA'+str(layer)] = dA_prev_tmp
        grads['dW'+str(layer+1)] = dW_tmp
        grads['db'+str(layer+1)] = db_tmp

#     current_cache = caches[L-1]
#     grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = func_linear_activation_backward(dAL, current_cache, activation = "sigmoid")

#     for l in reversed(range(L-1)):
#         # lth layer: (RELU -> LINEAR) gradients.
#         current_cache = caches[l]
#         dA_prev_temp, dW_temp, db_temp = func_linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation = "relu")
#         grads["dA" + str(l + 1)] = dA_prev_temp
#         grads["dW" + str(l + 1)] = dW_temp
#         grads["db" + str(l + 1)] = db_temp

    return grads
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="update-parameters"><a href="#update-parameters" class="headerlink" title="update parameters"></a>update parameters</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_update_parameters(parameters, grads, lr):
    """
    Update parameters using gradient descent

    :param parameters -- python dictionary containing your parameters 
    :param grads -- python dictionary containing your gradients, output of L_model_backward
    :param lr: learning rate
    :return:
        parameters -- python dictionary containing your updated parameters 
                      parameters["W" + str(l)] = ... 
                      parameters["b" + str(l)] = ...
    """
    L = len(parameters) // 2

    for layer in range(1, L+1):
        parameters['W'+str(layer)] = parameters['W'+str(layer)] - lr * grads['dW' + str(layer)]
        parameters['b'+str(layer)] = parameters['b'+str(layer)] - lr * grads['db' + str(layer)]

    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_predict(X, y, parameters):
    """
    This function is used to predict the results of a  L-layer neural network.

    :param X -- data set of examples you would like to label
    :param parameters -- parameters of the trained model
    :return:
        p -- predictions for the given dataset X
    """
    m = X.shape[1]
    L = len(parameters) // 2
    p = np.zeros((1, m))

    probas, caches = func_L_model_forward(X, parameters)

    for i in range(probas.shape[1]):
        if probas[0, i] > 0.5:
            p[0, i] = 1
        else:
            p[0, i] = 0

    print('acc: {}'.format(np.sum(p==y)/m))
    return p
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="dnn-model"><a href="#dnn-model" class="headerlink" title="dnn model"></a>dnn model</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layer_dnn_model(X, Y, layer_dims, lr=0.001, num_epochs=10000, print_cost=False):
    """
    多层神经网络模型
    :param X:
    :param Y:
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param lr: learning rate
    :param num_epochs:
    :param print_cost:
    :return params
    """
    np.random.seed(1)
    costs = []
#     m = X.shape[1]

    # 参数初始化
    parameters = func_L_layers_initialize_parameters(layer_dims)

    # loop
    for epoch in range(num_epochs):
        # 前向传播
        AL, caches = func_L_model_forward(X, parameters)

        # 计算损失
        cost = func_compute_cost(AL, Y)

        # 后向传播
        grads = func_L_model_backward(AL, Y, caches)

        # 更新参数
        parameters = func_update_parameters(parameters, grads, lr)

        # 打印信息
        if print_cost and epoch % 100 == 0:
            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))
        if epoch % 100 == 0:
            costs.append(cost)

    return parameters, costs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="其他通用函数"><a href="#其他通用函数" class="headerlink" title="其他通用函数"></a>其他通用函数</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">def load_cat_dataset():
    train_dataset = h5py.File('./深度学习之吴恩达课程作业1/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('./深度学习之吴恩达课程作业1/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

    train_set_x = train_set_x_orig/255
    test_set_x = test_set_x_orig/255

    return train_set_x, train_set_y, test_set_x, test_set_y, classes
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def load_dataset():
    np.random.seed(1)
    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)
    np.random.seed(2)
    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)
    # Visualize the data
    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);
    train_X = train_X.T
    train_Y = train_Y.reshape((1, train_Y.shape[0]))
    test_X = test_X.T
    test_Y = test_Y.reshape((1, test_Y.shape[0]))
    return train_X, train_Y, test_X, test_Y
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def load_planar_dataset(seed):

    np.random.seed(seed)

    m = 400 # number of examples
    N = int(m/2) # number of points per class
    D = 2 # dimensionality
    X = np.zeros((m,D)) # data matrix where each row is a single example
    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)
    a = 4 # maximum ray of the flower

    for j in range(2):
        ix = range(N*j,N*(j+1))
        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta
        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius
        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
        Y[ix] = j

    X = X.T
    Y = Y.T

    return X, Y
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def load_planar_dataset(randomness, seed):

    np.random.seed(seed)

    m = 50
    N = int(m/2) # number of points per class
    D = 2 # dimensionality
    X = np.zeros((m,D)) # data matrix where each row is a single example
    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)
    a = 2 # maximum ray of the flower

    for j in range(2):

        ix = range(N*j,N*(j+1))
        if j == 0:
            t = np.linspace(j, 4*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta
            r = 0.3*np.square(t) + np.random.randn(N)*randomness # radius
        if j == 1:
            t = np.linspace(j, 2*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta
            r = 0.2*np.square(t) + np.random.randn(N)*randomness # radius

        X[ix] = np.c_[r*np.cos(t), r*np.sin(t)]
        Y[ix] = j

    X = X.T
    Y = Y.T

    return X, Y
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_predict_dec(parameters, X):
    """
    Used for plotting decision boundary.

    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (m, K)

    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    """

    # Predict using forward propagation and a classification threshold of 0.5
    AL, cache = func_L_model_forward(X, parameters)
    predictions = (AL>0.5)
    return predictions
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_plot_decision_boundary(model, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)
    plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="通过参数初始化技巧提升dnn"><a href="#通过参数初始化技巧提升dnn" class="headerlink" title="通过参数初始化技巧提升dnn"></a>通过<code>参数初始化</code>技巧提升dnn</h2><ul>
<li>一个好的参数初始化选择能够：<ol>
<li>Speed up the convergence of gradient descent</li>
<li>Increase the odds of gradient descent converging to a lower training (and generalization) error</li>
</ol>
</li>
</ul>
<h3 id="1-zero-initialization"><a href="#1-zero-initialization" class="headerlink" title="1-zero initialization"></a>1-zero initialization</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_initialize_parameters_zeros(layer_dims):
    """
    zero initialization
    :param layer_dims -- python array (list) containing the size of each layer.

    :return:
        parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
            W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
            b1 -- bias vector of shape (layers_dims[1], 1)
            ...
            WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
            bL -- bias vector of shape (layers_dims[L], 1)
    """
    parameters = {}
    L = len(layer_dims)

    for layer in range(1, L):
        parameters['W'+str(layer)] = np.zeros((layer_dims[layer], layer_dims[layer-1]))
        parameters['b'+str(layer)] = np.zeros((layer_dims[layer], 1))
    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_initialize_parameters_zeros([3,2,1])
print(parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[0., 0., 0.],
       [0., 0., 0.]]), &#39;b1&#39;: array([[0.],
       [0.]]), &#39;W2&#39;: array([[0., 0.]]), &#39;b2&#39;: array([[0.]])}
</code></pre><h3 id="2-random-initialization"><a href="#2-random-initialization" class="headerlink" title="2-random initialization"></a>2-random initialization</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_initialize_parameters_random(layer_dims):
    """
    random initialization
    :param layer_dims -- python array (list) containing the size of each layer.

    :return:
        parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
            W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
            b1 -- bias vector of shape (layers_dims[1], 1)
            ...
            WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
            bL -- bias vector of shape (layers_dims[L], 1)
    """
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)

    for layer in range(1, L):
        parameters['W'+str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1]) * 10
        parameters['b'+str(layer)] = np.zeros((layer_dims[layer], 1))
    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_initialize_parameters_random([3,2,1])
print(parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[ 17.88628473,   4.36509851,   0.96497468],
       [-18.63492703,  -2.77388203,  -3.54758979]]), &#39;b1&#39;: array([[0.],
       [0.]]), &#39;W2&#39;: array([[-0.82741481, -6.27000677]]), &#39;b2&#39;: array([[0.]])}
</code></pre><h3 id="3-He-initialization"><a href="#3-He-initialization" class="headerlink" title="3-He initialization"></a>3-He initialization</h3><p>This function is similar to the previous <code>initialize_parameters_random(...)</code>. The only difference is that instead of multiplying <code>np.random.randn(..,..)</code> by 10, you will multiply it by $\sqrt{\frac{2}{\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. </p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_initialize_parameters_he(layer_dims):
    """
    he initialization
    :param layer_dims -- python array (list) containing the size of each layer.

    :return:
        parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
            W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
            b1 -- bias vector of shape (layers_dims[1], 1)
            ...
            WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
            bL -- bias vector of shape (layers_dims[L], 1)
    """
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)

    for layer in range(1, L):
        parameters['W'+str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1]) * np.sqrt(2/layer_dims[layer-1])
        parameters['b'+str(layer)] = np.zeros((layer_dims[layer], 1))
    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_initialize_parameters_he([2,4,1])
print(parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[ 1.78862847,  0.43650985],
       [ 0.09649747, -1.8634927 ],
       [-0.2773882 , -0.35475898],
       [-0.08274148, -0.62700068]]), &#39;b1&#39;: array([[0.],
       [0.],
       [0.],
       [0.]]), &#39;W2&#39;: array([[-0.03098412, -0.33744411, -0.92904268,  0.62552248]]), &#39;b2&#39;: array([[0.]])}
</code></pre><h3 id="overwrite-dnn-model"><a href="#overwrite-dnn-model" class="headerlink" title="overwrite dnn model"></a>overwrite dnn model</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layer_dnn_model(X, Y, layer_dims, lr=0.01, num_epochs=15000, print_cost=True, initialization=None):
    """
    多层神经网络模型
    :param X:
    :param Y:
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param lr: learning rate
    :param num_epochs:
    :param print_cost:
    :param initialization : flag to choose which initialization to use ("zeros","random" or "he")

    :return params
    """
#     np.random.seed(1)
    costs = []
#     m = X.shape[1]

    # 参数初始化
    if initialization == 'zeros':
        parameters = func_initialize_parameters_zeros(layer_dims)
    elif initialization == 'random':
        parameters = func_initialize_parameters_random(layer_dims)
    elif initialization == 'he':
        parameters = func_initialize_parameters_he(layer_dims)
    else:
        parameters = func_L_layers_initialize_parameters(layer_dims)
#         raise ValueError('error initialization set!')
    print('parameters: \n', parameters)

    # loop
    for epoch in range(num_epochs):
        # 前向传播
        AL, caches = func_L_model_forward(X, parameters)
        # 计算损失
        cost = func_compute_cost(AL, Y)

        # 后向传播
        grads = func_L_model_backward(AL, Y, caches)

        # 更新参数
        parameters = func_update_parameters(parameters, grads, lr)

        # 打印信息
        if print_cost and epoch % 1000 == 0:
            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))
#             print('AL:', AL)
        if epoch % 1000 == 0:
            costs.append(cost)


    return parameters, costs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="在模拟数据上训练"><a href="#在模拟数据上训练" class="headerlink" title="在模拟数据上训练"></a>在模拟数据上训练</h3><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_X, train_Y, test_X, test_Y = load_dataset()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_39_0.png" alt="png"></p>
<p>​    <img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_39_0.png" alt="output_39_0"></p>
<h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">layer_dims = [train_X.shape[0], 10, 5, 1]
print(layer_dims)

lr = 0.01
num_epochs = 15000
print_cost=True
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>[2, 10, 5, 1]
</code></pre><h4 id="正常初始化"><a href="#正常初始化" class="headerlink" title="正常初始化"></a>正常初始化</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost)
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>parameters: 
 {&#39;W1&#39;: array([[ 1.14858562, -0.43257711],
       [-0.37347383, -0.75870339],
       [ 0.6119356 , -1.62743362],
       [ 1.23376823, -0.53825456],
       [ 0.22559471, -0.17633148],
       [ 1.03386644, -1.45673947],
       [-0.22798339, -0.27156744],
       [ 0.80169606, -0.77774057],
       [-0.12192515, -0.62073964],
       [ 0.02984963,  0.41211259]]), &#39;b1&#39;: array([[0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.]]), &#39;W2&#39;: array([[-0.34804634,  0.36199342,  0.28510802,  0.15890266,  0.28487566,
        -0.21621373, -0.0388613 , -0.29591628, -0.08471365,  0.16771312],
       [-0.21872233, -0.12546448, -0.21730309, -0.26727749, -0.21226666,
        -0.0040049 , -0.35332456,  0.07412875,  0.52487553,  0.23465497],
       [-0.06066373, -0.28069292, -0.2362722 ,  0.53520114,  0.01606682,
        -0.20143571,  0.06037278,  0.66415899,  0.0379976 ,  0.19517676],
       [ 0.09492219, -0.11139118, -0.36129598, -0.11047187, -0.06605816,
         0.18550654,  0.26530985,  0.29444033,  0.09031064,  0.27990621],
       [-0.23856158,  0.3961917 ,  0.16220265, -0.09426523,  0.154483  ,
        -0.02389787,  0.35785263,  0.48060828,  0.69113963, -0.44161092]]), &#39;b2&#39;: array([[0.],
       [0.],
       [0.],
       [0.],
       [0.]]), &#39;W3&#39;: array([[-0.64582733, -0.22560399,  0.07157075,  0.39183465,  0.14115624]]), &#39;b3&#39;: array([[0.]])}
cost after epoch 0: 0.6960659020466242
cost after epoch 1000: 0.6892005439078841
cost after epoch 2000: 0.6850049132576146
cost after epoch 3000: 0.6798306324866793
cost after epoch 4000: 0.6729361200642482
cost after epoch 5000: 0.6578134850604765
cost after epoch 6000: 0.6308733720773914
cost after epoch 7000: 0.5847718750117528
cost after epoch 8000: 0.512240914977723
cost after epoch 9000: 0.41737297954575764
cost after epoch 10000: 0.32777046260752574
cost after epoch 11000: 0.25687394311562106
cost after epoch 12000: 0.20428449077110009
cost after epoch 13000: 0.16602627481234417
cost after epoch 14000: 0.13073861742391021
on the train set:
acc: 0.99
on the test set:
acc: 0.94
</code></pre><p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_43_1.png" alt="output_43_1"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.title("Model with np.sqrt(layer_dims[layer-1]) initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_44_0.png" alt="output_44_0"></p>
<h4 id="zero-初始化"><a href="#zero-初始化" class="headerlink" title="zero 初始化"></a>zero 初始化</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost, initialization='zeros')
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>parameters: 
 {&#39;W1&#39;: array([[0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.]]), &#39;b1&#39;: array([[0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.]]), &#39;W2&#39;: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), &#39;b2&#39;: array([[0.],
       [0.],
       [0.],
       [0.],
       [0.]]), &#39;W3&#39;: array([[0., 0., 0., 0., 0.]]), &#39;b3&#39;: array([[0.]])}
cost after epoch 0: 0.6931471805599453
cost after epoch 1000: 0.6931471805599453
cost after epoch 2000: 0.6931471805599453
cost after epoch 3000: 0.6931471805599453
cost after epoch 4000: 0.6931471805599453
cost after epoch 5000: 0.6931471805599453
cost after epoch 6000: 0.6931471805599453
cost after epoch 7000: 0.6931471805599453
cost after epoch 8000: 0.6931471805599453
cost after epoch 9000: 0.6931471805599453
cost after epoch 10000: 0.6931471805599453
cost after epoch 11000: 0.6931471805599453
cost after epoch 12000: 0.6931471805599453
cost after epoch 13000: 0.6931471805599453
cost after epoch 14000: 0.6931471805599453
on the train set:
acc: 0.5
on the test set:
acc: 0.5
</code></pre><p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_46_1.png" alt="output_46_1"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.title("Model with Zeros initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_47_0.png" alt="output_47_0"></p>
<h4 id="random-初始化"><a href="#random-初始化" class="headerlink" title="random 初始化"></a>random 初始化</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost, initialization='random')
# print(parameters)
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>parameters: 
 {&#39;W1&#39;: array([[ 17.88628473,   4.36509851],
       [  0.96497468, -18.63492703],
       [ -2.77388203,  -3.54758979],
       [ -0.82741481,  -6.27000677],
       [ -0.43818169,  -4.7721803 ],
       [-13.13864753,   8.8462238 ],
       [  8.81318042,  17.09573064],
       [  0.50033642,  -4.04677415],
       [ -5.45359948, -15.46477316],
       [  9.82367434, -11.0106763 ]]), &#39;b1&#39;: array([[0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.]]), &#39;W2&#39;: array([[-11.85046527,  -2.05649899,  14.86148355,   2.36716267,
        -10.2378514 ,  -7.129932  ,   6.25244966,  -1.60513363,
         -7.6883635 ,  -2.30030722],
       [  7.45056266,  19.76110783, -12.44123329,  -6.26416911,
         -8.03766095, -24.19083173,  -9.23792022, -10.23875761,
         11.23977959,  -1.31914233],
       [-16.23285446,   6.46675452,  -3.56270759, -17.43141037,
         -5.96649642,  -5.8859438 ,  -8.73882298,   0.29713815,
        -22.48257768,  -2.67761865],
       [ 10.13183442,   8.52797841,  11.081875  ,  11.19390655,
         14.87543132, -11.18300684,   8.45833407, -18.60889529,
         -6.02885104, -19.14472043],
       [ 10.48147512,  13.33737819,  -1.97414679,  17.74645031,
         -6.7472751 ,   1.50616865,   1.52945703, -10.64195274,
          4.37946611,  19.3897846 ]]), &#39;b2&#39;: array([[0.],
       [0.],
       [0.],
       [0.],
       [0.]]), &#39;W3&#39;: array([[-10.24930875,   8.99338446,  -1.54506852,  17.69627303,
          4.83788348]]), &#39;b3&#39;: array([[0.]])}
cost after epoch 0: 11.512925423650048
cost after epoch 1000: 11.512925423650048
cost after epoch 2000: 11.512925423650048
cost after epoch 3000: 11.512925423650048
cost after epoch 4000: 11.512925423650048
cost after epoch 5000: 11.512925423650048
cost after epoch 6000: 11.512925423650048
cost after epoch 7000: 11.512925423650048
cost after epoch 8000: 11.512925423650048
cost after epoch 9000: 11.512925423650048
cost after epoch 10000: 11.512925423650048
cost after epoch 11000: 11.512925423650048
cost after epoch 12000: 11.512925423650048
cost after epoch 13000: 11.512925423650048
cost after epoch 14000: 11.512925423650048
on the train set:
acc: 0.5
on the test set:
acc: 0.5
</code></pre><p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_49_1.png" alt="output_49_1"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.title("Model with random initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_50_0.png" alt="output_50_0"></p>
<font color="red" size="5">注意：</font>
* 这里训练不起来是因为参数随机初始化时，参数太大，导致wx+b很大或很少，在sigmoid计算时，处于梯度为0的情况下，导致参数没法更新，即梯度消失情况

#### he初始化


```python
parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost, initialization='he')
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
```

    parameters: 
     {'W1': array([[ 1.78862847,  0.43650985],
           [ 0.09649747, -1.8634927 ],
           [-0.2773882 , -0.35475898],
           [-0.08274148, -0.62700068],
           [-0.04381817, -0.47721803],
           [-1.31386475,  0.88462238],
           [ 0.88131804,  1.70957306],
           [ 0.05003364, -0.40467741],
           [-0.54535995, -1.54647732],
           [ 0.98236743, -1.10106763]]), 'b1': array([[0.],
           [0.],
           [0.],
           [0.],
           [0.],
           [0.],
           [0.],
           [0.],
           [0.],
           [0.]]), 'W2': array([[-0.52996892, -0.09196943,  0.66462575,  0.10586273, -0.45785063,
            -0.31886025,  0.27961805, -0.07178376, -0.34383407, -0.10287287],
           [ 0.33319929,  0.88374361, -0.55638887, -0.28014216, -0.35945513,
            -1.08184688, -0.41313235, -0.45789116,  0.50265822, -0.05899384],
           [-0.72595532,  0.28920205, -0.15932913, -0.77955637, -0.26682983,
            -0.26322741, -0.39081204,  0.01328842, -1.00545144, -0.11974675],
           [ 0.45310941,  0.38138279,  0.49559652,  0.50060672,  0.66524951,
            -0.50011927,  0.3782682 , -0.8322151 , -0.26961842, -0.85617793],
           [ 0.46874582,  0.59646569, -0.08828653,  0.79364539, -0.30174732,
             0.06735791,  0.0683994 , -0.47592259,  0.19585568,  0.86713753]]), 'b2': array([[0.],
           [0.],
           [0.],
           [0.],
           [0.]]), 'W3': array([[-0.6482232 ,  0.56879158, -0.09771871,  1.11921058,  0.30597462]]), 'b3': array([[0.]])}
    cost after epoch 0: 0.8830537463419762
    cost after epoch 1000: 0.6879825919728063
    cost after epoch 2000: 0.6751286264523371
    cost after epoch 3000: 0.6526117768893805
    cost after epoch 4000: 0.6082958970572938
    cost after epoch 5000: 0.5304944491717493
    cost after epoch 6000: 0.41386458170717944
    cost after epoch 7000: 0.3117803464844441
    cost after epoch 8000: 0.23696215330322565
    cost after epoch 9000: 0.18597287209206842
    cost after epoch 10000: 0.15015556280371814
    cost after epoch 11000: 0.12325079292273551
    cost after epoch 12000: 0.09917746546525927
    cost after epoch 13000: 0.0845705595402428
    cost after epoch 14000: 0.07357895962677369
    on the train set:
    acc: 0.9933333333333333
    on the test set:
    acc: 0.96




![output_53_1](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_53_1.png)




```python
plt.title("Model with he initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
```


![output_54_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_54_0.png)



### <font color="red" size="5">总结</font>

<ol>
<li>不同的参数初始化导致不同的结果</li>
<li>随机参数初始化是为了打破对称性以及确保各隐藏units可以学习不同的特征</li>
<li>不要初始化太大的值，会导致梯度消失或者爆炸</li>
<li>对于relu激活函数来说，he初始化方式能表现很好</li>
</ol>
<h2 id="通过正则化技巧提升dnn"><a href="#通过正则化技巧提升dnn" class="headerlink" title="通过正则化技巧提升dnn"></a>通过<code>正则化</code>技巧提升dnn</h2><p>Deep Learning models have so much flexibility and capacity that <strong>overfitting can be a serious problem</strong>, if the training dataset is not big enough. Sure it does well on the training set, but the learned network <strong>doesn’t generalize to new examples</strong> that it has never seen!</p>
<h3 id="1-L2-Regularization"><a href="#1-L2-Regularization" class="headerlink" title="1-L2 Regularization"></a>1-L2 Regularization</h3><p>The standard way to avoid overfitting is called <strong>L2 regularization</strong>. It consists of appropriately modifying your cost function, from:</p>
<script type="math/tex; mode=display">J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}</script><p>To:</p>
<script type="math/tex; mode=display">J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}</script><h4 id="1-1-overwrite-compute-cost"><a href="#1-1-overwrite-compute-cost" class="headerlink" title="1.1 overwrite compute cost"></a>1.1 overwrite compute cost</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_compute_cost_with_regularization(AL, Y, parameters, lambd):
    """
    Implement the cost function with L2 regularization. See formula (2) above.

    :param AL -- post-activation, output of forward propagation, of shape (output size, number of examples)
    :param Y -- "true" labels vector, of shape (output size, number of examples)
    :param parameters -- python dictionary containing parameters of the model
    :param lambd -- 正则系数
    :return:
        cost - value of the regularized loss function (formula (2))
    """
    cross_entropy_cost = func_compute_cost(AL, Y)

    m = Y.shape[1]
    L = len(parameters) // 2
    tmp = [parameters['W'+str(layer)] for layer in range(1, L+1)]

    L2_regularization_cost = sum([np.sum(np.square(w)) for w in tmp]) * lambd / 2 / m

    cost = cross_entropy_cost + L2_regularization_cost
    return cost
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_compute_cost_with_regularization_test_case():
    np.random.seed(1)
    Y_assess = np.array([[1, 1, 0, 1, 0]])
    W1 = np.random.randn(2, 3)
    b1 = np.random.randn(2, 1)
    W2 = np.random.randn(3, 2)
    b2 = np.random.randn(3, 1)
    W3 = np.random.randn(1, 3)
    b3 = np.random.randn(1, 1)
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}
    a3 = np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]])
    return a3, Y_assess, parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A3, Y_assess, parameters = func_compute_cost_with_regularization_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A3, Y_assess, parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(array([[0.40682402, 0.01629284, 0.16722898, 0.10118111, 0.40682402]]),
 array([[1, 1, 0, 1, 0]]),
 {&#39;W1&#39;: array([[ 1.62434536, -0.61175641, -0.52817175],
         [-1.07296862,  0.86540763, -2.3015387 ]]),
  &#39;b1&#39;: array([[ 1.74481176],
         [-0.7612069 ]]),
  &#39;W2&#39;: array([[ 0.3190391 , -0.24937038],
         [ 1.46210794, -2.06014071],
         [-0.3224172 , -0.38405435]]),
  &#39;b2&#39;: array([[ 1.13376944],
         [-1.09989127],
         [-0.17242821]]),
  &#39;W3&#39;: array([[-0.87785842,  0.04221375,  0.58281521]]),
  &#39;b3&#39;: array([[-1.10061918]])})
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_compute_cost_with_regularization(A3, Y_assess, parameters, lambd=0.1)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>1.7864859451590758
</code></pre><h4 id="1-2-overwrite-backward-propagation"><a href="#1-2-overwrite-backward-propagation" class="headerlink" title="1.2 overwrite backward propagation"></a>1.2 overwrite backward propagation</h4><script type="math/tex; mode=display">
\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_backward_with_regularization(dZ, cache, lambd):
    """
    Implement the linear portion of backward propagation for a single layer (layer l)

    :param dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    :param cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :param lambd -- 正则系数
    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1 / m * np.dot(dZ, A_prev.T) + lambd / m * W
    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)  # axis=1是行记录求和
    dA_prev = np.dot(W.T, dZ)

    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)
    assert(dA_prev.shape == A_prev.shape)
    return dA_prev, dW, db

def func_linear_activation_backward_with_regularization(dA, cache, lambd, activation):
    """
    Implement the backward propagation for the LINEAR->ACTIVATION layer.

    :param dA -- post-activation gradient for current layer l 
    :param cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    :param lambd -- 正则系数
    :param activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    :returns
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache = cache

    if activation == 'relu':
        dZ = func_relu_backward(dA, activation_cache)
    elif activation == 'sigmoid':
        dZ = func_sigmoid_backward(dA, activation_cache)
    else:
        raise ValueError('activation param')
    dA_prev, dW, db = func_linear_backward_with_regularization(dZ, linear_cache, lambd)
    return dA_prev, dW, db

def func_L_model_backward_with_regularization(AL, Y, caches, lambd):
    """
    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group

    :param AL -- probability vector, output of the forward propagation (L_model_forward())
    :param Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
    :param caches -- list of caches containing:
                    every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                    the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])
    :param lambd -- 正则系数
    :return:
        grads -- A dictionary with the gradients
                 grads["dA" + str(l)] = ... 
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ... 
    """
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)

    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))

    cur_cache = caches[L-1]
    grads['dA' + str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = func_linear_activation_backward_with_regularization(dAL, cur_cache, lambd, activation='sigmoid')

    for layer in reversed(range(L-1)):
        cur_cache = caches[layer]
        dA_prev_tmp, dW_tmp, db_tmp = func_linear_activation_backward_with_regularization(grads["dA"+str(layer+1)], cur_cache, lambd, activation='relu')
        grads['dA'+str(layer)] = dA_prev_tmp
        grads['dW'+str(layer+1)] = dW_tmp
        grads['db'+str(layer+1)] = db_tmp

#     current_cache = caches[L-1]
#     grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = func_linear_activation_backward(dAL, current_cache, activation = "sigmoid")

#     for l in reversed(range(L-1)):
#         # lth layer: (RELU -> LINEAR) gradients.
#         current_cache = caches[l]
#         dA_prev_temp, dW_temp, db_temp = func_linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation = "relu")
#         grads["dA" + str(l + 1)] = dA_prev_temp
#         grads["dW" + str(l + 1)] = dW_temp
#         grads["db" + str(l + 1)] = db_temp

    return grads
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_backward_propagation_with_regularization_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(3, 5)
    Y_assess = np.array([[1, 1, 0, 1, 0]])
    cache = (np.array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],
         [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]),
  np.array([[ 0.        ,  3.32524635,  2.13994541,  2.60700654,  0.        ],
         [ 0.        ,  4.1600994 ,  0.79051021,  1.46493512,  0.        ]]),
  np.array([[-1.09989127, -0.17242821, -0.87785842],
         [ 0.04221375,  0.58281521, -1.10061918]]),
  np.array([[ 1.14472371],
         [ 0.90159072]]),
  np.array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],
         [-0.69166075, -3.47645987, -2.25194702, -2.65416996, -0.69166075],
         [-0.39675353, -4.62285846, -2.61101729, -3.22874921, -0.39675353]]),
  np.array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],
         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]),
  np.array([[ 0.50249434,  0.90085595],
         [-0.68372786, -0.12289023],
         [-0.93576943, -0.26788808]]),
  np.array([[ 0.53035547],
         [-0.69166075],
         [-0.39675353]]),
  np.array([[-0.3771104 , -4.10060224, -1.60539468, -2.18416951, -0.3771104 ]]),
  np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]]),
  np.array([[-0.6871727 , -0.84520564, -0.67124613]]),
  np.array([[-0.0126646]]))
    return X_assess, Y_assess, cache
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">X_assess, Y_assess, cache = func_backward_propagation_with_regularization_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">X_assess, Y_assess, cache
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763],
        [-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038],
        [ 1.46210794, -2.06014071, -0.3224172 , -0.38405435,  1.13376944]]),
 array([[1, 1, 0, 1, 0]]),
 (array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],
         [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]),
  array([[0.        , 3.32524635, 2.13994541, 2.60700654, 0.        ],
         [0.        , 4.1600994 , 0.79051021, 1.46493512, 0.        ]]),
  array([[-1.09989127, -0.17242821, -0.87785842],
         [ 0.04221375,  0.58281521, -1.10061918]]),
  array([[1.14472371],
         [0.90159072]]),
  array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],
         [-0.69166075, -3.47645987, -2.25194702, -2.65416996, -0.69166075],
         [-0.39675353, -4.62285846, -2.61101729, -3.22874921, -0.39675353]]),
  array([[0.53035547, 5.94892323, 2.31780174, 3.16005701, 0.53035547],
         [0.        , 0.        , 0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ]]),
  array([[ 0.50249434,  0.90085595],
         [-0.68372786, -0.12289023],
         [-0.93576943, -0.26788808]]),
  array([[ 0.53035547],
         [-0.69166075],
         [-0.39675353]]),
  array([[-0.3771104 , -4.10060224, -1.60539468, -2.18416951, -0.3771104 ]]),
  array([[0.40682402, 0.01629284, 0.16722898, 0.10118111, 0.40682402]]),
  array([[-0.6871727 , -0.84520564, -0.67124613]]),
  array([[-0.0126646]])))
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">(Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">caches = [((X_assess, W1, b1), Z1), 
          ((A1, W2, b2), Z2), 
          ((A2, W3, b3), Z3)]
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">caches
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>[((array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763],
          [-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038],
          [ 1.46210794, -2.06014071, -0.3224172 , -0.38405435,  1.13376944]]),
   array([[-1.09989127, -0.17242821, -0.87785842],
          [ 0.04221375,  0.58281521, -1.10061918]]),
   array([[1.14472371],
          [0.90159072]])),
  array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],
         [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]])),
 ((array([[0.        , 3.32524635, 2.13994541, 2.60700654, 0.        ],
          [0.        , 4.1600994 , 0.79051021, 1.46493512, 0.        ]]),
   array([[ 0.50249434,  0.90085595],
          [-0.68372786, -0.12289023],
          [-0.93576943, -0.26788808]]),
   array([[ 0.53035547],
          [-0.69166075],
          [-0.39675353]])),
  array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],
         [-0.69166075, -3.47645987, -2.25194702, -2.65416996, -0.69166075],
         [-0.39675353, -4.62285846, -2.61101729, -3.22874921, -0.39675353]])),
 ((array([[0.53035547, 5.94892323, 2.31780174, 3.16005701, 0.53035547],
          [0.        , 0.        , 0.        , 0.        , 0.        ],
          [0.        , 0.        , 0.        , 0.        , 0.        ]]),
   array([[-0.6871727 , -0.84520564, -0.67124613]]),
   array([[-0.0126646]])),
  array([[-0.3771104 , -4.10060224, -1.60539468, -2.18416951, -0.3771104 ]]))]
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">grads = func_L_model_backward_with_regularization(A3, Y_assess, caches, lambd=0.7)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">grads
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>{&#39;dA2&#39;: array([[ 0.40761434,  0.67597688, -0.11491519,  0.61764379, -0.27955836],
        [ 0.50135568,  0.83143505, -0.14134288,  0.75968678, -0.34384996],
        [ 0.39816708,  0.66030979, -0.11225181,  0.60332869, -0.27307905]]),
 &#39;dW3&#39;: array([[-1.77691375, -0.11832879, -0.09397446]]),
 &#39;db3&#39;: array([[-0.38032985]]),
 &#39;dA1&#39;: array([[ 0.2048239 ,  0.33967455, -0.05774423,  0.31036251, -0.14047649],
        [ 0.3672018 ,  0.60895779, -0.10352203,  0.55640808, -0.25184181]]),
 &#39;dW2&#39;: array([[ 0.79276497,  0.85133932],
        [-0.0957219 , -0.01720463],
        [-0.13100772, -0.03750433]]),
 &#39;db2&#39;: array([[0.26135229],
        [0.        ],
        [0.        ]]),
 &#39;dA0&#39;: array([[ 0.        , -0.34789869,  0.05914233, -0.31787694,  0.        ],
        [ 0.        ,  0.29634039, -0.05037748,  0.27076784,  0.        ],
        [ 0.        , -0.96841679,  0.1646296 , -0.88484775,  0.        ]]),
 &#39;dW1&#39;: array([[-0.25604647,  0.1229883 , -0.28297132],
        [-0.17706304,  0.345361  , -0.44105717]]),
 &#39;db1&#39;: array([[0.11845857],
        [0.21236877]])}
</code></pre><h3 id="2-dropout"><a href="#2-dropout" class="headerlink" title="2-dropout"></a>2-dropout</h3><p>详情如以下视频</p>
<center>
<video width="620" height="440" src="./深度学习之吴恩达课程作业4/dropout1_kiank.mp4" type="video/mp4" controls>
</video>
</center>
<br>
<caption><center> <u> Figure 2 </u>: Drop-out on the second hidden layer. <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\_prob$ or keep it with probability $keep\_prob$ (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. </center></caption>

<center>
<video width="620" height="440" src="./深度学习之吴恩达课程作业4/dropout2_kiank.mp4" type="video/mp4" controls>
</video>
</center>

<caption><center> <u> Figure 3 </u>: Drop-out on the first and third hidden layers. <br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </center></caption>

<font color="red" size="5">注意：</font>
1. 以`1-keep_prob`值丢弃units，则要将 Divide $A^{[1]}$ by `keep_prob`. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)

#### 2.1-overwrite forward propagation


```python
def func_linear_forward(A, W, b):
    """
    linear forward
    :param A:
    :param W:
    :param b:
    :return Z,chche:
        Z -- the input of the activation function, also called pre-activation parameter 
        cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """
    Z = np.dot(W, A) + b

    assert(Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b)
    return Z, cache

def func_linear_activation_forward_with_dropout(A_prev, W, b, activation, keep_prob=None):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)
    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)
    :param b: bias vector, numpy array of shape (size of the current layer, 1)
    :param activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
    :param keep_prob -- probability of keeping a neuron active during drop-out, scalar
    :return:
        A -- the output of the activation function, also called the post-activation value 
        cache -- a python dictionary containing "linear_cache" and "activation_cache";
                 stored for computing the backward pass efficiently
    """
    Z, linear_cache = func_linear_forward(A_prev, W, b)
    D = None
    if activation == 'sigmoid':
        A, activation_cache = func_sigmoid(Z)
    elif activation == 'relu':
        A, activation_cache = func_relu(Z)
        if keep_prob:
            D = np.random.rand(*A.shape)  # Step 1: initialize matrix D1 = np.random.rand(..., ...)
            D = D < keep_prob  # convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
            A = A * D  # shut down some neurons of A
            A = A / keep_prob  # scale the value of neurons that haven't been shut down
    else:
        raise ValueError('activation param')

    assert(A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache, D)
    return A, cache

def func_L_model_forward_with_dropout(X, parameters, keep_prob=0.5):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation

    :param X -- data, numpy array of shape (input size, number of examples)
    :param parameters -- output of initialize_parameters_deep()
    :param keep_prob -- probability of keeping a neuron active during drop-out, scalar
    :return:
        AL -- last post-activation value
        caches -- list of caches containing:
                    every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                    the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """
    np.random.seed(1)
    caches = []
    A = X
    L = len(parameters) // 2

    for layer in range(1, L):
        A_prev = A
        W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]
        A, cache = func_linear_activation_forward_with_dropout(A_prev, W, b, 'relu', keep_prob)
        caches.append(cache)

    A_prev = A
    layer = L
    W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]
    A, cache = func_linear_activation_forward_with_dropout(A_prev, W, b, 'sigmoid')
    caches.append(cache)

    assert(A.shape == (1, X.shape[1]))
    return A, caches
```


```python
def func_forward_propagation_with_dropout_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(3, 5)
    W1 = np.random.randn(2, 3)
    b1 = np.random.randn(2, 1)
    W2 = np.random.randn(3, 2)
    b2 = np.random.randn(3, 1)
    W3 = np.random.randn(1, 3)
    b3 = np.random.randn(1, 1)
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}

    return X_assess, parameters
```


```python
X_assess, parameters = func_forward_propagation_with_dropout_test_case()
```


```python
X_assess, parameters
```




    (array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763],
            [-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038],
            [ 1.46210794, -2.06014071, -0.3224172 , -0.38405435,  1.13376944]]),
     {'W1': array([[-1.09989127, -0.17242821, -0.87785842],
             [ 0.04221375,  0.58281521, -1.10061918]]),
      'b1': array([[1.14472371],
             [0.90159072]]),
      'W2': array([[ 0.50249434,  0.90085595],
             [-0.68372786, -0.12289023],
             [-0.93576943, -0.26788808]]),
      'b2': array([[ 0.53035547],
             [-0.69166075],
             [-0.39675353]]),
      'W3': array([[-0.6871727 , -0.84520564, -0.67124613]]),
      'b3': array([[-0.0126646]])})




```python
AL, caches = func_L_model_forward_with_dropout(X_assess, parameters, keep_prob=0.7)
```


```python
AL, caches
```




    (array([[0.36974721, 0.00305176, 0.04565099, 0.49683389, 0.36974721]]),
     [((array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763],
               [-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038],
               [ 1.46210794, -2.06014071, -0.3224172 , -0.38405435,  1.13376944]]),
        array([[-1.09989127, -0.17242821, -0.87785842],
               [ 0.04221375,  0.58281521, -1.10061918]]),
        array([[1.14472371],
               [0.90159072]])),
       array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],
              [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]),
       array([[ True, False,  True,  True,  True],
              [ True,  True,  True,  True,  True]])),
      ((array([[0.        , 0.        , 3.05706487, 3.72429505, 0.        ],
               [0.        , 5.94299915, 1.1293003 , 2.09276446, 0.        ]]),
        array([[ 0.50249434,  0.90085595],
               [-0.68372786, -0.12289023],
               [-0.93576943, -0.26788808]]),
        array([[ 0.53035547],
               [-0.69166075],
               [-0.39675353]])),
       array([[ 0.53035547,  5.88414161,  3.08385015,  4.28707196,  0.53035547],
              [-0.69166075, -1.42199726, -2.92064114, -3.49524533, -0.69166075],
              [-0.39675353, -1.98881216, -3.55998747, -4.44246165, -0.39675353]]),
       array([[ True,  True,  True, False,  True],
              [ True,  True,  True,  True,  True],
              [False, False,  True,  True, False]])),
      ((array([[0.75765067, 8.40591658, 4.40550021, 0.        , 0.75765067],
               [0.        , 0.        , 0.        , 0.        , 0.        ],
               [0.        , 0.        , 0.        , 0.        , 0.        ]]),
        array([[-0.6871727 , -0.84520564, -0.67124613]]),
        array([[-0.0126646]])),
       array([[-0.53330145, -5.78898099, -3.04000407, -0.0126646 , -0.53330145]]),
       None)])



#### 2.2 overwrite backward propagation

1. You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to `A1`. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to `dA1`. 
2. During forward propagation, you had divided `A1` by `keep_prob`. In backpropagation, you'll therefore have to divide `dA1` by `keep_prob` again (the calculus interpretation is that if $A^{[1]}$ is scaled by `keep_prob`, then its derivative $dA^{[1]}$ is also scaled by the same `keep_prob`).



```python
def func_linear_backward(dZ, cache):
    """
    Implement the linear portion of backward propagation for a single layer (layer l)

    :param dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    :param cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1 / m * np.dot(dZ, A_prev.T)
    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)  # axis=1是行记录求和
    dA_prev = np.dot(W.T, dZ)

    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)
    assert(dA_prev.shape == A_prev.shape)
    return dA_prev, dW, db

def func_linear_activation_backward_with_dropout(dA, cache, activation, keep_prob=None):
    """
    Implement the backward propagation for the LINEAR->ACTIVATION layer.

    :param dA -- post-activation gradient for current layer l 
    :param cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    :param activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
    :param keep_prob -- 保留neuron的比例
    :returns
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache, D = cache

    if activation == 'relu':
        if keep_prob and D is not None:
            dA = dA * D  # shut down some neurons of A
            dA = dA / keep_prob  # scale the value of neurons that haven't been shut down
        dZ = func_relu_backward(dA, activation_cache)
    elif activation == 'sigmoid':
        dZ = func_sigmoid_backward(dA, activation_cache)
    else:
        raise ValueError('activation param')
    dA_prev, dW, db = func_linear_backward(dZ, linear_cache)
    return dA_prev, dW, db

def func_L_model_backward_with_dropout(AL, Y, caches, keep_prob):
    """
    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group

    :param AL -- probability vector, output of the forward propagation (L_model_forward())
    :param Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
    :param caches -- list of caches containing:
                    every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                    the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])
    :param keep_prob -- 保留neuron的比例
    :return:
        grads -- A dictionary with the gradients
                 grads["dA" + str(l)] = ... 
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ... 
    """
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)

    # todo divide by zero encountered in true_divide
    dAL = - (np.divide(Y, AL, where=AL!=0) - np.divide(1-Y, 1-AL, where=(1-AL)!=0))

    cur_cache = caches[L-1]
    grads['dA' + str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = func_linear_activation_backward_with_dropout(dAL, cur_cache, activation='sigmoid')

    for layer in reversed(range(L-1)):
        cur_cache = caches[layer]
        dA_prev_tmp, dW_tmp, db_tmp = func_linear_activation_backward_with_dropout(grads["dA"+str(layer+1)], cur_cache, activation='relu', keep_prob=keep_prob)
        grads['dA'+str(layer)] = dA_prev_tmp
        grads['dW'+str(layer+1)] = dW_tmp
        grads['db'+str(layer+1)] = db_tmp

#     current_cache = caches[L-1]
#     grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = func_linear_activation_backward(dAL, current_cache, activation = "sigmoid")

#     for l in reversed(range(L-1)):
#         # lth layer: (RELU -> LINEAR) gradients.
#         current_cache = caches[l]
#         dA_prev_temp, dW_temp, db_temp = func_linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation = "relu")
#         grads["dA" + str(l + 1)] = dA_prev_temp
#         grads["dW" + str(l + 1)] = dW_temp
#         grads["db" + str(l + 1)] = db_temp

    return grads
```


```python
def func_backward_propagation_with_dropout_test_case():
    np.random.seed(1)
    X_assess = np.random.randn(3, 5)
    Y_assess = np.array([[1, 1, 0, 1, 0]])
    cache = (np.array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],
           [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]), np.array([[ True, False,  True,  True,  True],
           [ True,  True,  True,  True, False]], dtype=bool), np.array([[ 0.        ,  0.        ,  4.27989081,  5.21401307,  0.        ],
           [ 0.        ,  8.32019881,  1.58102041,  2.92987024,  0.        ]]), np.array([[-1.09989127, -0.17242821, -0.87785842],
           [ 0.04221375,  0.58281521, -1.10061918]]), np.array([[ 1.14472371],
           [ 0.90159072]]), np.array([[ 0.53035547,  8.02565606,  4.10524802,  5.78975856,  0.53035547],
           [-0.69166075, -1.71413186, -3.81223329, -4.61667916, -0.69166075],
           [-0.39675353, -2.62563561, -4.82528105, -6.0607449 , -0.39675353]]), np.array([[ True, False,  True, False,  True],
           [False,  True, False,  True,  True],
           [False, False,  True, False, False]], dtype=bool), np.array([[ 1.06071093,  0.        ,  8.21049603,  0.        ,  1.06071093],
           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]), np.array([[ 0.50249434,  0.90085595],
           [-0.68372786, -0.12289023],
           [-0.93576943, -0.26788808]]), np.array([[ 0.53035547],
           [-0.69166075],
           [-0.39675353]]), np.array([[-0.7415562 , -0.0126646 , -5.65469333, -0.0126646 , -0.7415562 ]]), np.array([[ 0.32266394,  0.49683389,  0.00348883,  0.49683389,  0.32266394]]), np.array([[-0.6871727 , -0.84520564, -0.67124613]]), np.array([[-0.0126646]]))


    return X_assess, Y_assess, cache
```


```python
X_assess, Y_assess, cache = func_backward_propagation_with_dropout_test_case()
```


```python
(Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
```


```python
caches = [((X_assess, W1, b1), Z1, D1), 
          ((A1, W2, b2), Z2, D2), 
          ((A2, W3, b3), Z3, None)]
```


```python
grads = func_L_model_backward_with_dropout(A3, Y_assess, caches, keep_prob=0.8)
```


```python
grads
```




    {'dA2': array([[ 0.46544685,  0.34576201, -0.00239743,  0.34576201, -0.22172585],
            [ 0.57248826,  0.42527883, -0.00294878,  0.42527883, -0.27271738],
            [ 0.45465921,  0.3377483 , -0.00234186,  0.3377483 , -0.21658692]]),
     'dW3': array([[-0.06951191,  0.        ,  0.        ]]),
     'db3': array([[-0.2715031]]),
     'dA1': array([[ 0.29235551,  0.        , -0.00150587,  0.        , -0.13926998],
            [ 0.5241257 ,  0.        , -0.00269967,  0.        , -0.24967881]]),
     'dW2': array([[-0.00256518, -0.0009476 ],
            [ 0.        ,  0.        ],
            [ 0.        ,  0.        ]]),
     'db2': array([[0.06033089],
            [0.        ],
            [0.        ]]),
     'dA0': array([[ 0.        ,  0.        ,  0.00192791,  0.        ,  0.        ],
            [ 0.        ,  0.        , -0.0016422 ,  0.        ,  0.        ],
            [ 0.        ,  0.        ,  0.00536657,  0.        ,  0.        ]]),
     'dW1': array([[0.00019884, 0.00028657, 0.00012138],
            [0.00035647, 0.00051375, 0.00021761]]),
     'db1': array([[-0.00037647],
            [-0.00067492]])}




```python
grads['dA2'] * D2 / 0.8
```




    array([[ 0.58180856,  0.        , -0.00299679,  0.        , -0.27715731],
           [ 0.        ,  0.53159854, -0.        ,  0.53159854, -0.34089673],
           [ 0.        ,  0.        , -0.00292733,  0.        , -0.        ]])




```python
grads['dA1'] * D1 / 0.8
```




    array([[ 0.36544439,  0.        , -0.00188234,  0.        , -0.17408748],
           [ 0.65515713,  0.        , -0.00337459,  0.        , -0.        ]])



### overwrite dnn model


```python
def func_L_layer_dnn_model(X, Y, layer_dims, lr=0.3, num_epochs=30000, print_cost=True, lambd=0, keep_prob=1):
    """
    多层神经网络模型
    :param X:
    :param Y:
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param lr: learning rate
    :param num_epochs:
    :param print_cost:
    :param lambd: regularilization parameters
    :param keep_prob: dropout keep prob

    :return params
    """
#     np.random.seed(1)
    costs = []
    grads = {}
#     m = X.shape[1]

    # 参数初始化
    parameters = func_L_layers_initialize_parameters(layer_dims, seed=3)

    # loop
    for epoch in range(num_epochs):
        # 前向传播
        if keep_prob == 1:
            AL, caches = func_L_model_forward(X, parameters)
        elif keep_prob < 1:
            AL, caches = func_L_model_forward_with_dropout(X, parameters, keep_prob)

        # 计算损失
        if lambd == 0:
            cost = func_compute_cost(AL, Y)
        else:
            cost = func_compute_cost_with_regularization(AL, Y, parameters, lambd)

        # 后向传播
        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, 
                                            # but this assignment will only explore one at a time
        if lambd == 0 and keep_prob == 1:
            grads = func_L_model_backward(AL, Y, caches)
        elif lambd != 0:
            grads = func_L_model_backward_with_regularization(AL, Y, caches, lambd)
        elif keep_prob < 1:
            grads = func_L_model_backward_with_dropout(AL, Y, caches, keep_prob)

        # 更新参数
        parameters = func_update_parameters(parameters, grads, lr)

        # 打印信息
        if print_cost and epoch % 1000 == 0:
            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))
        if epoch % 1000 == 0:
            costs.append(cost)

    return parameters, costs
```

### 在模拟数据上训练
#### 导入数据


```python
def load_2D_dataset():
    data = scipy.io.loadmat('./深度学习之吴恩达课程作业4/data.mat')
    train_X = data['X'].T
    train_Y = data['y'].T
    test_X = data['Xval'].T
    test_Y = data['yval'].T

    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);

    return train_X, train_Y, test_X, test_Y
```


```python
train_X, train_Y, test_X, test_Y = load_2D_dataset()
```


![output_96_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_96_0.png)



#### 设置参数


```python
layer_dims = [train_X.shape[0], 20, 3, 1]
print(layer_dims)

lr = 0.3
num_epochs = 30000
print_cost=True

lambd = 0
keep_prob=1
```

    [2, 20, 3, 1]


#### baseline model: 没有任何正则化手段的模型


```python
parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost, lambd=lambd, keep_prob=keep_prob)
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
```

    cost after epoch 0: 0.6557412523481002
    cost after epoch 1000: 0.2222669386845115
    cost after epoch 2000: 0.20288702607598844
    cost after epoch 3000: 0.1825149792468696
    cost after epoch 4000: 0.18053978306217752
    cost after epoch 5000: 0.17620471758400458
    cost after epoch 6000: 0.1683273039211542
    cost after epoch 7000: 0.16583593654672965
    cost after epoch 8000: 0.163126719144605
    cost after epoch 9000: 0.15942612253245111
    cost after epoch 10000: 0.16329987525724202
    cost after epoch 11000: 0.16098614487789192
    cost after epoch 12000: 0.15764474148192972
    cost after epoch 13000: 0.15213599644222855
    cost after epoch 14000: 0.14843713518977336
    cost after epoch 15000: 0.1479640092257416
    cost after epoch 16000: 0.1463487630634997
    cost after epoch 17000: 0.14634375863077015
    cost after epoch 18000: 0.1433508853738097
    cost after epoch 19000: 0.13936488362305466
    cost after epoch 20000: 0.13851642423258528
    cost after epoch 21000: 0.14118678351792227
    cost after epoch 22000: 0.13629319468823384
    cost after epoch 23000: 0.1315186013752767
    cost after epoch 24000: 0.13032326002084876
    cost after epoch 25000: 0.13285370211500933
    cost after epoch 26000: 0.12814812861960656
    cost after epoch 27000: 0.13263846518850458
    cost after epoch 28000: 0.12678384582783875
    cost after epoch 29000: 0.12408776091061005
    on the train set:
    acc: 0.9478672985781991
    on the test set:
    acc: 0.915




​    ![output_100_1](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_100_1.png)



```python
plt.title("Model without regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
```


![output_101_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_101_0.png)



#### L2 regularization model


```python
layer_dims = [train_X.shape[0], 20, 3, 1]
print(layer_dims)

lr = 0.3
num_epochs = 30000
print_cost=True

lambd = 0.7
keep_prob=1
```

    [2, 20, 3, 1]



```python
parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost, lambd=lambd, keep_prob=keep_prob)
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
```

    cost after epoch 0: 0.6974484493131264
    cost after epoch 1000: 0.27304563479326766
    cost after epoch 2000: 0.26985760798332925
    cost after epoch 3000: 0.269508280641266
    cost after epoch 4000: 0.26926544493616217
    cost after epoch 5000: 0.2690430474349705
    cost after epoch 6000: 0.26889745607345616
    cost after epoch 7000: 0.26877517178557253
    cost after epoch 8000: 0.26867325020602606
    cost after epoch 9000: 0.2685595174585008
    cost after epoch 10000: 0.2684918873282239
    cost after epoch 11000: 0.2684141282822975
    cost after epoch 12000: 0.268370115369031
    cost after epoch 13000: 0.2683177401855035
    cost after epoch 14000: 0.26826704893477976
    cost after epoch 15000: 0.2682199033729047
    cost after epoch 16000: 0.2681687054797271
    cost after epoch 17000: 0.2681517100224052
    cost after epoch 18000: 0.2681331699914915
    cost after epoch 19000: 0.2680892379993576
    cost after epoch 20000: 0.26809163371273015
    cost after epoch 21000: 0.26803821899502356
    cost after epoch 22000: 0.26806045797644007
    cost after epoch 23000: 0.26782909050763876
    cost after epoch 24000: 0.2678953595054706
    cost after epoch 25000: 0.2679428566388702
    cost after epoch 26000: 0.26785446611618824
    cost after epoch 27000: 0.26791625501065636
    cost after epoch 28000: 0.26789526792627955
    cost after epoch 29000: 0.2678261099093219
    on the train set:
    acc: 0.9383886255924171
    on the test set:
    acc: 0.93




![output_104_1](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_104_1.png)




```python
plt.title("Model with L2-regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
```


![output_105_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_105_0.png)



#### dropout model


```python
layer_dims = [train_X.shape[0], 20, 3, 1]
print(layer_dims)

lr = 0.3
num_epochs = 30000
print_cost=True

lambd = 0
keep_prob=0.86
```

    [2, 20, 3, 1]



```python
parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost, lambd=lambd, keep_prob=keep_prob)
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
```

    cost after epoch 0: 0.6543912405149825
    cost after epoch 1000: 0.1759784892976791
    cost after epoch 2000: 0.10396707442944592
    cost after epoch 3000: 0.07661297787708934
    cost after epoch 4000: 0.06719891270763614
    cost after epoch 5000: 0.06466905010089666
    cost after epoch 6000: 0.06240987262093488
    cost after epoch 7000: 0.0618787353396944
    cost after epoch 8000: 0.06143321818157481
    cost after epoch 9000: 0.06119214378902588
    cost after epoch 10000: 0.06101698660803223
    cost after epoch 11000: 0.06093245086027745
    cost after epoch 12000: 0.06082640200152239
    cost after epoch 13000: 0.060862641336133956
    cost after epoch 14000: 0.06083521540719877
    cost after epoch 15000: 0.0606645721999514
    cost after epoch 16000: 0.06064947241699113
    cost after epoch 17000: 0.0606258566616184
    cost after epoch 18000: 0.060658745096947145
    cost after epoch 19000: 0.06059651334487958
    cost after epoch 20000: 0.060582435841229895
    cost after epoch 21000: 0.06055919942402353
    cost after epoch 22000: 0.06057898394513002
    cost after epoch 23000: 0.06056461470733185
    cost after epoch 24000: 0.060503590314540175
    cost after epoch 25000: 0.06050179006911904
    cost after epoch 26000: 0.06052211745803752
    cost after epoch 27000: 0.06047314743833395
    cost after epoch 28000: 0.06047206696729297
    cost after epoch 29000: 0.06048596676559831
    on the train set:
    acc: 0.9289099526066351
    on the test set:
    acc: 0.95




![output_108_1](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_108_1.png)




```python
plt.title("Model with dropout")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
```


![output_109_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A4/output_109_0.png)



### <font color="red" size="5">总结</font>

<p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system. </p>
<h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><ul>
<li>检查反向传播过程是否正确？则需要梯度检验</li>
</ul>
<h3 id="工作原理介绍"><a href="#工作原理介绍" class="headerlink" title="工作原理介绍"></a>工作原理介绍</h3><ol>
<li>确保你的前向传播和计算loss完全正确！</li>
<li>检验反向传播过程，也就是检查$\frac{\partial J}{\partial \theta}$是否计算正确</li>
<li>回顾导数的定义</li>
</ol>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}</script><ul>
<li>则可以使用$J(\theta + \varepsilon)$ and $J(\theta - \varepsilon)$ (in the case that $\theta$ is a real number), since you’re confident your implementation for $J$ is correct</li>
</ul>
<h3 id="1D-梯度检验"><a href="#1D-梯度检验" class="headerlink" title="1D 梯度检验"></a>1D 梯度检验</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p> To show that the <code>func_1d_backward_propagation()</code> function is correctly computing the gradient $\frac{\partial J}{\partial \theta}$, let’s implement gradient checking.</p>
<ul>
<li>First compute “gradapprox” using the formula above (1) and a small value of $\varepsilon$. Here are the Steps to follow:<ol>
<li>$\theta^{+} = \theta + \varepsilon$</li>
<li>$\theta^{-} = \theta - \varepsilon$</li>
<li>$J^{+} = J(\theta^{+})$</li>
<li>$J^{-} = J(\theta^{-})$</li>
<li>$gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon}$</li>
</ol>
</li>
<li>Then compute the gradient using backward propagation, and store the result in a variable “grad”</li>
<li>Finally, compute the relative difference between “gradapprox” and the “grad” using the following formula:<script type="math/tex; mode=display">difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2} \tag{2}</script>You will need 3 Steps to compute this formula:<ul>
<li>1’. compute the numerator using np.linalg.norm(…)</li>
<li>2’. compute the denominator. You will need to call np.linalg.norm(…) twice.</li>
<li>3’. divide them.</li>
</ul>
</li>
<li>If this difference is small (say less than $10^{-7}$), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation. </li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_1d_forward_propagation(x, theta):
    """
    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)

    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well

    Returns:
    J -- the value of function J, computed using the formula J(theta) = theta * x
    """
    return theta * x
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">x, theta = 2, 4
J = func_1d_forward_propagation(x, theta)
print(J)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre><code>8
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_1d_backward_propagation(x, theta):
    dtheta = x
    return dtheta
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">dtheta = func_1d_backward_propagation(x, theta)
print(dtheta)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>2
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_1d_gradient_check(x, theta, epsilon=1e-7):
    """
    Implement the backward propagation presented in Figure 1.

    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)

    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """
    theta_plus = theta + epsilon
    theta_minus = theta - epsilon

    J_plus = func_1d_forward_propagation(x, theta_plus)
    J_minus = func_1d_forward_propagation(x, theta_minus)

    grad_approx = (J_plus - J_minus) / 2 / epsilon

    grad = func_1d_backward_propagation(x, theta)

    numerator = np.linalg.norm(grad - grad_approx)
    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)
    difference = numerator / denominator

    if difference < 1e-7:
        print('the gradient is correct!')
    else:
        print('the gradient is wrong!')
    return difference
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">difference = func_1d_gradient_check(x, theta)
difference
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>the gradient is correct!





2.919335883291695e-10
</code></pre><h3 id="N-dimensional-gradient-checking"><a href="#N-dimensional-gradient-checking" class="headerlink" title="N-dimensional gradient checking"></a>N-dimensional gradient checking</h3><h4 id="check-function-自己写的"><a href="#check-function-自己写的" class="headerlink" title="check function=自己写的"></a>check function=自己写的</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_nd_gradient_check(parameters, gradients, X, Y, epsilon=1e-7):
    """
    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n

    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. 
    x -- input datapoint, of shape (input size, 1)
    y -- true "label"
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)

    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """

    gradients_approx = {}

    for key, value in parameters.items():
#         print(key)
        parameters_copy = copy.copy(parameters)

        value_shape = value.shape  # w参数的规模
        value_vector = np.reshape(value, (-1, 1))  # 将w展开成(n_w, 1)
#         print(value_vector.shape, value_shape)
        gradapprox_vector = np.zeros((value_vector.shape[0], 1))  # 每改变一个w，对应一个gradapprox
        for i in range(value_vector.shape[0]):  # 每个w过一遍
            theta_plus = np.copy(value_vector)
            theta_plus[i][0] = theta_plus[i][0] + epsilon
            parameters_copy[key] = theta_plus.reshape(value_shape)
    #         print(parameters_copy)
            AL, _ = func_L_model_forward(X, parameters_copy)
            cost_plus = func_compute_cost(AL, Y)

            theta_minus = np.copy(value_vector)
            theta_minus[i][0] = theta_minus[i][0] - epsilon
            parameters_copy[key] = theta_minus.reshape(value_shape)
            AL, _ = func_L_model_forward(X, parameters_copy)
            cost_minus = func_compute_cost(AL, Y)

            gradapprox_vector[i] = (cost_plus - cost_minus) / 2 / epsilon
#         print(gradapprox_vector, gradapprox_vector.shape)
        gradients_approx['d'+key] = gradapprox_vector.reshape(value_shape)


    # 将gradients转成vector
    count = 0
    for key in gradients_approx.keys():
#         print(key)
        new_vector = np.reshape(gradients[key], (-1, 1))
        new_vector_approx = np.reshape(gradients_approx[key], (-1, 1))

        if count == 0:
            theta = new_vector
            theta_approx = new_vector_approx
        else:
            theta = np.concatenate((theta, new_vector), axis=0)
            theta_approx = np.concatenate((theta_approx, new_vector_approx), axis=0)
        count += 1

    # 计算difference
#     print(theta.shape, theta_approx.shape)
#     print(theta, theta_approx)
    numerator = np.linalg.norm(theta - theta_approx)
    denominator = np.linalg.norm(theta) + np.linalg.norm(theta_approx)
#     print(numerator, denominator)
#     print(np.linalg.norm(theta), np.linalg.norm(theta_approx))
    difference = numerator / denominator

    if difference > 2e-7:
        print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
    else:
        print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")

    return difference
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="测试用的"><a href="#测试用的" class="headerlink" title="测试用的"></a>测试用的</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def dictionary_to_vector(parameters):
    """
    Roll all our parameters dictionary into a single vector satisfying our specific required shape.
    """
    keys = []
    count = 0
    for key in ["W1", "b1", "W2", "b2", "W3", "b3"]:

        # flatten parameter
        new_vector = np.reshape(parameters[key], (-1,1))
        keys = keys + [key]*new_vector.shape[0]

        if count == 0:
            theta = new_vector
        else:
            theta = np.concatenate((theta, new_vector), axis=0)
        count = count + 1

    return theta, keys
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def vector_to_dictionary(theta):
    """
    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.
    """
    parameters = {}
    parameters["W1"] = theta[:20].reshape((10,2))
    parameters["b1"] = theta[20:30].reshape((10,1))
    parameters["W2"] = theta[30:80].reshape((5,10))
    parameters["b2"] = theta[80:85].reshape((5,1))
    parameters["W3"] = theta[85:90].reshape((1,5))
    parameters["b3"] = theta[90:91].reshape((1,1))

    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def gradients_to_vector(gradients):
    """
    Roll all our gradients dictionary into a single vector satisfying our specific required shape.
    """

    count = 0
    for key in ["dW1", "db1", "dW2", "db2", "dW3", "db3"]:
        # flatten parameter
        new_vector = np.reshape(gradients[key], (-1,1))

        if count == 0:
            theta = new_vector
        else:
            theta = np.concatenate((theta, new_vector), axis=0)
        count = count + 1

    return theta
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python"># GRADED FUNCTION: gradient_check_n

def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):
    """
    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n

    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. 
    x -- input datapoint, of shape (input size, 1)
    y -- true "label"
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)

    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """

    # Set-up variables
    parameters_values, _ = dictionary_to_vector(parameters)
    grad = gradients_to_vector(gradients)
    num_parameters = parameters_values.shape[0]
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))

    # Compute gradapprox
    for i in range(num_parameters):

        # Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".
        # "_" is used because the function you have to outputs two parameters but we only care about the first one
        ### START CODE HERE ### (approx. 3 lines)
        thetaplus = np.copy(parameters_values)                                      # Step 1
        thetaplus[i][0] += epsilon                                # Step 2
        AL, _ = func_L_model_forward(X, vector_to_dictionary( thetaplus  ))
        J_plus[i] = func_compute_cost(AL, Y)
#         J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary( thetaplus  ))                                   # Step 3
        ### END CODE HERE ###

        # Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".
        ### START CODE HERE ### (approx. 3 lines)
        thetaminus = np.copy(parameters_values)                                     # Step 1
        thetaminus[i][0] -= epsilon                               # Step 2        
        AL, _ = func_L_model_forward(X, vector_to_dictionary( thetaminus  ))
        J_minus[i] = func_compute_cost(AL, Y)
#         J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary( thetaminus  ))                                  # Step 3
        ### END CODE HERE ###

        # Compute gradapprox[i]
        ### START CODE HERE ### (approx. 1 line)
        gradapprox[i] = (J_plus[i] - J_minus[i]) / 2/ epsilon
        ### END CODE HERE ###

    # Compare gradapprox to backward propagation gradients by computing difference.
    ### START CODE HERE ### (approx. 1 line)
    numerator = np.linalg.norm(grad - gradapprox)                               # Step 1'
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2'
#     print(numerator, denominator)
#     print(grad, gradapprox)
    difference = numerator / denominator                                          # Step 3'
    ### END CODE HERE ###

    if difference > 2e-7:
        print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
    else:
        print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")

    return difference
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="overwrite-dnn-model-1"><a href="#overwrite-dnn-model-1" class="headerlink" title="overwrite dnn model"></a>overwrite dnn model</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layer_dnn_model(X, Y, layer_dims, lr=0.001, num_epochs=10000, print_cost=False):
    """
    多层神经网络模型
    :param X:
    :param Y:
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param lr: learning rate
    :param num_epochs:
    :param print_cost:
    :return params
    """
    np.random.seed(1)
    costs = []
#     m = X.shape[1]

    # 参数初始化
    parameters = func_L_layers_initialize_parameters(layer_dims)

    # loop
    for epoch in range(num_epochs):
        # 前向传播
        AL, caches = func_L_model_forward(X, parameters)

        # 计算损失
        cost = func_compute_cost(AL, Y)

        # 后向传播
        grads = func_L_model_backward(AL, Y, caches)

        if epoch == num_epochs-1:
            # 梯度检验
            # print(grads, caches)
            difference = func_nd_gradient_check(parameters, grads, X, Y)
            difference = gradient_check_n(parameters, grads, X, Y)


        # 更新参数
        parameters = func_update_parameters(parameters, grads, lr)

        # 打印信息
        if print_cost and epoch % 100 == 0:
            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))
        if epoch % 100 == 0:
            costs.append(cost)

    return parameters, costs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="在模拟数据上训练-1"><a href="#在模拟数据上训练-1" class="headerlink" title="在模拟数据上训练"></a>在模拟数据上训练</h4><h5 id="导入数据-1"><a href="#导入数据-1" class="headerlink" title="导入数据"></a>导入数据</h5><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_X, train_Y, test_X, test_Y = load_dataset()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_129_0.png" alt="output_129_0"></p>
<h5 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h5><pre class="line-numbers language-lang-python"><code class="language-lang-python">layer_dims = [train_X.shape[0], 10, 5, 1]
print(layer_dims)

lr = 0.1
num_epochs = 1500
print_cost=True
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>[2, 10, 5, 1]
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, costs = func_L_layer_dnn_model(train_X, train_Y, layer_dims, lr, num_epochs, print_cost)
print('on the train set:')
pred_train = func_predict(train_X, train_Y, parameters)
print('on the test set:')
pred_test = func_predict(test_X, test_Y, parameters)

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>cost after epoch 0: 0.6960659020466242
cost after epoch 100: 0.6891991419051238
cost after epoch 200: 0.6850092483958543
cost after epoch 300: 0.6798479812645738
cost after epoch 400: 0.6729753561826027
cost after epoch 500: 0.6580667774079205
cost after epoch 600: 0.6311404895208866
cost after epoch 700: 0.5853323852501582
cost after epoch 800: 0.5132274924019706
cost after epoch 900: 0.41849780592362346
cost after epoch 1000: 0.3288154213170582
cost after epoch 1100: 0.2578987108855108
cost after epoch 1200: 0.20516738899849638
cost after epoch 1300: 0.16682454264195795
cost after epoch 1400: 0.13371779611157564
[92mYour backward propagation works perfectly fine! difference = 1.0610826936117557e-08[0m
[92mYour backward propagation works perfectly fine! difference = 1.0610826936117557e-08[0m
on the train set:
acc: 0.99
on the test set:
acc: 0.94
</code></pre><p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_132_1.png" alt="output_132_1"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in parameters.items():
    print(key, value.shape)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>W1 (10, 2)
b1 (10, 1)
W2 (5, 10)
b2 (5, 1)
W3 (1, 5)
b3 (1, 1)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.title("Model with np.sqrt(layer_dims(layer-1)) initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
func_plot_decision_boundary(lambda x: func_predict_dec(parameters, x.T), train_X, train_Y)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/output_134_0.png" alt="output_134_0"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><font color="red" size="5">总结</font></h3><ol>
<li>梯度检验很慢，因为每一个w的分量都要计算plus和minus，所以不用每一个epoch都梯度检验，只需要某些epoch进行检验梯度计算是否正确即可</li>
<li>梯度检验不适用于dropout，先检验是否正确，再加dropout</li>
</ol>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io" rel="external nofollow noreferrer">Myhaa</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/">https://myhaa.github.io/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                    <span class="chip bg-color">神经网络</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">感谢您的赏识！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'a1900fd4b8fb7a569ef7',
        clientSecret: 'd1176a5ad242e4887008f5d4389ea5a35f199c44',
        repo: 'myhaa.github.io',
        owner: 'myhaa',
        admin: ["myhaa"],
        id: '2021-11-03T09-19-18',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/11/08/shen-du-xue-xi-ng-zhi-you-hua-fang-fa-shi-jian/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/19.jpg" class="responsive-img" alt="深度学习之吴恩达课程作业5">
                        
                        <span class="card-title">深度学习之吴恩达课程作业5</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            参数求解优化方法
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-11-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                        <span class="chip bg-color">神经网络</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="读书笔记之深度学习推荐系统">
                        
                        <span class="card-title">读书笔记之深度学习推荐系统</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            深度学习推荐系统的读书
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-category">
                                    读书笔记
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">
                        <span class="chip bg-color">推荐系统</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Myhaa's Blog<br />'
            + '文章作者: Myhaa<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>

    
<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">年份</span>
            <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    window.setTimeout("siteTime()", 1000);
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2019";
                    var startMonth = "11";
                    var startDate = "11";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">














    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="/libs/instantpage/instantpage.js" type="module"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
    

</body>

</html>
