<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="读书笔记之深度学习推荐系统, 统计学 数据挖掘 机器学习 计算广告">
    <meta name="description" content="参考书籍
王喆的深度学习推荐系统

读书笔记之深度学习推荐系统ch1_互联网的增长引擎-推荐系统
互联网企业的核心需求是“增长”，而推荐系统正处在“增长引擎”的核心位置

推荐系统要解决的“用户痛点”是用户如何在“信息过载”的情况下高效的获">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>读书笔记之深度学习推荐系统 | Myhaa&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    
    <script src="/libs/jquery/jquery.min.js"></script>
    
<link rel="alternate" href="/atom.xml" title="Myhaa's Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Myhaa's Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Myhaa's Blog</div>
        <div class="logo-desc">
            
            要么孤独，要么庸俗
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/myhaa" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/myhaa" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        读书笔记之深度学习推荐系统
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">
                                <span class="chip bg-color">推荐系统</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-category">
                                读书笔记
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-23
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    71 分
                </div>
                
				
                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul>
<li><a href>王喆的深度学习推荐系统</a></li>
</ul>
<h1 id="读书笔记之深度学习推荐系统"><a href="#读书笔记之深度学习推荐系统" class="headerlink" title="读书笔记之深度学习推荐系统"></a>读书笔记之深度学习推荐系统</h1><h2 id="ch1-互联网的增长引擎-推荐系统"><a href="#ch1-互联网的增长引擎-推荐系统" class="headerlink" title="ch1_互联网的增长引擎-推荐系统"></a>ch1_互联网的增长引擎-推荐系统</h2><ul>
<li><p>互联网企业的核心需求是“增长”，而推荐系统正处在“增长引擎”的核心位置</p>
</li>
<li><p>推荐系统要解决的“用户痛点”是用户如何在“信息过载”的情况下高效的获得感兴趣的信息</p>
</li>
<li><p>推荐系统的技术架构示意图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20210923094027196.png" alt="image-20210923094027196"></p>
</li>
</ul>
<h2 id="ch2-前深度学习时代-推荐系统的进化之路"><a href="#ch2-前深度学习时代-推荐系统的进化之路" class="headerlink" title="ch2_前深度学习时代-推荐系统的进化之路"></a>ch2_前深度学习时代-推荐系统的进化之路</h2><ul>
<li><p>协同过滤</p>
<ul>
<li><p>基于用户的</p>
<ul>
<li><p>适合业务场景</p>
<ul>
<li>适合社交属性强的应用场景</li>
</ul>
</li>
<li><p>相似度计算</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20210923100728637.png" alt="image-20210923100728637"></p>
</li>
<li><p>缺点</p>
<ul>
<li>用户数往往大于物品数，需要维护的用户相似度矩阵存储开销非常大</li>
<li>用户历史数据往往比较稀疏，对于只有几次购买或者点击行为的用户来说，找到相似用户的准确度较低</li>
</ul>
</li>
</ul>
</li>
<li><p>基于物品的</p>
<ul>
<li>适合业务场景<ul>
<li>适合兴趣变化较稳定的业务场景</li>
</ul>
</li>
<li>步骤<ul>
<li>基于历史数据构建用户-物品共现矩阵</li>
<li>基于共现矩阵计算两两物品相似度</li>
<li>获得用户历史行为数据中正反馈物品列表</li>
<li>根据正反馈物品，找出相似物品，根据相似度分支进行推荐</li>
</ul>
</li>
</ul>
</li>
<li><p>协同过滤的缺点</p>
<ul>
<li>不好泛化，热门商品有很强的头部效应，跟谁都相关</li>
<li>仅利用用户-物品的交互信息，无法有效引入用户年龄、性别、商品描述等用户、物品、上下文特征</li>
</ul>
</li>
</ul>
</li>
<li><p>基于矩阵分解=协同过滤的进化</p>
<ul>
<li>针对协同过滤算法的头部效应，泛化能力弱的问题来提出的</li>
</ul>
</li>
</ul>
<h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><ul>
<li>协同过滤、罗辑回归、因子分解机等传统模型可解释性强、硬件环境要求低、易于快速训练和部署</li>
<li>传统推荐模型是深度推荐模型的基础</li>
</ul>
<h3 id="传统推荐模型的演化关系"><a href="#传统推荐模型的演化关系" class="headerlink" title="传统推荐模型的演化关系"></a>传统推荐模型的演化关系</h3><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171328366.png" alt="image-20220311171328366"></p>
<ul>
<li><p>协同过滤算法族</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171507593.png" alt="image-20220311171507593"></p>
<ul>
<li><p>用户协同过滤</p>
<ul>
<li>基于用户-物品共现矩阵，计算用户相似度</li>
<li><p>用户相似度计算</p>
<ul>
<li><p>余弦相似度</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171603085.png" alt="image-20220311171603085"></p>
</li>
<li><p>皮尔逊相关系数</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171617995.png" alt="image-20220311171617995"></p>
<ul>
<li><p>通过使用用户平均分对各独立评分进行修正，减小用户评分偏置的影响</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171629428.png" alt="image-20220311171629428"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>获得TOP n相似用户后，假设目标用户与其相似用户的喜好是相似的</p>
</li>
<li><p>常用的方式是利用用户相似度和相似用户的评价加权平均获得目标用户的评价预测</p>
<ul>
<li>例如：用户u对物品p的评分=与用户u相似用户s对物品p的评分和*相似度/与用户u相似用户s的所有相似度和</li>
</ul>
</li>
<li><p>适用于发现热点、以及跟踪热点对趋势</p>
</li>
<li><p>缺点</p>
<ul>
<li>用户数远大于物品数，需要维护用户相似度矩阵，这导致存储开销非常大</li>
<li>用户历史数据非常稀疏，找到相似用户准确度非常低，不适用那些正向反馈获取困难的场景</li>
</ul>
</li>
</ul>
</li>
<li><p>物品协同过滤</p>
<ul>
<li>基于用户-物品共现矩阵，计算物品相似度</li>
<li>计算目标用户u对物品p对评分=计算用户u对与物品p相似对所有物品h对评分和</li>
<li>适用于兴趣变化相对稳定的场景，例如推荐视频</li>
<li><p>缺点</p>
<ul>
<li>没有很强的泛化能力</li>
<li>热门物品具有很强头部效应，容易与大量物品产生相似性</li>
<li>而尾部物品由于特征稀疏，很少与其他产生相似，导致很少被推荐</li>
<li>仅利用用户-物品交互信息，无法有效引入其他信息如用户年龄、性别、商品描述等特征</li>
</ul>
</li>
</ul>
</li>
<li><p>矩阵分解模型</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171710076.png" alt="image-20220311171710076"></p>
<ul>
<li>为了更好处理稀疏共现矩阵，增强模型泛化能力，降低头部效应问题</li>
<li><p>矩阵分解算法的原理</p>
<ul>
<li>矩阵分解算法期望为每个用户和物品生成一个隐向量</li>
<li><p>矩阵分解算法将m<em>n的共现矩阵分解成m</em>k的用户矩阵和k*n的物品矩阵</p>
<ul>
<li>其中m是用户数量</li>
<li>n是物品数量</li>
<li><p>k是隐向量维度</p>
<ul>
<li>k越大，隐向量表达能力越强，但泛化能力降低</li>
<li>在具体应用中，k要通过多次试验来得到</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>矩阵分解的求解过程</p>
<ul>
<li><p>特征值分解</p>
<ul>
<li>只能应用于方阵，不适用于用户-物品矩阵</li>
</ul>
</li>
<li><p>奇异值分解，SVD</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311171925088.png" alt="image-20220311171925088"></p>
</li>
<li><p>梯度下降</p>
<ul>
<li>主要方法</li>
<li><p>目标函数是让原始评分与用户向量和物品向量之积的差尽可能小</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311172111757.png" alt="image-20220311172111757"></p>
<ul>
<li><p>为了消除用户和物品打分的偏差，损失函数一般为</p>
<ul>
<li><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311172259515.png" alt="image-20220311172259515"></p>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>根据损失函数计算对应函数梯度并更新参数</p>
<ul>
<li><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311172433312.png" alt="image-20220311172433312"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>优点和局限性</p>
<ul>
<li><p>优点</p>
<ul>
<li>泛化能力强，在一定程度上解决了数据稀疏</li>
<li>空间复杂度低，只需存储用户、物品隐向量，复杂度(n+m)*k</li>
<li>更好的扩展性和灵活性，类似于深度学习中的embedding</li>
</ul>
</li>
<li><p>局限性</p>
<ul>
<li>没有加入用户、物品和上下文相关的特征，丧失很多有效信息</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>逻辑回归模型族</p>
<ul>
<li><p>LR模型</p>
<ul>
<li><p>基于LR的推荐流程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311172636928.png" alt="image-20220311172636928"></p>
</li>
<li><p>数学形式</p>
<ul>
<li>y=sigmoid(wx+b)</li>
</ul>
</li>
<li><p>训练方法</p>
<ul>
<li>梯度下降</li>
</ul>
</li>
<li><p>优势</p>
<ul>
<li><p>数学含义的支撑</p>
<ul>
<li>用户是否点击显然应该服从伯努利分布</li>
</ul>
</li>
<li><p>可解释性强</p>
</li>
<li><p>工程化需要</p>
<ul>
<li>易于并行化</li>
<li>模型简单</li>
<li>训练开销小</li>
</ul>
</li>
</ul>
</li>
<li><p>局限性</p>
<ul>
<li>表达能力不强</li>
<li>无法进行特征交叉，特征筛选等高级操作</li>
<li>需要大量人工特征工程</li>
</ul>
</li>
</ul>
</li>
<li><p>POLY2模型-特征交叉的开始</p>
<ul>
<li>该模型对所有特征进行两两交叉</li>
<li><p>缺陷</p>
<ul>
<li>POLY2进行无选择的特征交叉，原本稀疏的特征向量更加稀疏，导致大部分交叉特征的权重缺乏有效数据进行训练，无法收敛</li>
<li>权重参数的数量由n上升到n^2，极大增加训练复杂度</li>
</ul>
</li>
</ul>
</li>
<li><p>LS-PLM模型，即MLR</p>
<ul>
<li><p>在逻辑回归基础上，采用分而治之思想，先对样本进行分片，再在样本分片中应用LR进行CTR预估</p>
</li>
<li><p>数学形式</p>
<ul>
<li>首先用聚类函数对样本进行分类（softmax），再使用LR在分片中计算具体CTR</li>
</ul>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173309808.png" alt="image-20220311173309808"></p>
</li>
<li><p>优点</p>
<ul>
<li>端到端对非线性学习能力，省去大量人工样本处理和特征工程</li>
<li>模型的稀疏性强，在建模时加入了L1，L2正则</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>因子分解机模型族</p>
<ul>
<li><p>FM-隐向量特征交叉</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311172935591.png" alt="image-20220311172935591"></p>
<ul>
<li>与POLY2相比，其主要区别是用两个向量的内积取代了单一的权重系数</li>
<li>就是FM为每个特征学习了一个隐权重向量，在特征交叉时，使用两个特征隐向量内积作为交叉特征的权重</li>
<li>权重参数数量减少为nk，使用梯度下降法进行训练求解，训练复杂度可以降低到nk级别，极大的降低了训练开销</li>
<li>隐向量的引入使模型能更好解决数据稀疏性问题</li>
<li>FM虽然丢失了某些具体特征组合的精确记忆能力，但是泛化能力大大提高</li>
<li>FM较容易线上部署和服务</li>
</ul>
</li>
<li><p>FFM</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311172948760.png" alt="image-20220311172948760"></p>
<ul>
<li>在FM基础上引入特征域感知的概念，即每个特征对应的不是唯一一个隐向量，而是一组隐向量，跟不同特征域的特征交叉时，使用其对应特征域的隐向量</li>
<li>为模型引入更多有价值的信息，使模型表达能力更强</li>
<li>但是计算复杂度上升到kn^2，训练开销大</li>
</ul>
</li>
</ul>
</li>
<li><p>组合模型族</p>
<ul>
<li><p>GBDT+LR</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173111374.png" alt="image-20220311173111374"></p>
<ul>
<li>GBDT构建特征工程，得到新的离散特征向量</li>
<li>将这些离散特征向量作为LR的输入，两步分开，独立训练</li>
<li><p>训练过程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173213515.png" alt="image-20220311173213515"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>- 但是GBDT容易过拟合
</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173439045.png" alt="image-20220311173439045"></p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173457851.png" alt="image-20220311173457851"></p>
<h2 id="ch3-深度学习在推荐系统中的应用"><a href="#ch3-深度学习在推荐系统中的应用" class="headerlink" title="ch3_深度学习在推荐系统中的应用"></a>ch3_深度学习在推荐系统中的应用</h2><h3 id="概要-1"><a href="#概要-1" class="headerlink" title="概要"></a>概要</h3><ul>
<li>与传统机器学习相比，深度学习模型的表达能力强，能够挖掘出更多数据中潜藏的模式</li>
<li>深度学习中的模型结构非常灵活，能够根据业务场景和数据特点，灵活调整模型结构，使模型与应用场景完美契合</li>
</ul>
<h3 id="深度学习推荐模型的演化关系图"><a href="#深度学习推荐模型的演化关系图" class="headerlink" title="深度学习推荐模型的演化关系图"></a>深度学习推荐模型的演化关系图</h3><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173529222.png" alt="image-20220311173529222"></p>
<h3 id="几个深度推荐模型的具体讲解"><a href="#几个深度推荐模型的具体讲解" class="headerlink" title="几个深度推荐模型的具体讲解"></a>几个深度推荐模型的具体讲解</h3><ul>
<li><p>AutoRec-单隐层神经网络推荐模型</p>
<ul>
<li><p>基本原理</p>
<ul>
<li>利用协同过滤中的共现矩阵，完成物品向量或者用户向量的自编码</li>
</ul>
</li>
<li><p>模型结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173635048.png" alt="image-20220311173635048"></p>
</li>
<li><p>2种方式</p>
<ul>
<li>基于用户的AutoRec</li>
<li>基于物品的AutoRec</li>
</ul>
</li>
<li><p>预估过程</p>
<ul>
<li>基于用户的预估，输入用户向量即可得到所有物品的预测</li>
<li>基于物品的预估，需要输入所有物品向量，才能得到某个用户对所有物品的预测</li>
</ul>
</li>
<li><p>特点与局限性</p>
<ul>
<li><p>特点</p>
<ul>
<li>模型具有一定泛化能力，表达能力</li>
</ul>
</li>
<li><p>局限</p>
<ul>
<li>模型太简单，表达能力不足</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>DeepCrossing-经典的深度学习架构</p>
<ul>
<li><p>应用场景</p>
<ul>
<li>微软搜索引擎Bing中的搜索广告推荐</li>
<li><p>使用的特征</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173742614.png" alt="image-20220311173742614"></p>
</li>
</ul>
</li>
<li><p>网络结构</p>
<ul>
<li><p>图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173806357.png" alt="image-20220311173806357"></p>
</li>
<li><p>Embedding层</p>
<ul>
<li>将稀疏的类别型特征转换成稠密的Embedding向量</li>
<li>Embedding层的结构主要以经典的全连接层为主</li>
<li>一般来说，Embedding向量的维度应该远小于原始稀疏特征向量</li>
<li>数值型特征不需要Embedding</li>
</ul>
</li>
<li><p>Stacking层</p>
<ul>
<li>把不同的Embedding向量和数值型特征拼接在一起</li>
</ul>
</li>
<li><p>Multiple Residual Units层</p>
<ul>
<li><p>多层残差网络结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311173927901.png" alt="image-20220311173927901"></p>
</li>
</ul>
</li>
<li><p>Scoring层</p>
<ul>
<li><p>对于二分类</p>
<ul>
<li>逻辑回归模型</li>
</ul>
</li>
<li><p>对于多分类</p>
<ul>
<li>softmax模型</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>优缺点</p>
<ul>
<li>相比之前的模型，他可以通过调整神经网络的深度进行特征之间的深度交叉</li>
</ul>
</li>
</ul>
</li>
<li><p>NeuralCF-CF与深度学习的结合</p>
<ul>
<li><p>矩阵分解的网络化表示</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174001083.png" alt="image-20220311174001083"></p>
</li>
<li><p>模型结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174108696.png" alt="image-20220311174108696"></p>
</li>
<li><p>NeuralCF混合模型结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174123877.png" alt="image-20220311174123877"></p>
</li>
<li><p>优势和局限性</p>
<ul>
<li><p>优势</p>
<ul>
<li>基于用户向量和物品向量这两个Embedding层，利用不同的互操作层进行特征的交叉组合</li>
</ul>
</li>
<li><p>局限性</p>
<ul>
<li>基于协同过滤的思想进行构造的，所以其没有引入其他类型的特征，无疑浪费很多信息</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>PNN-加强特征交叉能力</p>
<ul>
<li>和DeepCrossing的唯一区别在于PNN使用乘积层代替了Stacking层，也就是不同的特征Embedding向量不再是简单拼接，而是通过Product操作进行两两交互</li>
<li><p>网络架构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174210393.png" alt="image-20220311174210393"></p>
</li>
<li><p>Product层的多种特征交叉方式</p>
<ul>
<li><p>内积操作</p>
</li>
<li><p>外积操作</p>
<ul>
<li>对输入特征向量的各维度进行两两交叉，形成M*M的方形矩阵</li>
<li>外积计算无疑直接将问题复杂度从M提升到M^2</li>
</ul>
</li>
</ul>
</li>
<li><p>PNN在经过对特征的线性和乘积操作后，并没有将结果直接送到上层的L1全连接层，而是在乘积层内部又进行了局部全连接层的转换，分别将线性部分z，乘积部分p映射成D1维的输入向量lz和lp（D1为L1隐层的神经元数量），再将lz和lp叠加，输入到L2隐层</p>
</li>
<li><p>优势和局限性</p>
<ul>
<li><p>优势</p>
<ul>
<li>PNN模型定义的内积和外积操作显然更有针对性的强调了不同特征之间的交互，从而让模型更容易捕获特征的交叉信息</li>
</ul>
</li>
<li><p>局限性</p>
<ul>
<li>外积操作为了优化训练效率进行了大量简化操作</li>
<li>对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的有价值信息</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Wide&amp;Deep模型-记忆能力和泛化能力的综合</p>
<ul>
<li><p>介绍</p>
<ul>
<li><p>单层的Wide部分</p>
<ul>
<li>主要作用是让模型具有较强的记忆能力</li>
<li>记忆能力可以被理解为模型直接学习并利用历史数据中物品或者特征的共现频率能力</li>
<li>例如协同过滤、逻辑回归等简单模型具有较强的记忆能力，原始数据往往可以直接影响推荐结果</li>
<li>对于多层神经网络来说，特征会被多层处理，不断与其他特征进行交叉，因此模型对于强特征的记忆反而没有简单模型深刻</li>
</ul>
</li>
<li><p>多层的Deep部分</p>
<ul>
<li>主要作用是让模型具有泛化能力</li>
<li>泛化能力可以理解为模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力</li>
<li>矩阵分解比协同过滤泛化能力强，是因为矩阵分解引入了隐向量的结构，使得数据稀少的用户或物品也能生成隐向量，从而获得有数据支撑的推荐得分</li>
<li>多层神经网络通过特征多次自动组合，可以深度发掘数据中潜在的模式，即使是非常稀疏的特征向量输入，也能得到稳定平滑的推荐概率，这就是简单模型所缺乏的泛化能力</li>
</ul>
</li>
</ul>
</li>
<li><p>模型结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174342990.png" alt="image-20220311174342990"></p>
<ul>
<li>把单输入层的Wide部分与由Embedding层和多隐层组成的Deep部分连接起来，一起输入最终的输出层</li>
<li>单层的Wide部分善于处理大量稀疏的ID类特征</li>
<li>Deep部分利用神经网络表达能力强的特点，进行深层的特征交叉，挖掘藏在特征背后的数据模式</li>
<li>最终利用逻辑回归模型，输出层将Wide部分和Deep部分组合起来，形成统一模型</li>
<li><p>google play推荐团队的业务模型结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174421033.png" alt="image-20220311174421033"></p>
</li>
</ul>
</li>
<li><p>模型的进化-Deep&amp;Cross模型</p>
<ul>
<li>主要思路是使用Cross网络替代原有的Wide部分</li>
<li>设计Cross网络的目的是增加特征之间的交互力度，使用多层交叉层对输入向量进行特征交叉</li>
<li><p>模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174458645.png" alt="image-20220311174458645"></p>
</li>
<li><p>交叉层的操作</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174533553.png" alt="image-20220311174533553"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>- 模型影响力

    - 优势

        - 抓住了业务问题的本质特点，能够融合传统模型记忆能力和深度学习模型泛化能力
        - 模型的结构并不复杂，比较容易工程实现、训练和上线

    - 不足

        - 坑太多
        - Wide层和Deep层的特征是分开的
</code></pre><ul>
<li><p>FNN-用FM的隐向量完成Embedding层初始化</p>
<ul>
<li><p>模型结构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174601895.png" alt="image-20220311174601895"></p>
</li>
<li><p>FNN模型与FM模型结合</p>
<ul>
<li><p>问题的关键在于Embedding层的改进，FNN直接用FM得到各特征的隐向量作为Embedding层初始化，相当于引入了有价值先验信息</p>
<ul>
<li>这样做的原因在于采用随机初始化的话，Embedding层收敛速度非常缓慢，主要原因还是因为Embedding层参数数量往往占整个神经网络的大半</li>
<li>由于输入向量稀疏，在SGD过程中，只有与非零特征相连的Embedding层权重会被更新</li>
</ul>
</li>
</ul>
</li>
<li><p>利用FM初始化Embedding层的过程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174628039.png" alt="image-20220311174628039"></p>
</li>
<li><p>需要注意的点</p>
<ul>
<li>在训练FM点过程中，并没有对特征域进行区分，但是在FNN模型中，特征被分为不同特征域，因此每个特征域具有对应的Embedding层</li>
<li>并且每个特征域的Embedding维度都应与FM隐向量维度保持一致</li>
</ul>
</li>
</ul>
</li>
<li><p>DeepFM-用FM代替Wide部分</p>
<ul>
<li><p>模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311174645721.png" alt="image-20220311174645721"></p>
</li>
<li><p>优势与局限</p>
<ul>
<li><p>优势</p>
<ul>
<li>用FM替换了Wide部分，加强了浅层网络部分特征组合的能力</li>
<li>FM部分与Deep部分共享相同的Embedding层</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>NFM-FM的神经网络化尝试</p>
<ul>
<li><p>NFM对FM的二阶部分的改进</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311182903067.png" alt="image-20220311182903067"></p>
</li>
<li><p>NFM的深度网络部分模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311182920971.png" alt="image-20220311182920971"></p>
</li>
<li><p>NFM可以视为Wide&amp;Deep模型的进化</p>
<ul>
<li>相比Wide&amp;Deep，NFM模型增加了特征交叉池化层</li>
</ul>
</li>
</ul>
</li>
<li><p>AFM-引入注意力机制的FM</p>
<ul>
<li><p>介绍</p>
<ul>
<li>AFM可以认为是NFM模型的延续</li>
<li>注意力机制，假设不同的交叉特征对于结果的影响程度不同</li>
<li>AFM模型引入注意力机制是通过在特征交叉池化层和最终的输出层之间加入注意力网络进行实现</li>
</ul>
</li>
<li><p>模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311182945737.png" alt="image-20220311182945737"></p>
</li>
<li><p>注意力网络的数学形式</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183029489.png" alt="image-20220311183029489"></p>
</li>
</ul>
</li>
<li><p>DIN-引入注意力机制的深度学习网络</p>
<ul>
<li><p>业务场景</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183148074.png" alt="image-20220311183148074"></p>
</li>
<li><p>问题</p>
<ul>
<li>序列中的商品既没有区分重要程度，也和广告特征中的商品ID没有关系</li>
<li>然而事实上，广告特征和用户特征的关联程度是非常强的</li>
</ul>
</li>
<li><p>模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183314045.png" alt="image-20220311183314045"></p>
</li>
<li><p>注意力部分的表达式</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183358525.png" alt="image-20220311183358525"></p>
</li>
<li><p>g(vi,va)的计算方式</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183503781.png" alt="image-20220311183503781"></p>
</li>
<li><p>注意力机制在数学形式上只是将过去的平均操作或加和操作换成了加权和或者加权平均操作</p>
</li>
</ul>
</li>
<li><p>DIEN-序列模型与推荐系统的结合</p>
<ul>
<li><p>介绍</p>
<ul>
<li>模型应用场景和DIN一致，其创新在于用序列模型模拟了用户兴趣的进化过程</li>
</ul>
</li>
<li><p>DIEN的进化动机</p>
<ul>
<li>时间相关的序列，就一定存在或深或浅的前后依赖关系</li>
<li><p>序列信息的重要性</p>
<ul>
<li>它加强了最近行为对下次行为预测的影响</li>
<li>序列模型能够学习到购买趋势的信息</li>
</ul>
</li>
</ul>
</li>
<li><p>DIEN模型的架构</p>
<ul>
<li><p>模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183525993.png" alt="image-20220311183525993"></p>
</li>
<li><p>兴趣进化网络</p>
<ul>
<li><p>行为序列层</p>
<ul>
<li>主要作用是将原始的id类行为序列转换成Embedding行为序列</li>
</ul>
</li>
<li><p>兴趣抽取层</p>
<ul>
<li><p>主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣</p>
<ul>
<li><p>基本结构是GRU</p>
<ul>
<li>GRU解决了RNN梯度消失问题</li>
<li>GRU参数量比LSTM少，训练收敛速度快</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>兴趣进化层</p>
<ul>
<li><p>主要作用是通过在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程</p>
<ul>
<li>AUGRU-基于注意力更新门的GRU结构</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>工程实现需注意</p>
<ul>
<li>序列模型比较高的训练复杂度</li>
<li>线上推断过程中的串行推断，延迟较大</li>
</ul>
</li>
</ul>
</li>
<li><p>DRN-强化学习推荐模型</p>
<ul>
<li><p>强化学习</p>
<ul>
<li>它的研究起源于机器人领域，针对智能体（Agent）在不断变化的环境（Environment）中决策和学习的过程中进行建模。</li>
<li>在智能体的学习过程中，会完成收集外部反馈（Reward），改变自身状态（State），再根据自身状态对下一步的行动（Action）进行决策，在行动后持续收集反馈的循环，即行动-反馈-状态更新-</li>
</ul>
</li>
<li><p>深度强化学习推荐系统框架</p>
<ul>
<li><p>框架图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183559621.png" alt="image-20220311183559621"></p>
</li>
<li><p>详解</p>
<ul>
<li><p>智能体</p>
<ul>
<li>推荐系统本身，包括深度学习推荐模型、探索策略以及相关的数据存储</li>
</ul>
</li>
<li><p>环境</p>
<ul>
<li>由新闻网站或App、用户组成的整个推荐系统外部环境，在环境中，用户接收推荐结果并做出相应反馈</li>
</ul>
</li>
<li><p>行动</p>
<ul>
<li>推荐系统的结果排序后推荐给用户的动作</li>
</ul>
</li>
<li><p>反馈</p>
<ul>
<li>点击or转化</li>
</ul>
</li>
<li><p>状态</p>
<ul>
<li>对环境以及自身所处具体情况的刻画</li>
</ul>
</li>
</ul>
</li>
<li><p>迭代过程</p>
<ul>
<li>初始化推荐系统</li>
<li>基于当前状态进行行动</li>
<li>用户对推荐系统的行动进行反馈</li>
<li>重复</li>
</ul>
</li>
</ul>
</li>
<li><p>深度学习推荐模型</p>
<ul>
<li>智能体部分是强化学习框架的核心</li>
<li><p>DQN-大脑角色</p>
<ul>
<li><p>通过行动进行质量评估，得到行动的效用得分，以此进行行动决策</p>
<ul>
<li>任何深度学习模型都可以作为智能体的推荐模型，并没有特殊限制</li>
</ul>
</li>
<li><p>网络结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183624918.png" alt="image-20220311183624918"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>DRN学习过程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183639638.png" alt="image-20220311183639638"></p>
</li>
<li><p>DRN在线学习算法-竞争梯度下降算法</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183658268.png" alt="image-20220311183658268"></p>
</li>
</ul>
</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183815811.png" alt="image-20220311183815811"></p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183828123.png" alt="image-20220311183828123"></p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220311183839575.png" alt="image-20220311183839575"></p>
<h2 id="ch4-Embedding技术在推荐系统中的应用"><a href="#ch4-Embedding技术在推荐系统中的应用" class="headerlink" title="ch4_Embedding技术在推荐系统中的应用"></a>ch4_Embedding技术在推荐系统中的应用</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><ul>
<li>Embedding操作的主要作用是将稀疏向量转换成稠密向量，便于上层深度神经网络处理</li>
</ul>
<h3 id="什么是Embedding"><a href="#什么是Embedding" class="headerlink" title="什么是Embedding"></a>什么是Embedding</h3><ul>
<li>Embedding就是用一个低维稠密向量表示一个对象，这里所说的对象可以是一个词、一个商品、也可以是一部电影</li>
<li><p>不同领域</p>
<ul>
<li><p>词</p>
<ul>
<li>Embedding(man)和Embedding(woman)的距离向量几乎一致</li>
</ul>
</li>
<li><p>电影</p>
<ul>
<li>Embedding(复仇者联盟)和Embedding(钢铁侠)在Embedding向量空间内两点之间的距离应该很近</li>
</ul>
</li>
<li><p>电商</p>
<ul>
<li>Embedding(键盘)和Embedding(鼠标)应该距离很近</li>
</ul>
</li>
<li><p>对深度学习推荐系统的重要性</p>
<ul>
<li>推荐场景中大量使用one-hot编码对类别、ID类特征进行编码，导致样本特征向量极度稀疏，而深度学习的结构特点使其不利于稀疏特征向量的处理，因此几乎所有深度学习推荐模型都会由Embedding层负责将高维稀疏特征向量转换成稠密低维特征向量</li>
<li>相比MF等传统方法产生的特征向量，Embedding的表达能力更强，特别是Graph Embedding技术提出后，Embedding几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息</li>
<li>Embedding对物品、用户相似度的计算是常用的推荐系统召回层技术。在局部敏感哈希等快速最近邻搜索技术应用于推荐系统后，Embedding更适用于对海量被选物品进行快速初筛，过滤出几百到几千量级的物品交由深度学习网络进行精排</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Word2vec-经典的Embedding方法"><a href="#Word2vec-经典的Embedding方法" class="headerlink" title="Word2vec-经典的Embedding方法"></a>Word2vec-经典的Embedding方法</h3><ul>
<li><p>什么是Word2vec</p>
<ul>
<li><p>CBOW模型</p>
<ul>
<li>假定每个词都跟其相邻的词的关系最密切</li>
<li>输入是w_t周边的词，输出是w_t</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Skip-gram模型</p>
<ul>
<li>每个词决定了相邻的词</li>
<li>输入是w_t，输出是周边的词</li>
<li>Skip-gram的效果较好</li>
</ul>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092355460.png" alt="image-20220315092355460"></p>
</li>
</ul>
<ul>
<li><p>训练过程</p>
<ul>
<li><p>训练样本准备</p>
<ul>
<li>选取一个长度为2c+1（目标词前后各选c个词）的滑动窗口，从语料库中抽取一个句子，将滑动窗口从左到右滑动，每移动一次，窗口中的词组就形成一个训练样本</li>
</ul>
</li>
<li><p>极大似然估计与softmax函数-多分类</p>
</li>
<li><p>得到词向量</p>
<ul>
<li><p>神经网络结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092442798.png" alt="image-20220315092442798"></p>
</li>
<li><p>得到词向量</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092506516.png" alt="image-20220315092506516"></p>
</li>
</ul>
</li>
<li><p>负采样训练方法</p>
<ul>
<li><p>问题</p>
<ul>
<li>假设语料库中的词数量为10000，这就意味着输出层神经元有10000个，在每次迭代更新隐层到输出层神经元的权重时，都需要计算所有字典中的所有10000个词的预测误差</li>
</ul>
</li>
<li><p>解决办法</p>
<ul>
<li>采用负采样的方法进行训练，具体是w_t作为正样本的目标，而w_t之外的词则可以作为负样本，这样优化目标就从一个多分类问题退化成一个近似二分类问题</li>
</ul>
</li>
</ul>
</li>
<li><p>层级softmax</p>
</li>
</ul>
</li>
<li><p>意义</p>
<ul>
<li>奠基性意义</li>
</ul>
</li>
</ul>
<h3 id="Item2vec-Word2vec在推荐系统领域的推广"><a href="#Item2vec-Word2vec在推荐系统领域的推广" class="headerlink" title="Item2vec-Word2vec在推荐系统领域的推广"></a>Item2vec-Word2vec在推荐系统领域的推广</h3><ul>
<li><p>基本原理</p>
<ul>
<li>将句子序列改为用户浏览的物品序列</li>
<li>Item2vec与Word2vec唯一不同在于，Item2vec摒弃了时间窗口的概念，认为序列中任意两个物品都相关</li>
</ul>
</li>
<li><p>广义的Item2vec</p>
<ul>
<li><p>双塔模型</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092624653.png" alt="image-20220315092624653"></p>
</li>
</ul>
</li>
<li><p>特点与局限性</p>
<ul>
<li>可以利用任何序列数据生成物品Embedding向量</li>
<li>只能利用序列型数据，其他信息浪费了</li>
</ul>
</li>
</ul>
<h3 id="GraphEmbedding-引入更多结构信息的图嵌入技术"><a href="#GraphEmbedding-引入更多结构信息的图嵌入技术" class="headerlink" title="GraphEmbedding-引入更多结构信息的图嵌入技术"></a>GraphEmbedding-引入更多结构信息的图嵌入技术</h3><ul>
<li><p>在互联网场景下，数据对象之间更多呈现的是图结构</p>
<ul>
<li><p>典型的场景是由用户行为数据生成的物品关系图，以及由属性和实体组成的知识图谱</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092700409.png" alt="image-20220315092700409"></p>
</li>
</ul>
</li>
<li><p>GraphEmbedding是一种对图结构中的节点进行Embedding编码的方法</p>
<ul>
<li><p>DeepWalk-基础的GraphEmbedding方法</p>
<ul>
<li>主要思想是由物品组成的图结构上进行随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入Word2vec进行训练，得到物品的Embedding</li>
<li><p>算法流程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092730446.png" alt="image-20220315092730446"></p>
</li>
<li><p>随机游走的跳转概率</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315092805313.png" alt="image-20220315092805313"></p>
</li>
</ul>
</li>
<li><p>Node2vec-同质性和结构性的权衡</p>
<ul>
<li><p>通过调整随机游走权重的方法使GraphEmbedding的结果更倾向于体现网络的同质性和结构性</p>
<ul>
<li><p>同质性</p>
<ul>
<li>指的是距离相近节点的Embedding应尽量相似</li>
<li>为了表达同质性，需要让随机游走的过程更倾向于深度优先搜索，因为DFS更有可能通过多次跳转，游走到远方的节点上，但无论怎样，DFS但游走更大概率会在一个大的集团内部进行，这就使得一个集团或者社区内部的节点的Embedding更为相似，从而更多地表达网络同质性</li>
</ul>
</li>
<li><p>结构性</p>
<ul>
<li>指的是结构上相似的节点的Embedding应尽量近似</li>
<li>为了表达结构性，在随机游走过程中，需要让游走过程更倾向于宽度优先搜索，因为BFS会更多地在当前节点的邻域内游走遍历，相当于对当前节点周边的网络结构进行一次微观扫描。当前节点是局部中心节点，还是边缘节点，或是连接性节点，其生成的序列包含的节点数量和顺序必然是不同的，从而让最终embedding抓取到更多结构性信息</li>
</ul>
</li>
</ul>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315093449276.png" alt="image-20220315093449276"></p>
</li>
<li><p>如何控制BFS和DFS的倾向性呢？</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315093603660.png" alt="image-20220315093603660"></p>
<ul>
<li>通过控制节点间的跳转概率</li>
<li>其中d_tx指节点t到节点x的距离</li>
<li><p>参数p和q共同控制着随机游走的倾向性</p>
<ul>
<li>参数p称为返回参数，p越小，随机游走回节点t的可能性越大，就更注重表达网络的结构性</li>
<li>参数q被称为进出参数，q越小，随机游走到远方节点的可能性越大，就更注重网络的同质性</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>- 实验结果

  ![image-20220315093649690](%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20220315093649690.png)
</code></pre><ul>
<li><p>EGES-阿里巴巴的综合性GraphEmbedding方法</p>
<ul>
<li>基本思想是在DeepWalk生成的GraphEmbedding基础上引入补充信息</li>
<li><p>单纯使用用户行为生成的物品相关图所产生的问题？</p>
<ul>
<li>如果遇到新加入的物品，或者没有过过多互动信息的长尾物品，则推荐系统将出现严重的冷启动问题</li>
</ul>
</li>
<li><p>解决问题</p>
<ul>
<li>第一步通过用户行为序列生成物品关系图</li>
<li><p>利用相同属性、相同类型等信息建立物品之间等边，生成基于内容的知识图谱</p>
<ul>
<li>基于知识图谱生成的物品向量可以被称为补充信息Embedding向量</li>
<li>当然，根据补充信息类别的不同，可以有多个补充信息Embedding向量</li>
</ul>
</li>
</ul>
</li>
<li><p>如何融合一个物品的多个Embedding向量呢？</p>
<ul>
<li><p>最简单的方法是在深度神经网络中加入平均池化层，将不同Embedding平均起来</p>
</li>
<li><p>为了防止简单平均池化导致有效Embedding信息的丢失，阿里巴巴在此基础上对每个Embedding向量加上权重（类似DIN模型的注意力机制），如图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315093829635.png" alt="image-20220315093829635"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>LINE</p>
</li>
<li><p>SDNE</p>
</li>
</ul>
<h3 id="Embedding与深度学习推荐系统的结合"><a href="#Embedding与深度学习推荐系统的结合" class="headerlink" title="Embedding与深度学习推荐系统的结合"></a>Embedding与深度学习推荐系统的结合</h3><ul>
<li><p>Embedding技术主要应用在如下三个方向</p>
<ul>
<li><p>深度学习网络中的Embedding层</p>
<ul>
<li>完成从高维稀疏特征向量到低维稠密向量的转换</li>
<li><p>Deep Crossing、FNN、Wide&amp;Deep模型的Embedding层</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094115394.png" alt="image-20220315094115394"></p>
</li>
<li><p>Embedding层的图示和矩阵表达</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094130905.png" alt="image-20220315094130905"></p>
</li>
<li><p>将Embedding层与整个深度学习网络整合后一同进行训练是理论上最优的选择，但是这样做的缺点是Embedding层输入向量的维度往往很大，导致参数数量巨大，会拖慢整个神经网络的收敛速度</p>
<ul>
<li>因此在工程上大多采用预训练Embedding层的方法替代</li>
</ul>
</li>
</ul>
</li>
<li><p>作为预训练的Embedding特征向量</p>
<ul>
<li>为了解决Embedding层训练开销巨大的问题，Embedding的训练往往是独立于深度学习网络进行</li>
<li><p>例如FNN模型，则是将FM模型训练得到的各特征隐向量作为Embedding层的初始化权重，从而加快整个网络的收敛速度</p>
<ul>
<li>在FNN原始实现中，整个梯度下降过程还是会更新Embedding的权重</li>
<li>如果想进一步加快网络收敛速度，可以采取固定Embedding层权重，仅更新其他层权重的方法</li>
</ul>
</li>
<li><p>虽然将Embedding过程和深度学习网络剥离开会损失一定的信息，但是训练过程的独立带来训练灵活性的提升</p>
<ul>
<li>例如物品或用户的Embedding是比较稳定的（因为用户的兴趣、物品的属性不可能在短时间内发生巨大变化），Embedding的训练频率其实不需要很高，甚至可以降低到周的级别。使用不同训练频率更新Embedding模型和神经网络模型，是训练开销和模型效果二者之间权衡后最优方案</li>
</ul>
</li>
</ul>
</li>
<li><p>通过计算用户和物品的Embedding相似度</p>
<ul>
<li>Embedding作为推荐系统召回层的方法</li>
<li><p>YouTube推荐系统召回层模型的结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094203971.png" alt="image-20220315094203971"></p>
</li>
<li><p>在模型部署过程中，没有必要部署整个深度网络来完成从原始特征向量到最终输出的预测过程。<br>只需要将用户Embedding和物品Embedding存储到线上内存数据库，通过内积运算再排序的方法就可以得到物品的排序</p>
<ul>
<li><p>但是在整体候选集量级很大的情况下，通过遍历内积运算也会消耗大量计算时间，导致线上推断的延迟。那么怎么实现快速召回呢？</p>
<ul>
<li><p>局部敏感哈希-让Embedding插上翅膀的快速搜索</p>
<ul>
<li><p>快速Embedding最近邻搜索-KD树</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094314439.png" alt="image-20220315094314439"></p>
</li>
<li><p>局部敏感哈希</p>
<ul>
<li><p>基本原理</p>
<ul>
<li>基本思想是让相邻的点落入到同一个桶，这样最近邻搜索时就只需要搜索该桶或者几个桶，能实现常数时间级别的搜索</li>
<li><p>高维空间点向低维空间映射图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094327301.png" alt="image-20220315094327301"></p>
</li>
<li><p>根据哈希函数进行分桶</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094416951.png" alt="image-20220315094416951"></p>
</li>
<li><p>仅凭一个哈希函数来分桶，会存在相近点误判的情况，有效的解决办法是采用m个哈希函数同时进行分桶，同时掉进m个哈希函数的同一个桶的两点，相似概率大大增加</p>
<ul>
<li><p>多桶策略</p>
<ul>
<li><p>多桶如何生成最终候选集，AND OR问题</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094435370.png" alt="image-20220315094435370"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="总结-深度学习推荐系统的核心操作"><a href="#总结-深度学习推荐系统的核心操作" class="headerlink" title="总结-深度学习推荐系统的核心操作"></a>总结-深度学习推荐系统的核心操作</h3><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094454222.png" alt="image-20220315094454222"></p>
<h2 id="ch5-多角度审视推荐系统"><a href="#ch5-多角度审视推荐系统" class="headerlink" title="ch5_多角度审视推荐系统"></a>ch5_多角度审视推荐系统</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><ul>
<li>推荐模型的作用是重要的，但是它不是推荐系统的全部</li>
</ul>
<h3 id="推荐系统的特征工程"><a href="#推荐系统的特征工程" class="headerlink" title="推荐系统的特征工程"></a>推荐系统的特征工程</h3><ul>
<li>Garbage in garbage out</li>
<li><p>构建特征工程应该遵循的基本原则</p>
<ul>
<li>特征的本质其实是对某个行为过程相关信息对抽象表达</li>
<li><p>尽可能的让特征工程抽取出的一组特征能够保留推荐环境及用户行为过程中的所有有用信息，尽量摒弃冗余信息</p>
<ul>
<li><p>例如：电影推荐场景下，应该如何抽取特征才能代表用户点击某个电影这一行为呢</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315174654189.png" alt="image-20220315174654189"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>常用的特征类别</p>
<ul>
<li><p>用户行为数据</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315174801415.png" alt="image-20220315174801415"></p>
<ul>
<li>人与物的连接日志</li>
<li>将代表用户行为的物品ID序列转成multi-hot向量，将其作为特征向量</li>
<li>预训练好物品的Embedding，再通过平均或者类似于DIN模型注意力机制的方法生成历史行为Embedding向量，将其作为特征向量</li>
</ul>
</li>
<li><p>用户关系数据</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315174954514.png" alt="image-20220315174954514"></p>
<ul>
<li>人与人的连接日志</li>
<li>可以将用户关系作为召回层的一种物品召回方式</li>
<li>也可以通过用户关系建立关系图，使用GraphEmbedding的方法生成用户和物品的Embedding</li>
<li>还可以直接利用关系数据，通过好友的特征为用户添加新的属性特征</li>
<li>甚至可以利用用户关系数据直接建立社会化推荐系统</li>
</ul>
</li>
<li><p>属性、标签类数据</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315175012158.png" alt="image-20220315175012158"></p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315175022129.png" alt="image-20220315175022129"></p>
<ul>
<li><p>人口属性数据（性别、年龄、住址等）<br>用户兴趣标签</p>
<ul>
<li>用户注册信息、第三方DMP、用户选择</li>
</ul>
</li>
<li><p>物品标签<br>物品属性（商品类别、价格、电影分类、年代、演员、导演等）</p>
<ul>
<li>用户或者系统添加、后天录入、第三方DMP</li>
</ul>
</li>
</ul>
</li>
<li><p>内容类数据</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315175045899.png" alt="image-20220315175045899"></p>
<ul>
<li>内容类数据无法直接转换成推荐系统可以消化的特征，需要通过自然语言处理、计算机视觉等技术手段提取关键内容特征，再输入推荐系统</li>
</ul>
</li>
<li><p>上下文信息</p>
<ul>
<li>上下文信息是描述推荐行为产生的场景信息，最常用的上下文信息是时间和通过GPS获得的地点信息</li>
<li>包含但不限于时间、地点、季节、月份、是否节假日、天气、空气质量、社会大事件等信息</li>
</ul>
</li>
<li><p>统计类特征</p>
<ul>
<li>指通过统计方法计算出的特征</li>
<li>例如历史CTR、历史CVR、物品热门程度、物品流程程度等</li>
<li>统计类特征一般是连续型特征，仅需经过标准化归一化等处理就可以直接输入推荐系统进行训练</li>
</ul>
</li>
<li><p>组合类特征</p>
<ul>
<li>组合类特征是指将不同特征进行组合后生成的新特征</li>
<li>最常见的是年龄+性别组成的人口属性分段特征</li>
</ul>
</li>
</ul>
</li>
<li><p>常用的特征处理方法</p>
<ul>
<li>如何在原始特征上进行处理，生成可供推荐系统的特征向量</li>
<li><p>连续型特征</p>
<ul>
<li><p>归一化</p>
<ul>
<li>目的是统一各特征的量纲</li>
</ul>
</li>
<li><p>离散化</p>
<ul>
<li>通过确定分位数的形式将原来的连续值进行分桶，最终形成离散值的过程</li>
<li>主要目的是防止连续值带来的过拟合现象及特征值不均匀的情况</li>
</ul>
</li>
<li><p>加非线性函数</p>
<ul>
<li>直接将原来的特征通过非线性函数做变换，然后将原来的特征及变换后的特征一起加入模型进行训练的过程</li>
<li>加非线性函数的目的是更好的捕获特征与优化目标之间的线性关系</li>
</ul>
</li>
</ul>
</li>
<li><p>类别型特征</p>
<ul>
<li>类别型特征的典型例子是用户的历史行为数据、属性标签类数据等</li>
<li>使用one-hot编码将其转换成一个数值向量</li>
<li><p>面对同一特征域非唯一的类别选择，可以采用multi-hot编码</p>
<ul>
<li>进行one-hot和multi-hot编码的主要问题是特征向量维度过大，特征过于稀疏，容易造成模型欠拟合，模型的权重参数数量过多，导致模型收敛过慢</li>
</ul>
</li>
<li><p>可以将类别型特征转成Embedding向量，再与其他特征组合</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>特征工程与业务理解</p>
<ul>
<li>传统的人工特征组合、过滤的工作已经不存在了，取而代之的是将特征工程与模型结构统一思考、整体建模的深度学习模式</li>
<li>不变的是，只有深入了解业务的运行模式，了解用户在业务场景下思考方式和行为动机，才能精确地抽取出最有价值的特征，构建成功的深度学习模型</li>
</ul>
</li>
</ul>
<h3 id="推荐系统召回层的主要策略"><a href="#推荐系统召回层的主要策略" class="headerlink" title="推荐系统召回层的主要策略"></a>推荐系统召回层的主要策略</h3><ul>
<li><p>召回层和排序层的功能特点</p>
<ul>
<li><p>召回</p>
<ul>
<li>召回阶段负责将海量的候选集快速缩小为几百到几千的规模</li>
<li>利用少量的特征和简单的模型或规则进行候选集的快速筛选，减少精准排序阶段的时间开销</li>
<li>待计算的候选集数量大、速度快、模型简单、特征较少，尽量让用户感兴趣的物品在这个阶段被快速召回，即保证相关物品的召回率</li>
<li>设计召回层时，计算速度和召回率其实是矛盾的两个指标，为提高计算速度，需要使召回策略尽量简单；而提高召回率，要求召回策略能够尽量选出排序模型需要的候选集，这要求召回策略不能过于简单。</li>
</ul>
</li>
<li><p>排序</p>
<ul>
<li>排序阶段则负责对缩小后的候选集进行精准排序</li>
<li>排序阶段一般使用复杂模型，利用多特征进行精准排序</li>
<li>首要目标是得到精准的排序结果。需处理的物品数量少，可利用较多特征，使用比较复杂的模型</li>
</ul>
</li>
<li><p>如图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315175557939.png" alt="image-20220315175557939"></p>
</li>
</ul>
</li>
<li><p>多路召回策略</p>
<ul>
<li>指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用的策略</li>
<li>多路召回策略是计算速度和召回率之间进行权衡的结果</li>
<li><p>常见的多路召回策略</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315175611709.png" alt="image-20220315175611709"></p>
</li>
<li><p>每一路召回策略会拉回K个候选物品，对于不同的召回策略，K值可以选择不同的大小</p>
<ul>
<li>这里的K值是超参数，一般需要通过离线评估加线上ABtest的方式确定合理的取值范围</li>
</ul>
</li>
<li><p>局限性</p>
<ul>
<li>虽然多路召回是实用的工程方法</li>
<li>但是策略选择到候选集大小参数的调整都需要人工参与，策略之间的信息也是割裂的，无法综合考虑不同策略对于一个物品的影响</li>
</ul>
</li>
</ul>
</li>
<li><p>基于Embedding的召回方法</p>
<ul>
<li><p>通过计算用户和物品的Embedding相似度</p>
<ul>
<li>Embedding作为推荐系统召回层的方法</li>
</ul>
</li>
</ul>
<ul>
<li><p>YouTube推荐系统召回层模型的结构图</p>
<p>  <img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094203971.png" alt="image-20220315094203971"></p>
</li>
<li><p>在模型部署过程中，没有必要部署整个深度网络来完成从原始特征向量到最终输出的预测过程。<br>只需要将用户Embedding和物品Embedding存储到线上内存数据库，通过内积运算再排序的方法就可以得到物品的排序</p>
<ul>
<li><p>但是在整体候选集量级很大的情况下，通过遍历内积运算也会消耗大量计算时间，导致线上推断的延迟。那么怎么实现快速召回呢？</p>
<ul>
<li><p>局部敏感哈希-让Embedding插上翅膀的快速搜索</p>
<ul>
<li><p>快速Embedding最近邻搜索-KD树</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315094314439.png" alt="image-20220315094314439"></p>
</li>
<li><p>局部敏感哈希</p>
<ul>
<li><p>基本原理</p>
<pre><code>- 基本思想是让相邻的点落入到同一个桶，这样最近邻搜索时就只需要搜索该桶或者几个桶，能实现常数时间级别的搜索

- 高维空间点向低维空间映射图

  ![image-20220315094327301](%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20220315094327301.png)

- 根据哈希函数进行分桶

  ![image-20220315094416951](%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20220315094416951.png)

- 仅凭一个哈希函数来分桶，会存在相近点误判的情况，有效的解决办法是采用m个哈希函数同时进行分桶，同时掉进m个哈希函数的同一个桶的两点，相似概率大大增加

  - 多桶策略

    - 多桶如何生成最终候选集，AND OR问题

      ![image-20220315094435370](%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20220315094435370.png)
</code></pre></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>优势</p>
</li>
<li><p>将多路召回中使用到的兴趣标签、热门度、流行趋势、物品属性等信息作为附加信息融合到最终的Embedding中，相当于考虑了多路召回的多种策略</p>
<ul>
<li><p>Embedding召回的另一个优势是评分的连续性</p>
<ul>
<li>多路召回中不同召回策略产生的相似度、热度评分值不具备可比性，无法据此决定每个召回策略放回候选集的大小</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="推荐系统的实时性"><a href="#推荐系统的实时性" class="headerlink" title="推荐系统的实时性"></a>推荐系统的实时性</h3><ul>
<li><p>为什么推荐系统实时性重要</p>
<ul>
<li><p>从用户体验角度来说</p>
<ul>
<li>用户期望更快找到自己感兴趣的东西</li>
<li>只有根据用户反馈、实时的满足用户期望目标</li>
</ul>
</li>
<li><p>从机器学习角度</p>
<ul>
<li>推荐系统更新速度越快，代表用户最近习惯和爱好的特征更新越快，越能为用户进行更有效的推荐</li>
<li>推荐系统更新越快，模型越能发现最新流行的数据模式，越能让模型快速抓住新流行趋势</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐系统特征实时性</p>
<ul>
<li>特征实时性指的是实时的收集和更新模型的输入特征，使推荐系统总能使用最新特征进行预测和推荐</li>
<li><p>推荐系统的数据流架构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315180037473.png" alt="image-20220315180037473"></p>
</li>
<li><p>影响特征实时性的三个主要阶段</p>
<ul>
<li>客户端实时特征</li>
<li>流计算平台的准实时特征处理</li>
<li><p>分布式批处理平台的全量特征处理</p>
<ul>
<li>模型训练和离线评估</li>
<li>特征保存入特征数据库，供之后线上推荐模型使用</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐系统模型实时性</p>
<ul>
<li>特征实时性会根据用户最近行为更快发现用户可能感兴趣商品，但绝对不会发现一个刚刚流行起来的爆款商品、一个刚刚开始的促销活动、以及该用户相似人群的最新偏好</li>
<li>要发现这类全局性的数据变化，需要实时的更新推荐模型</li>
<li><p>模型更新方式</p>
<ul>
<li><p>全量更新</p>
<ul>
<li>模型利用某段时间内的所有训练样本进行训练</li>
</ul>
</li>
<li><p>增量更新</p>
<ul>
<li>仅将新加入的样本给模型进行增量训练</li>
<li>缺点就是模型往往无法找到全局最优点</li>
<li>因此在实际业务中，经常采用增量更新+全局更新相结合的方式，进行几轮增量后，再进行全局，再增量</li>
</ul>
</li>
<li><p>在线学习</p>
<ul>
<li>在线学习是模型实时更新的主要方法，也就是在获得一个新样本的同时更新模型</li>
<li><p>缺点</p>
<ul>
<li>线上进行模型训练和大量模型参数存储和更新，工程要求高</li>
<li><p>模型稀疏性不强，导致模型体量大</p>
<ul>
<li>使用FTRL方法解决</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>局部更新</p>
<ul>
<li>降低训练效率低的部分更新频率</li>
<li>提高训练效率高的部分更新频率</li>
<li>例如GBDT+LR模型，GBDT每天更新，LR实时更新</li>
<li>较多应用在Embedding+神经网络的模型中，Embedding层更新慢一点，神经网络层更新频繁一点</li>
</ul>
</li>
<li><p>客户端模型实时更新</p>
<ul>
<li>如果用户Embedding是由用户点击过的物品Embedding进行平均得到，那么最先得到用户最新点击物品信息的客户端，就可以根据用户点击物品的Embedding实时更新用户Embedding，并保存该Embedding。在下次推荐时，将该Embedding传给服务器，服务器进行计算和返回</li>
</ul>
</li>
</ul>
</li>
<li><p>模型实时性与训练方式关系</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315180104240.png" alt="image-20220315180104240"></p>
</li>
<li><p>用木桶理论看到推荐系统的迭代升级</p>
<ul>
<li>找到拖慢推荐系统实时性的短板，替换或者改进它</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="推荐系统优化目标"><a href="#推荐系统优化目标" class="headerlink" title="推荐系统优化目标"></a>推荐系统优化目标</h3><ul>
<li><p>YouTube以观看时长为优化目标</p>
<ul>
<li>分类问题，理论上很难预测播放时长，但是YouTube巧妙地把播放时长转换成正样本的权重，输出层则用加权逻辑回归进行训练</li>
<li><p>YouTube推荐模型输出层</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315180604599.png" alt="image-20220315180604599"></p>
</li>
</ul>
</li>
<li><p>模型优化与应用场景统一性</p>
<ul>
<li>优化目标的制定还应该考虑的要素是模型优化场景和应用场景的统一性</li>
<li><p>阿里多目标优化模型</p>
<ul>
<li>对电商类网站而言，公司的商业目标是通过推荐时用户产生更多购买</li>
<li><p>按照优化目标应与公司商业目标一致的原则，电商类推荐模型应该是一个CVR预估模型</p>
<ul>
<li><p>但是用户登录后直接看到的并不是具体的商品详情页，这会导致CVR模型的训练场景和预估场景不一致，如图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315180646279.png" alt="image-20220315180646279"></p>
</li>
<li><p>为了同时优化CTR和CVR模型，阿里提出多目标优化模型ESMM，它是一个同时模拟曝光点击和点击转化两个阶段的模型</p>
</li>
<li><p>ESMM模型结构图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220315180712959.png" alt="image-20220315180712959"></p>
<ul>
<li>从模型结构来看，底层的Embedding层是CTR和CVR共享的。共享的目的是解决CVR任务正样本稀疏的问题，利用CTR的数据生成更准确的用户和物品的特征表达</li>
<li>中间层是CTR和CVR部分，各自利用完全隔离的神经网络拟合自己的优化目标-pctr和pcvr。最终将其相乘得到pctcvr</li>
</ul>
</li>
<li><p>阿里通过构建ESMM多目标模型同时解决了“训练空间和预测空间不一致”以及“同时利用点击和转化数据进行全局优化”这两个关键问题</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="推荐系统中比模型结构更重要的是什么？"><a href="#推荐系统中比模型结构更重要的是什么？" class="headerlink" title="推荐系统中比模型结构更重要的是什么？"></a>推荐系统中比模型结构更重要的是什么？</h3><ul>
<li><p>DIEN模型对阿里为什么有效？</p>
<ul>
<li>应用场景存在兴趣的进化</li>
<li>用户兴趣的进化过程能够完整捕获</li>
</ul>
</li>
<li><p>在构建推荐模型的过程中，从应用场景出发，基于用户行为和数据的特点，提出合理的改进模型的动机才是最重要的</p>
</li>
</ul>
<h3 id="冷启动的解决办法"><a href="#冷启动的解决办法" class="headerlink" title="冷启动的解决办法"></a>冷启动的解决办法</h3><ul>
<li><p>类别</p>
<ul>
<li>物品冷启动</li>
<li>用户冷启动</li>
<li>系统冷启动</li>
</ul>
</li>
<li><p>解决办法</p>
<ul>
<li><p>基于规则</p>
<ul>
<li>热门排行榜</li>
<li>最近流行趋势</li>
<li>最高评分</li>
<li>基于年龄、性别、IP等信息粗粒度规则推荐</li>
<li><p>局限性</p>
<ul>
<li>该过程与推荐系统的主模型是割裂的</li>
</ul>
</li>
</ul>
</li>
<li><p>丰富冷启动过程中可获得的用户和物品特征</p>
<ul>
<li>改进的方法是通过在模型中加入更多用户或物品的属性特征，而非历史数据特征</li>
<li><p>属性特征包含</p>
<ul>
<li><p>用户的注册信息</p>
<ul>
<li>年龄</li>
<li>性别</li>
<li>学历</li>
<li>职业</li>
<li>地理信息</li>
</ul>
</li>
<li><p>第三方DMP库提供的用户信息</p>
</li>
<li><p>物品的内容特征</p>
<ul>
<li>物品分类</li>
<li>标签</li>
<li>描述文字</li>
</ul>
</li>
<li><p>引导用户输入的冷启动特征</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>利用主动学习、迁移学习和“探索与利用”机制</p>
<ul>
<li><p>主动学习</p>
<ul>
<li><p>被动学习与主动学习流程图</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321201539332.png" alt="image-20220321201539332"></p>
</li>
</ul>
</li>
<li><p>迁移学习</p>
<ul>
<li>迁移学习是在某领域知识不足的情况下，迁移其他领域的数据或者知识，用于本领域的学习</li>
<li>例如阿里的ESMM模型用CTR数据生成用户和物品的Embedding，然后共享给CVR模型，这使得CVR模型在没有转化数据时能够用CTR模型的知识来完成冷启动</li>
</ul>
</li>
<li><p>探索与利用机制</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321201915748.png" alt="image-20220321201915748"></p>
<ul>
<li><p>传统方法</p>
<ul>
<li><p>e-greedy</p>
<ul>
<li>选一个[0,1]的数e，每次以e的概率在所有老虎机中随机选择，以1-e的概率选择截止当前平均收益最大的老虎机，在摇臂后，对回报期望进行更新</li>
<li>比较粗暴和生硬，探索一段时间后，再进行探索的意义没之前大了</li>
</ul>
</li>
<li><p>thompson sampling</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321202035129.png" alt="image-20220321202035129"></p>
<ul>
<li>假设每个老虎机能够赢钱的概率为p，同时p服从beta(win,lose)分布，每次试验后，选中一个老虎机，摇臂后，有收益，win+1，无收益，lose+1</li>
<li>每次选择老虎机的方式：利用老虎机现有beta分布产生一个随机数，选择随机数最大那个老虎机进行尝试</li>
<li><p>它的好处</p>
<ul>
<li><p>能够对新物品友好</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321202405890.png" alt="image-20220321202405890"></p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321202520348.png" alt="image-20220321202520348"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>ucb</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321202744640.png" alt="image-20220321202744640"></p>
<ul>
<li>假设有K个老虎机，每个老虎机摇臂m次，获得老虎机j的期望x_j</li>
<li>用t表示至今摇臂总次数，n_j表示老虎机j至今摇臂次数，则每个老虎机的ucb值为：ucb(j)=x_j+\sqrt(2logt/n_j)</li>
<li>选择ucb值最大的老虎机摇臂，更新该老虎机收益期望</li>
<li>重复上述步骤</li>
</ul>
</li>
<li><p>弊端</p>
<ul>
<li>无法引入用户上下文和个性化信息，只能进行全局性的探索</li>
</ul>
</li>
</ul>
</li>
<li><p>个性化方法</p>
<ul>
<li><p>基于上下文的多臂老虎机算法</p>
</li>
<li><p>LinUCB</p>
<ul>
<li><p>线性模型的数学形式</p>
</li>
<li><p>训练模型参数</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321203105756.png" alt="image-20220321203105756"></p>
</li>
<li><p>探索部分</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321203302905.png" alt="image-20220321203302905"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>      - 算法伪代码

        ![image-20220321203345842](%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20220321203345842.png)

      - 局限性

          - 需要严格的理论支撑才能得到预测标准差的具体形式，深度学习的数学形式难以被正确表达，几乎不可能通过严格理论推导得到

  - 基于模型的方法

      - 如果CTR预测模型或者推荐模型是深度模型，那么如何将探索与利用的思想与模型进行整合呢？
      - 回顾强化学习模型DRN，对于已经训练好的当前网络Q，通过对其模型参数W添加一个较小的随机扰动，得到新的模型参数，这个新的网络称为探索网络，再通过系统的实时反馈决定保留探索网络还是沿用当前网络Q

  - 探索与利用在推荐系统中的应用

      - 物品冷启动
      - 发掘用户新兴趣
      - 增加结果多样性
</code></pre><h2 id="ch6-深度学习推荐系统的工程实现"><a href="#ch6-深度学习推荐系统的工程实现" class="headerlink" title="ch6_深度学习推荐系统的工程实现"></a>ch6_深度学习推荐系统的工程实现</h2><h3 id="推荐系统的数据流"><a href="#推荐系统的数据流" class="headerlink" title="推荐系统的数据流"></a>推荐系统的数据流</h3><ul>
<li><p>批处理大数据架构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321203827047.png" alt="image-20220321203827047"></p>
<ul>
<li>分布式存储HDFS</li>
<li>mapreduce批量处理以及落盘的静态数据</li>
</ul>
</li>
<li><p>流计算</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321203852180.png" alt="image-20220321203852180"></p>
<ul>
<li><p>滑动窗口</p>
<ul>
<li>在每个“窗口”内，数据被短暂缓存并消费</li>
<li>延迟仅与“窗口”的大小有关</li>
<li>“窗口”的大小基本以分钟级别居多</li>
</ul>
</li>
<li><p>知名开源流计算平台</p>
<ul>
<li>storm</li>
<li>spark streaming</li>
<li><p>flink</p>
<ul>
<li>把所有数据看作“流”，批处理只是流处理的一种特殊情况</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>lambda</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321204107630.png" alt="image-20220321204107630"></p>
<ul>
<li><p>实时流</p>
<ul>
<li>保障数据实时性更多是以增量计算为主</li>
</ul>
</li>
<li><p>离线处理</p>
<ul>
<li>对数据进行全量运算保障最终一致性</li>
</ul>
</li>
</ul>
</li>
<li><p>kappa</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220321204454564.png" alt="image-20220321204454564"></p>
<ul>
<li>为解决lambda架构的代码冗余问题而产生</li>
<li><p>通过同样的流计算框架来实现批处理，就是流计算和批处理用同样计算逻辑</p>
<ul>
<li><p>如何在离线环境下利用同样的流计算框架来实现批处理？</p>
<ul>
<li><p>原始数据存储</p>
<ul>
<li>将未经流处理的日志或数据原封不动的保存到分布式文件系统</li>
</ul>
</li>
<li><p>数据广播</p>
<ul>
<li>将这些原始数据按时间顺序进行重播，并用同样的流计算框架进行处理</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>大数据平台与推荐系统的整合</p>
<ul>
<li>以HDFS为代表的离线海量数据存储平台，主要负责存储离线训练用的训练样本</li>
<li>以Redis为代表的在线实时特征数据库，主要负责为模型的在线服务提供实时特征</li>
</ul>
</li>
</ul>
<h3 id="推荐模型离线训练"><a href="#推荐模型离线训练" class="headerlink" title="推荐模型离线训练"></a>推荐模型离线训练</h3><ul>
<li><p>Spark MLlib</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322193941393.png" alt="image-20220322193941393"></p>
<ul>
<li><p>分布式计算、计算节点间不共享内存，需要通过网络通信来交换数据</p>
</li>
<li><p>架构</p>
<ul>
<li>Cluster Manager（集群管理节点）进行调度</li>
<li><p>Worker Node（工作节点）进行具体计算任务执行</p>
<ul>
<li>数据可能分为不同的Partition（数据分片）</li>
</ul>
</li>
<li><p>最终结果返回Driver Program（驱动程序）</p>
</li>
</ul>
</li>
<li><p>执行时，会将程序拆解成一个任务DAG（有向无环图）</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322194010253.png" alt="image-20220322194010253"></p>
<ul>
<li><p>并行处理部分</p>
<ul>
<li>map</li>
<li>filter</li>
</ul>
</li>
<li><p>不能并行部分</p>
<ul>
<li>join</li>
<li>group by</li>
<li>reduce</li>
<li><p>shuffle</p>
<ul>
<li>需要在不同节点之间交换数据，非常消耗计算、通信和存储资源，应尽量避免</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>并行训练原理</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322194107114.png" alt="image-20220322194107114"></p>
<ul>
<li><p>spark并行梯度下降法的原理</p>
<ul>
<li>把当前模型参数广播到各个数据Partition</li>
<li>各计算节点进行数据抽样得到mini_batch数据，分别计算梯度，再通过treeAggregate操作汇总梯度，得到最终梯度gradientSum</li>
<li>利用最终梯度更新模型权重</li>
</ul>
</li>
</ul>
</li>
<li><p>并行训练局限性</p>
<ul>
<li>采用全局广播的方式，在每轮迭代前广播模型全部参数，当参数过多时，耗资源</li>
<li>采用同步阻断式的梯度下降方式，每轮梯度下降由最慢节点决定，如果某个节点因数据倾斜导致计算时间过长，则会阻断其他所有节点执行新任务</li>
<li>不支持复杂深度学习网络结构和大量可调超参，只支持标准的MLP训练，不支持RNN，LSTM等复杂网络</li>
</ul>
</li>
</ul>
</li>
<li><p>参数服务器-Parameter Server</p>
<ul>
<li><p>几乎完美解决机器学习模型分布式训练问题</p>
</li>
<li><p>架构</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322194412618.png" alt="image-20220322194412618"></p>
</li>
</ul>
</li>
</ul>
<pre><code>- 资源管理中心（resource　manager）

  - 负责总体资源分配调度

![image-20220322194335127](%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/image-20220322194335127.png)

- 服务器节点组（server　group）

  - server　node

    - 多个server　node，每个负责维护一部分参数

  - server　manager

    - 负责维护和分配server资源

- 多个工作节点组（worker　group）

  - 每个节点对应一个模型训练任务
  - 工作节点组之间不通信，任务节点只与server通信
</code></pre><ul>
<li><p>分布式训练原理</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322194528201.png" alt="image-20220322194528201"></p>
<ul>
<li>和Spark　ＭＬｌｉｂ一样并行训练局部梯度，再汇总梯度进行更新参数</li>
<li>server节点主要功能保存模型参数，接受worker节点计算出的局部梯度、汇总计算全局梯度，并更新参数</li>
<li>worker节点的主要功能是保存部分训练数据，从server拉取最新模型参数，计算局部梯度，上传给server节点</li>
</ul>
</li>
<li><p>分布式训练流程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322194618555.png" alt="image-20220322194618555"></p>
<ul>
<li>每个worker载入一部分训练数据</li>
<li>worker节点从server节点pull最新模型参数</li>
<li>worker节点利用本节点数据计算梯度</li>
<li>worker节点将梯度push到server节点</li>
<li>server节点汇总梯度更新模型</li>
<li>重复上述步骤</li>
</ul>
</li>
<li><p>好在哪里？</p>
<ul>
<li><p>异步非阻断式</p>
<ul>
<li>其他节点计算梯度的进度不会影响本节点的梯度计算，无须等待其计算完再进行下一步</li>
<li>可以设置“最大延迟”等参数来限制异步计算的程度</li>
<li><p>好处</p>
<ul>
<li>加快了训练速度，完全并行</li>
</ul>
</li>
<li><p>坏处</p>
<ul>
<li>训练结果和原本单点串行训练结果不一致，对模型收敛速度有一定影响</li>
</ul>
</li>
</ul>
</li>
<li><p>论文中指出计算效率有明显提高，异步更新带来的不一致性影响没有想象中大</p>
</li>
<li><p>解决spark MLlib 单点master节点效率低下问题，参数服务器服务节点组采取多个server的架构</p>
<ul>
<li><p>server node如何决定自己负责哪部分参数范围？<br>有新server node加入时，如何保证已有参数范围不发生大的变化？</p>
<ul>
<li><p>一致性哈希</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220322194818604.png" alt="image-20220322194818604"></p>
<ul>
<li>将模型参数的key映射到一个环形的哈希空间</li>
<li>根据server node的节点数量n，将环形哈希空间等分成nm个范围，让每个server间隔的分配m个哈希范围<br>这样保证负载均衡性，避免哈希值过于集中带来负载不均</li>
<li>在新加入一个server node时，让其找到哈希环上的插入点，让其负责插入点到下一个插入点之间的哈希范围<br>这样相当于把原来的某段哈希范围分为两份，新的节点负责后半段，原来的节点负责前半段，这样不会影响其他哈希范围的哈希分配，自然不存在大量重哈希带来的数据大混洗问题</li>
<li>删除一个server node时，移除其相关插入点，让临近节点负责该哈希范围</li>
</ul>
</li>
</ul>
</li>
<li><p>处理梯度过程中，server node之间可以高效协同，基于一致性哈希提供了参数范围pull和push推送能力，让模型并行训练实现更加灵活</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>技术要点总结</p>
<ul>
<li>用异步非阻断式的分布式梯度下降策略</li>
<li>实现多个server node节点架构，避免单master节点带来的带宽瓶颈和内存瓶颈</li>
<li>使用一致性哈希、参数范围pull,push等工程手段实现信息的最小传递，避免广播操作带来的全局网络阻塞和带宽浪费</li>
<li>参数服务器仅仅是一个管理并行训练梯度的权重平台，并不涉及具体的模型实现，因此参数服务器往往作为MXNet,TensorFlow的一个组件</li>
</ul>
</li>
</ul>
<ul>
<li><p>TensorFlow</p>
<ul>
<li><p>基本原理</p>
<ul>
<li><p>将复杂模型转化成操作有向图进行计算</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408193613088.png" alt="image-20220408193613088"></p>
</li>
</ul>
</li>
<li><p>基于任务关系图的并行训练过程</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408193657337.png" alt="image-20220408193657337"></p>
<ul>
<li>存在依赖关系的任务节点或者子图之间需要串行执行</li>
</ul>
</li>
<li><p>不存在依赖关系的任务节点或者子图之间可以并行执行</p>
<ul>
<li><p>使用一个任务队列来解决依赖关系调度问题</p>
<ul>
<li>当一个任务的前序任务全部执行完时，就可以将当前任务推送到任务队列尾，有空闲计算节点时，该计算节点就从任务队列拉取出一个队首的任务进行执行</li>
</ul>
</li>
<li><p>和spark的DAG图有想通之处，不同之处在于spark只是理清任务先后关系，任务的粒度还停留在join，reduce等粗粒度操作，并行机制更多是任务内部的并行执行；而tensor flow把任务拆解成非常细粒度的操作级别，通过并行执行互不依赖的子任务加速训练过程</p>
</li>
</ul>
</li>
<li><p>单机训练和分布式训练</p>
</li>
</ul>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408193740470.png" alt="image-20220408193740470"></p>
<ul>
<li><p>单机</p>
<ul>
<li>虽然执行过程中包括CPU、GPU的并行计算过程，但总体上处于共享内存的环境，不用过多考虑通信问题</li>
<li>在一个worker节点上进行，内部按照任务关系图的方式在不同GPU+CPU节点间进行并行计算</li>
</ul>
</li>
<li><p>分布式</p>
<ul>
<li>多台不共享内存的独立计算节点组成的集群环境，计算节点间需要依靠网络通信，和参数服务器差不多<ul>
<li>平台存在多worker节点，如果采用参数服务器策略（tf.distribute.experimental.ParameterServerStrategy）,则各worker节点以数据并行的方式进行训练，也就是说，各worker以同样的任务关系图进行训练，但是训练数据不同，产生的梯度以Parameter Server的方式汇总更新</li>
</ul>
</li>
<li><p>GPU拥有多核优势，则负责计算密集度高的张量计算<br>  CPU主要负责数据和任务的调度</p>
<ul>
<li>举例来说，处理两个向量的元素乘操作时，CPU会居中调度，把两个向量对应范围的元素发送给GPU处理，再收集处理结果，最终生成结果向量</li>
</ul>
</li>
</ul>
</li>
<li><p>技术要点总结</p>
<ul>
<li>主要原理是将模型训练过程转换成任务关系图，让数据以张量的形式在任务关系图中流动，完成整个训练</li>
<li>tensor flow基于任务关系图进行任务调度和并行计算</li>
<li><p>对应分布式tensor flow来说，其并行训练分两层</p>
<ul>
<li>一层是基于Parameter Server架构的数据并行训练过程</li>
<li>另一层是每个worker节点内部，CPU+GPU任务级别的并行计算过程</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="推荐系统线上服务"><a href="#推荐系统线上服务" class="headerlink" title="推荐系统线上服务"></a>推荐系统线上服务</h3><ul>
<li><p>预存推荐结果或embedding结果</p>
<ul>
<li><p>在离线环境下生成每个用户的推荐结果，然后将结果预存到redis等线上数据库中，在线上环境直接取出数据推荐给用户</p>
<ul>
<li><p>优点</p>
<ul>
<li>无须线上推断过程，线上线下完全解耦，可以灵活选用任意离线机器学习模型</li>
<li>线上没有复杂计算，延迟较低</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>需要存储用户<em>物品</em>应用场景等组合推荐结果，redis等数据库无法支撑大规模结果的存储</li>
<li>无法引入线上场景类特征，推荐系统的灵活性和效果受限</li>
</ul>
</li>
</ul>
</li>
<li><p>直接存储推荐结果适用于用户规模较小，或者冷启动、热门榜单等特殊场景</p>
</li>
<li>存储用户和物品embedding结果是另一个“以存代算”的方法，相比直接存储推荐结果，存储embedding的方式大大减少了存储量，是业界常采用的模型上线手段</li>
<li>存储embedding同样无法支持线上场景特征的引入，无法进行复杂模型的线上推断，表达能力受限</li>
</ul>
</li>
<li><p>自研模型线上服务平台</p>
<ul>
<li><p>为什么放着灵活且成熟的tensorflow不用，而要从头进行模型和平台自研呢？</p>
<ul>
<li>第一个原因：tensorflow等通用平台为了灵活性和通用性，需要支持大量冗余功能，导致平台过重，难以修改和定制；而自研平台可以根据公司业务和需求进行定制化实现，并兼顾模型服务的效率</li>
<li><p>第二个原因：当模型的需求比较特殊时，大部分深度学习框架无法支持</p>
<ul>
<li>例如，某些推荐系统召回模型、探索与利用模型、冷启动等算法</li>
</ul>
</li>
</ul>
</li>
<li><p>自研平台往往只有大公司采用</p>
<ul>
<li>因为实现模型的时间成本较高</li>
<li>新模型层出不穷，自研模型迭代周期长</li>
</ul>
</li>
</ul>
</li>
<li><p>预训练embedding+轻量级线上模型</p>
<ul>
<li><p>复杂网络离线训练、生成embedding存入内存数据库、线上实现逻辑回归或浅层MLP等轻量级模型拟合优化目标的上线方式</p>
<ul>
<li>例如双塔模型</li>
</ul>
</li>
<li><p>graph embedding可以融入大量用户和物品信息，输出层不用设计太复杂</p>
</li>
<li>虽然实用高效，但是对线上线下进行了割裂，无法实现end2end训练+end2end部署这种完美方式</li>
</ul>
</li>
<li><p>利用PMML转换并部署模型</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408193836647.png" alt="image-20220408193836647"></p>
<ul>
<li>PMML的全称是”预测模型标记语言（Predictive Model Markup Language）“，是一种通用的以XML的形式表示不同模型结构参数的标记语言</li>
<li>在模型上线过程中，PMML经常作为中间媒介连接离线训练平台和线上预测平台</li>
</ul>
</li>
</ul>
<h2 id="ch7-推荐系统的评估"><a href="#ch7-推荐系统的评估" class="headerlink" title="ch7_推荐系统的评估"></a>ch7_推荐系统的评估</h2><h3 id="离线评估方法和基本评价指标"><a href="#离线评估方法和基本评价指标" class="headerlink" title="离线评估方法和基本评价指标"></a>离线评估方法和基本评价指标</h3><ul>
<li><p>离线评估的主要方法</p>
<ul>
<li><p>Holdout检验</p>
<ul>
<li>将原始的样本集合随机划分为训练集和验证集</li>
<li><p>缺点</p>
<ul>
<li>在验证集上计算出来的评估指标和训练集和验证集的划分有直接关系，如果仅进行少量Holdout检验，得到的结论存在较大的随机性</li>
</ul>
</li>
</ul>
</li>
<li><p>交叉检验</p>
<ul>
<li><p>k-fold交叉验证</p>
<ul>
<li>将全部样本划分成k个大小相等的样本子集</li>
<li>依次遍历这K个子集，每次把当前子集作为验证集，其余子集作为训练集</li>
<li>最后将所有K次评估指标进行评价得到最终评估指标</li>
<li>K通常取10</li>
</ul>
</li>
<li><p>留一验证</p>
<ul>
<li>每次留下1个样本作为验证集，其余所有样本作为测试集</li>
<li>样本总数为n，进行n次验证</li>
<li>在样本数较多时，时间开销很大</li>
</ul>
</li>
</ul>
</li>
<li><p>自助法</p>
<ul>
<li>当样本规模比较小时，通过自助法来进行n次有放回随机采样，得到大小为n的训练集，将没有被抽样的样本作为验证集，就和随机森林的oob机制一样</li>
</ul>
</li>
</ul>
</li>
<li><p>离线评估指标</p>
<ul>
<li>准确率</li>
<li>精确率和召回率</li>
<li>F1分数</li>
<li>均方根误差</li>
<li>对数损失函数</li>
<li><p>缺点</p>
<ul>
<li>更多地将推荐模型视为类似于点击率预估的预测模型，而不是排序模型，因为预测模型的要求是预测概率具有物理意义，而事实上，推荐系统的最终结果是一个排序列表，并不具有类似点击率这样的物理意义</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="直接评估推荐序列的离线指标"><a href="#直接评估推荐序列的离线指标" class="headerlink" title="直接评估推荐序列的离线指标"></a>直接评估推荐序列的离线指标</h3><ul>
<li><p>P-R曲线</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194007133.png" alt="image-20220408194007133"></p>
<ul>
<li>P-R曲线上的一个点代表“在某一阈值下，模型将大于该阈值的结果判定为正样本，将小于该阈值的结果判定为负样本时，排序结果对应的召回率和精确率”</li>
<li>只有通过P-R曲线的整体表现，才能对模型进行全面的评估</li>
<li>AUC指的是P-R曲线下的面积，越大，排序模型性能越好</li>
<li>横坐标是召回率，纵坐标是精确率</li>
</ul>
</li>
<li><p>ROC曲线</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194052562.png" alt="image-20220408194052562"></p>
<ul>
<li>横坐标是FPR，纵坐标是TPR</li>
<li>AUC对正负样本比不敏感</li>
</ul>
</li>
<li><p>MAP-平均精度均值</p>
<ul>
<li>在推荐系统、信息检索领域常用的评估指标</li>
<li>该指标是对平均精度的再次平均</li>
<li><p>mAP如何计算？</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194255850.png" alt="image-20220408194255850"></p>
</li>
<li><p>值得注意的地方</p>
<ul>
<li>mAP的计算方法和P-R曲线、ROC曲线的计算方法完全不同，因为mAP需要对每个用户的样本进行分用户排序，而P-R曲线和ROC曲线均是对全量测试样本进行排序</li>
</ul>
</li>
</ul>
</li>
<li><p>合理选择评估指标</p>
<ul>
<li>归一化折扣累计收益（NDGG）</li>
<li>覆盖率</li>
<li>多样性</li>
<li>根据业务场景选择2-4个有代表性的离线指标，进行高效率离线试验才是离线评估正确的打开方式</li>
</ul>
</li>
</ul>
<h3 id="更接近线上环境的离线评估方法-Replay"><a href="#更接近线上环境的离线评估方法-Replay" class="headerlink" title="更接近线上环境的离线评估方法-Replay"></a>更接近线上环境的离线评估方法-Replay</h3><ul>
<li><p>模型评估的逻辑闭环</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194425477.png" alt="image-20220408194425477"></p>
<ul>
<li>离线评估的重点是让离线评估的结果能够尽量接近线上结果</li>
</ul>
</li>
<li><p>动态离线评估方法</p>
<ul>
<li><p>传统离线评估方法和动态离线评估方法对比</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194451591.png" alt="image-20220408194451591"></p>
</li>
<li><p>Replay方法</p>
<ul>
<li>整个动态评估过程也变成逐一样本回放的精准线上仿真过程，这就是经典的仿真式离线评估方法-Replay</li>
</ul>
</li>
</ul>
</li>
<li><p>Netflix的Replay评估方法实践</p>
<ul>
<li><p>实践难题</p>
<ul>
<li><p>既然是模拟在线数据流，就要求在每个样本产生时，样本中不能包含任何“未来信息”，要避免“数据穿越”的现象发生</p>
<ul>
<li>例如8月20的历史CTR只能使用过去的CTR来计算</li>
</ul>
</li>
</ul>
</li>
<li><p>Netflix的离线评估数据架构-时光机</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194557110.png" alt="image-20220408194557110"></p>
<ul>
<li>以天为单位启动</li>
<li><p>Snapshot（数据快照）</p>
<ul>
<li>主要功能是把当天的各类日志、特征、数据整合起来，形成当天的、供模型训练和评估的样本数据</li>
<li><p>场景信息</p>
<ul>
<li>用户的资料、设备信息、物品信息等</li>
</ul>
</li>
<li><p>系统日志流</p>
<ul>
<li>用户观看历史、用户推荐列表、用户评价等</li>
</ul>
</li>
</ul>
</li>
<li><p>在时光机的架构上，使用某个时间段的样本进行一次Replay评估</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="AB测试与线上评估指标"><a href="#AB测试与线上评估指标" class="headerlink" title="AB测试与线上评估指标"></a>AB测试与线上评估指标</h3><ul>
<li><p>线上AB测试都是验证新模块、新功能、新产品是否有效的主要测试方法</p>
</li>
<li><p>什么是ABtest</p>
<ul>
<li>是一个随机试验，通常被分为实验组和对照组</li>
<li>在利用控制变量法保持单一变量的前提下，将AB两组数据进行对比，得出实验结论</li>
<li><p>相对离线评估，线上ABtest的优势</p>
<ul>
<li>离线评估无法完全消除数据有偏现象的影响</li>
<li>离线评估无法完全还原线上工程环境</li>
<li>线上系统的某些商业指标在离线中无法计算，例如用户点击率、留存时长等等</li>
</ul>
</li>
</ul>
</li>
<li><p>ABtest分桶原则</p>
<ul>
<li><p>在ABtest分桶过程中，需要注意的是样本独立性和采样方式的无偏性</p>
<ul>
<li>同一个用户在测试全程只能被分到同一个桶中</li>
<li>在分桶过程中所用的用户ID应该是一个随机数</li>
</ul>
</li>
<li><p>在实际ABtest场景中，同一个网站或应用往往要同时进行多组不同类型的ABtest</p>
<ul>
<li><p>层与层之间的流量要正交</p>
<ul>
<li>即实验中每组的流量穿越该层后，都会被再次随机打散，且均匀地分布在下层试验的每个组中</li>
</ul>
</li>
</ul>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194659078.png" alt="image-20220408194659078"></p>
</li>
<li><p>同层之间的流量要互斥</p>
<ul>
<li>如果同层之间进行多组ABtest，那么不同测试之间的流量是不重叠的</li>
<li>如果只进行一组，那么实验组和对照组的流量是不重叠的</li>
</ul>
</li>
</ul>
</li>
<li><p>线上ABtest的评估指标</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194749202.png" alt="image-20220408194749202"></p>
<ul>
<li><p>电商类</p>
<ul>
<li>点击率、转化率、客单价</li>
</ul>
</li>
<li><p>新闻类</p>
<ul>
<li>留存率、平均停留时长、平均点击个数</li>
</ul>
</li>
<li><p>视频类</p>
<ul>
<li>播放完成率、平均播放时长、播放总时长</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="快速线上评估方法-Interleaving"><a href="#快速线上评估方法-Interleaving" class="headerlink" title="快速线上评估方法-Interleaving"></a>快速线上评估方法-Interleaving</h3><p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194823815.png" alt="image-20220408194823815"></p>
<ul>
<li><p>介绍</p>
<ul>
<li>Interleaving方法被当作线上ABtest的预选阶段进行候选算法的快速筛选，从大量初始想法中筛选出少量优秀的推荐算法。再对缩小的算法集合进行传统的ABtest，以测量它们对用户行为的长期影响</li>
<li>不区分AB组，而是把不同的被测对象同时提供给受试者，最后根据受试者喜好得出结果</li>
</ul>
</li>
<li><p>传统ABtest存在的统计学问题</p>
<ul>
<li>例如可乐重消费人群在AB两组的微小不平衡，有可能造成结果的不准确</li>
</ul>
</li>
<li><p>Interleaving方法的实现</p>
<ul>
<li><p>传统ABtest与Interleaving方法比较</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194859951.png" alt="image-20220408194859951"></p>
</li>
<li><p>使用Interleaving进行测试时，必须考虑位置偏差的存在，避免来自算法A的视频总排在第一位</p>
</li>
</ul>
</li>
<li><p>Interleaving与ABtest的灵敏度比较</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194918300.png" alt="image-20220408194918300"></p>
</li>
<li><p>Interleaving与ABtest的相关性</p>
<ul>
<li><p>存在非常强的相关性</p>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194937367.png" alt="image-20220408194937367"></p>
</li>
<li><p>但是Interleaving试验展示页面是候选算法集混合而成，如果需要更全面、真实的线上测试指标，ABtest是最权威的测试方法</p>
</li>
</ul>
</li>
<li><p>方法优缺点</p>
<ul>
<li>工程实现较ABtest复杂</li>
<li>只是对用户对推荐算法结果偏好程度的相对测量，不能得出一个算法真实的表现</li>
</ul>
<p><img src="/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/image-20220408194956393.png" alt="image-20220408194956393"></p>
</li>
</ul>
<h2 id="ch8-深度学习推荐系统的前沿实践"><a href="#ch8-深度学习推荐系统的前沿实践" class="headerlink" title="ch8_深度学习推荐系统的前沿实践"></a>ch8_深度学习推荐系统的前沿实践</h2><h3 id="Facebook的深度学习推荐系统"><a href="#Facebook的深度学习推荐系统" class="headerlink" title="Facebook的深度学习推荐系统"></a>Facebook的深度学习推荐系统</h3><ul>
<li><p>应用场景</p>
<ul>
<li>标准的CTR预估场景，系统输入用户、广告、上下文的相关特征，预测CTR</li>
<li>需要注意的是：Facebook广告系统的其他模块需要利用CTR计算广告出价、投资回报率等预估值，因此模型预估值应该是一个具有物理意义的精准CTR，需要做CTR校正</li>
</ul>
</li>
<li><p>以GBDT+LR组合模型为基础的CTR预估模型</p>
<ul>
<li>通过GDBT自动进行特征筛选和组合，生成新的离散型特征向量，再将该特征向量输入LR模型，预测CTR</li>
<li>使用GBDT构建特征工程和利用LR预测CTR两步采用相同的优化目标</li>
<li>在实际应用中选择了600作为子树规模</li>
<li>GBDT部分几天更新一次，LR部分准实时更新</li>
</ul>
</li>
<li><p>实时数据流架构</p>
<ul>
<li>最重要的作用是准实时的把来自不同数据流的数据整合起来，形成训练样本，并和点击数据整合，形成完整的有标签样本</li>
<li><p>需要注意的点</p>
<ul>
<li><p>数据等待窗口的设定</p>
<ul>
<li>曝光发生后，需等待多久才能判定这个曝光行为是否产生点击</li>
</ul>
</li>
<li><p>分布式架构与全局统一的行为id</p>
<ul>
<li>为每个行为建立全局统一的请求id</li>
<li>如果在等待窗口过期时还没有匹配到点击，就会当作负样本</li>
</ul>
</li>
<li><p>数据流保护机制</p>
<ul>
<li>需要异常检测机制，如因为请求ID异常导致所有样本都是负样本，此时训练的模型是有问题的</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>降采样和模型修正</p>
<ul>
<li><p>降采样</p>
<ul>
<li><p>均匀采样</p>
<ul>
<li></li>
</ul>
</li>
<li><p>负样本采样</p>
<ul>
<li></li>
<li><p>负采样带来的问题是CTR预估值的漂移</p>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>GBDT+LR组合模型的工程实践</p>
<ul>
<li>特征工程模型化</li>
<li>模型复杂性和实效性的权衡</li>
<li>有想法要用数据验证</li>
</ul>
</li>
<li><p>深度学习模型DLRM</p>
<ul>
<li><p>模型结构</p>
<ul>
<li></li>
<li><p>特征工程</p>
<ul>
<li>类别、ID类特征用one-hot编码</li>
<li>数值型连续特征</li>
</ul>
</li>
<li><p>Embedding层</p>
<ul>
<li>稀疏特征转成n维Embedding向量</li>
<li>连续特征通过MLP转成同样n维向量</li>
</ul>
</li>
<li><p>神经网络层</p>
</li>
<li><p>特征交互层</p>
<ul>
<li>两两内积</li>
</ul>
</li>
<li><p>目标拟合层</p>
<ul>
<li>全连接多层神经网络，最后一层sigmoid函数给出最终点击率预估</li>
</ul>
</li>
</ul>
</li>
<li><p>并行训练方法</p>
<ul>
<li><p>模型并行</p>
<ul>
<li>Embedding部分并行，指在一个设备上仅保存一部分Embedding层参数，每个设备进行并行mini-bath梯度更新时，仅更新自己节点上的部分参数</li>
</ul>
</li>
<li><p>数据并行</p>
<ul>
<li>MLP层和特征交互层进行数据并行指的是每个设备上已经有了全部模型参数，每个设备利用部分数据计算梯度，再利用全量规约的方法汇总梯度进行参数更新</li>
</ul>
</li>
</ul>
</li>
<li><p>效果对比</p>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Airbnb基于Embedding的实时搜索推荐系统"><a href="#Airbnb基于Embedding的实时搜索推荐系统" class="headerlink" title="Airbnb基于Embedding的实时搜索推荐系统"></a>Airbnb基于Embedding的实时搜索推荐系统</h3><ul>
<li><p>应用场景</p>
<ul>
<li>是一个典型的搜索推荐场景，租客输入地点、价位等信息后，给出房源推荐列表</li>
<li><p>交互方式</p>
<ul>
<li>租客点击房源</li>
<li><p>预定</p>
<ul>
<li>房主可能拒绝、同意或者不响应</li>
</ul>
</li>
</ul>
</li>
<li><p>做法</p>
<ul>
<li><p>先对租客和房源分别进行Embedding，进而利用结果构建诸多特征，作为排序模型输入</p>
<ul>
<li><p>短期兴趣Embedding</p>
<ul>
<li>进行房源相似推荐</li>
</ul>
</li>
<li><p>长期兴趣Embedding</p>
<ul>
<li>照顾之前预定偏好</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>短期兴趣Embedding方法</p>
<ul>
<li><p>点击房源组成的序列</p>
<ul>
<li>在房源详情页停留超过30秒才算序列中一个序列点</li>
<li>如果用户超过30分钟没有动作，那么这个序列被打断</li>
</ul>
</li>
<li><p>基于Word2vec的skip-gram模型</p>
<ul>
<li>正样本是Session内点击序列滑动窗口中的房源</li>
<li>负样本是在确定中心房源后从房源集合中随机选取一个房源作为负样本</li>
</ul>
</li>
<li><p>特制</p>
<ul>
<li><p>针对业务特点，将产生预定行为的称为预定会话，反之为探索性会话</p>
<ul>
<li>每个预定会话只有最后一个房源是被预定的，不管被预定房源是否在滑动窗口中，都假设这个房源与滑动窗口的正样本相关</li>
</ul>
</li>
<li><p>为了更好发现同一市场内部房源差异，加入特制负样本，即与中心房源同一市场的房源集合随机抽样</p>
</li>
</ul>
</li>
<li><p>最终目标函数</p>
<ul>
<li></li>
</ul>
</li>
<li><p>冷启动</p>
<ul>
<li>有新房源，找附近3个同类型相似价格的房源向量进行平均得到</li>
</ul>
</li>
</ul>
</li>
<li><p>长期兴趣Embedding方法</p>
<ul>
<li><p>使用预定会话，例如用户过去1年预定的房源序列</p>
<ul>
<li><p>这里不能采取Word2vec的方法生成Embedding</p>
<ul>
<li>因为预定行为数量远远少于点击行为</li>
<li>单一用户预定行为很少</li>
<li>大部分房源被预定次数很少</li>
</ul>
</li>
<li><p>解决办法</p>
<ul>
<li><p>基于某些属性规则做相似用户和相似房源的聚合</p>
<ul>
<li></li>
</ul>
</li>
<li><p>得到用户属性预定序列后，如何生成属性Embedding</p>
<ul>
<li></li>
</ul>
</li>
<li><p>得到元组序列后，如何确定中心词</p>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>搜索词的Embedding</p>
<ul>
<li>将搜索词和房源置于同一向量空间进行Embedding，通过二者余弦相似度进行排序</li>
</ul>
</li>
<li><p>实时搜索排序模型及其特征工程</p>
<ul>
<li><p>基于Embedding得到不同的用户房源相关特征，然后输入搜索排序模型，得到结果</p>
<ul>
<li><p>生成的特征有</p>
<ul>
<li>候选房源与用户点击房源相似度</li>
<li>与忽略</li>
<li>长点击</li>
<li>收藏</li>
<li>联系</li>
<li>预定</li>
<li>最后长点击</li>
<li>候选房源属性与用户属性相似度</li>
<li>最近点击</li>
<li>最后点击</li>
</ul>
</li>
<li><p>排序模型-支持Pairwise lambda Rank的GBDT模型</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li><p>工程与理论结合</p>
<ul>
<li>对Word2vec改造</li>
<li>利用用户属性和房源属性聚合稀疏数据</li>
</ul>
</li>
<li><p>业务与知识结合</p>
<ul>
<li>引入与业务强相关的目标项</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="YouTube深度学习视频推荐系统"><a href="#YouTube深度学习视频推荐系统" class="headerlink" title="YouTube深度学习视频推荐系统"></a>YouTube深度学习视频推荐系统</h3><ul>
<li><p>应用场景</p>
<ul>
<li>商业模式不同，它们大部分内容都是采购或者自制电影，其头部效应没有那么明显</li>
<li>视频基数大，用户很难发现喜欢内容</li>
<li>利润来源于视频广告，而广告曝光机会与用户观看时长成正比</li>
<li>构建以用户观看时长为优化目标的排序模型</li>
</ul>
</li>
<li><p>推荐系统架构</p>
<ul>
<li><p>整体架构图</p>
<ul>
<li></li>
</ul>
</li>
<li><p>候选集生成模型-召回</p>
<ul>
<li><p>架构图</p>
<ul>
<li></li>
</ul>
</li>
<li><p>softmax多分类</p>
</li>
<li><p>用户历史观看视频Embedding、搜索词Embedding、地理、性别、年龄等特征</p>
<ul>
<li>根据用户观看序列和搜索序列，采用word2vec方法生成Embedding，再作为模型输入</li>
</ul>
</li>
<li><p>模型服务</p>
<ul>
<li>根据模型得到用户Embedding和视频Embedding</li>
<li>再线上根据局部敏感哈希等最近邻搜索方法计算</li>
</ul>
</li>
</ul>
</li>
<li><p>排序模型-精排</p>
<ul>
<li><p>引入更多特征进行精排</p>
<ul>
<li>当前候选视频的Embedding</li>
<li>用户观看过的最后N个视频Embedding平均值</li>
<li>用户语言Embedding和当前候选视频语言Embedding</li>
<li>该用户自上次观看同频道视频时间</li>
<li>该视频已经曝光给该用户的次数</li>
</ul>
</li>
<li><p>以sigmoid作为输出层</p>
<ul>
<li>加权逻辑回归，权重是观看时长</li>
</ul>
</li>
</ul>
</li>
<li><p>训练和预测样本处理</p>
<ul>
<li><p>候选集生成模型会把推荐问题转成多分类，每个视频都是一个分类，则会有数百万分类，低效如何解决？</p>
<ul>
<li>通过负样本采样减少每次每次预测的分类数量</li>
</ul>
</li>
<li><p>训练集没有采用原始日志，而是每个用户提取等数量训练样本</p>
<ul>
<li>这样做的目的是减少高度活跃用户对模型损失的过度影响</li>
</ul>
</li>
<li><p>为什么不采用随机留一法进行验证集？而是以用户最后一次观看行为为验证集</p>
<ul>
<li>构建验证集时避免引入未来信息，产生与事实不符的数据穿越问题</li>
</ul>
</li>
</ul>
</li>
<li><p>如何处理用户对新视频的偏好？</p>
<ul>
<li><p>引入ExampleAge特征，即训练样本产生的时刻距离当前时刻的时间</p>
<ul>
<li>在做模型服务时，不管候选视频是哪个，可以直接将这个特征设为0</li>
<li>与其他特征交叉时，起到时间戳的作用</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>强烈建议阅读论文原文</li>
</ul>
</li>
</ul>
<h3 id="阿里巴巴深度学习推荐系统的进化"><a href="#阿里巴巴深度学习推荐系统的进化" class="headerlink" title="阿里巴巴深度学习推荐系统的进化"></a>阿里巴巴深度学习推荐系统的进化</h3><ul>
<li><p>应用场景</p>
<ul>
<li>根据用户的历史行为、输入的搜索词及其他商品和用户信息，在网站或app的不同推荐位置为用户推荐感兴趣的商品</li>
<li><p>例如购买一个无线鼠标的过程</p>
<ul>
<li>登录</li>
<li>搜索</li>
<li>浏览</li>
<li>点击</li>
<li>加入购物车</li>
<li>支付</li>
<li>购买成功</li>
</ul>
</li>
<li><p>多模态信息</p>
<ul>
<li>文本类的描述信息</li>
<li>数字类的价格，购买量等信息</li>
<li>商品图片信息</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐模型体系</p>
<ul>
<li><p>推荐体系模型图</p>
<ul>
<li></li>
</ul>
</li>
<li><p>演化阶段</p>
<ul>
<li><p>基础的深度学习模型</p>
<ul>
<li>基于Embedding+MLP深度学习模型架构</li>
<li>将用户行为历史的Embedding简单的通过加权和池化操作叠加</li>
<li>再与其他用户特征、广告特征、场景特征连接后输入上层神经网络进行训练</li>
</ul>
</li>
<li><p>DIN模型</p>
<ul>
<li>利用注意力机制替换模型是Sum Pooling操作</li>
<li>根据候选广告和用户历史行为之间的关系确定每个历史行为的权重</li>
</ul>
</li>
<li><p>DIEN</p>
<ul>
<li>在DIN基础上，进一步改进对用户历史行为建模，使用序列模型在用户行为历史上抽取用户兴趣并模拟用户兴趣的演化过程</li>
</ul>
</li>
<li><p>MIMN</p>
<ul>
<li>在DIEN基础上，将用户的兴趣细分为不同兴趣通道，进一步模拟用户在不同兴趣通道的演化过程，生成不同兴趣通道的记忆向量，再利用注意力机制作用于多层神经网络</li>
</ul>
</li>
</ul>
</li>
<li><p>技术架构</p>
<ul>
<li><p>架构图</p>
<ul>
<li></li>
</ul>
</li>
<li><p>用户兴趣表达模块</p>
<ul>
<li>B架构将A架构的用户行为特征在线数据库替换成用户兴趣表达在线数据库，这样在线上就可以直接根据用户兴趣Embedding进行MLP推断</li>
</ul>
</li>
<li><p>用户兴趣中心模块</p>
<ul>
<li>B架构增加一个服务模块-用户兴趣中心，一个新用户行为事件产生时，该中心会更新对应用户的兴趣向量</li>
</ul>
</li>
<li><p>离线部分</p>
<ul>
<li>学习模块定期利用系统日志训练并更新模型，A架构将模型直接部署在实时预估服务器中，而B架构将生成用户兴趣向量部分部署在用户兴趣中心模块，其他部分部署在实时预估服务器</li>
</ul>
</li>
<li><p>在线部分</p>
<ul>
<li>流量请求到来时，其中携带用户ID和待排序的候选商品ID</li>
<li>实时预估服务器根据用户ID和候选ID获取用户和商品特征</li>
<li>进行预估和排序，返回排序结果给请求方</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>工程实践性强</li>
<li>对用户行为观察精确</li>
<li>模型微创新</li>
</ul>
</li>
<li><ul>
<li></li>
</ul>
</li>
</ul>
<h2 id="ch9-构建属于自己的推荐系统知识框架"><a href="#ch9-构建属于自己的推荐系统知识框架" class="headerlink" title="ch9_构建属于自己的推荐系统知识框架"></a>ch9_构建属于自己的推荐系统知识框架</h2><h3 id="推荐系统整体知识架构图"><a href="#推荐系统整体知识架构图" class="headerlink" title="推荐系统整体知识架构图"></a>推荐系统整体知识架构图</h3><ul>
<li><h3 id="推荐模型发展的时间线"><a href="#推荐模型发展的时间线" class="headerlink" title="推荐模型发展的时间线"></a>推荐模型发展的时间线</h3></li>
<li></li>
</ul>
<h3 id="如何成为一名优秀的推荐工程师"><a href="#如何成为一名优秀的推荐工程师" class="headerlink" title="如何成为一名优秀的推荐工程师"></a>如何成为一名优秀的推荐工程师</h3><ul>
<li><p>4项能力</p>
<ul>
<li><p>知识</p>
<ul>
<li>具备基本的推荐系统领域相关知识<br>比如主流的推荐模型、Embedding的主要方法等</li>
</ul>
</li>
<li><p>工具</p>
<ul>
<li>具备编程能力，了解推荐系统相关工程实践工具<br>主要有TensorFlow、PyTorch等模型训练工具，Spark、Flink等大数据处理工具，以及一些模型服务相关的工具</li>
</ul>
</li>
<li><p>逻辑</p>
<ul>
<li>具备算法基础，思考的逻辑性、条理性极强</li>
</ul>
</li>
<li><p>业务</p>
<ul>
<li>对推荐系统的业务场景有所了解<br>理解应用场景、商业模式；从业务中发现用户动机，制定相应的优化目标并改进模型算法的能力</li>
</ul>
</li>
</ul>
</li>
<li><p>能力深度和广度</p>
<ul>
<li><p>例如公司希望改进目前的推荐模型，于是你提出以DIN为主要结构的模型改进方案<br>这时需要思考？深度</p>
<ul>
<li>DIN模型提出的动机是什么？是否适合自己公司当前的场景和数据特点</li>
<li>DIN模型的模型结构是什么？具体实现起来会有哪些工程难点？</li>
<li>DIN模型强调的注意力机制是什么？为什么在推荐系统中使用注意力机制会有效果上的提升？</li>
<li>DIN模型将用户和商品进行了Embedding，在实际使用中，应该如何实现Embedding过程？</li>
<li>是通过改进现有模型实现DIN模型，还是使用全新离线训练方式训练DIN模型？</li>
<li>线上部署和服务DIN模型会有哪些潜在问题，有哪些解决方案？</li>
</ul>
</li>
<li><p>广度</p>
<ul>
<li>与DIN模型类似的有哪些？适合哪些场景？</li>
<li>Embedding方法有哪些？各自优劣</li>
<li>训练和上线DIN的技术方案有哪些？如何与公司技术栈结合？</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io" rel="external nofollow noreferrer">Myhaa</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/">https://myhaa.github.io/2021/09/23/du-shu-bi-ji-zhi-shen-du-xue-xi-tui-jian-xi-tong/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">
                                    <span class="chip bg-color">推荐系统</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">感谢您的赏识！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'a1900fd4b8fb7a569ef7',
        clientSecret: 'd1176a5ad242e4887008f5d4389ea5a35f199c44',
        repo: 'myhaa.github.io',
        owner: 'myhaa',
        admin: ["myhaa"],
        id: '2021-09-23T09-07-26',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/11/03/shen-du-xue-xi-ng-zhi-jia-qiang-shen-du-wang-luo-xing-neng-de-yi-xie-ji-qiao/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.jpg" class="responsive-img" alt="L2W5加强深度网络性能的一些技巧">
                        
                        <span class="card-title">L2W5加强深度网络性能的一些技巧</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            加强深度网络性能的一些技巧
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-11-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                        <span class="chip bg-color">神经网络</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/09/13/tensorflow-xue-xi-zhi-bu-ping-heng-shu-ju-de-fen-lei/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.jpg" class="responsive-img" alt="TensorFlow学习之不平衡数据的分类">
                        
                        <span class="card-title">TensorFlow学习之不平衡数据的分类</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TensorFlow学习之不平衡数据的分类
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/TensorFlow%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    TensorFlow学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%88%86%E7%B1%BB/">
                        <span class="chip bg-color">分类</span>
                    </a>
                    
                    <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                        <span class="chip bg-color">神经网络</span>
                    </a>
                    
                    <a href="/tags/%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/">
                        <span class="chip bg-color">不平衡数据</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Myhaa's Blog<br />'
            + '文章作者: Myhaa<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>

    
<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">年份</span>
            <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    window.setTimeout("siteTime()", 1000);
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2019";
                    var startMonth = "11";
                    var startDate = "11";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">














    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="/libs/instantpage/instantpage.js" type="module"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
    

</body>

</html>
