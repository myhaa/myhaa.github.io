<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="深度学习之吴恩达课程作业3, 统计学 数据挖掘 机器学习 计算广告">
    <meta name="description" content="吴恩达深度学习课程作业L1W4
构建多层隐藏层神经网络

HW1参考
视频链接
作业链接
作业链接2

HW2参考
视频链接
作业链接
作业链接2

1-导入模块import time
import numpy as np
import h">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>深度学习之吴恩达课程作业3 | Myhaa&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    
    <script src="/libs/jquery/jquery.min.js"></script>
    
<link rel="alternate" href="/atom.xml" title="Myhaa's Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Myhaa's Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Myhaa's Blog</div>
        <div class="logo-desc">
            
            要么孤独，要么庸俗
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/myhaa" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/myhaa" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        深度学习之吴恩达课程作业3
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                <span class="chip bg-color">神经网络</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-03
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    4.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                
				
                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="吴恩达深度学习课程作业L1W4"><a href="#吴恩达深度学习课程作业L1W4" class="headerlink" title="吴恩达深度学习课程作业L1W4"></a>吴恩达深度学习课程作业L1W4</h1><ul>
<li>构建多层隐藏层神经网络</li>
</ul>
<h2 id="HW1参考"><a href="#HW1参考" class="headerlink" title="HW1参考"></a>HW1参考</h2><ol>
<li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li>
<li><a href="https://www.heywhale.com/mw/project/5dd798fbf41512002ceb38de" target="_blank" rel="noopener">作业链接</a></li>
<li><a href="https://github.com/suqi/deeplearning_andrewng" target="_blank" rel="noopener">作业链接2</a></li>
</ol>
<h2 id="HW2参考"><a href="#HW2参考" class="headerlink" title="HW2参考"></a>HW2参考</h2><ol>
<li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li>
<li><a href="https://github.com/JudasDie/deeplearning.ai" target="_blank" rel="noopener">作业链接</a></li>
<li><a href="https://github.com/suqi/deeplearning_andrewng" target="_blank" rel="noopener">作业链接2</a></li>
</ol>
<h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1-导入模块"></a>1-导入模块</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import time
import numpy as np
import h5py
import matplotlib.pyplot as plt
import scipy
from PIL import Image
from scipy import ndimage

%matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0)
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# load_extension
%load_ext autoreload  
# Then your module will be auto-reloaded by default
%autoreload 2  

np.random.seed(1)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">root_path = './深度学习之吴恩达课程作业3/'
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="2-一些函数"><a href="#2-一些函数" class="headerlink" title="2-一些函数"></a>2-一些函数</h2><h3 id="2-1-testCases-v3-py"><a href="#2-1-testCases-v3-py" class="headerlink" title="2.1-testCases_v3.py"></a>2.1-testCases_v3.py</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_forward_test_case():
    np.random.seed(1)
    """
    X = np.array([[-1.02387576, 1.12397796],
                 [-1.62328545, 0.64667545],
                 [-1.74314104, -0.59664964]])
    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])
    b = np.array([[1]])
    """
    A = np.random.randn(3,2)
    W = np.random.randn(1,3)
    b = np.random.randn(1,1)

    return A, W, b

def func_linear_activation_forward_test_case():
    """
    X = np.array([[-1.02387576, 1.12397796],
                 [-1.62328545, 0.64667545],
                 [-1.74314104, -0.59664964]])
    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])
    b = 5
    """
    np.random.seed(2)
    A_prev = np.random.randn(3,2)
    W = np.random.randn(1,3)
    b = np.random.randn(1,1)
    return A_prev, W, b

def func_L_model_forward_test_case():
    """
    X = np.array([[-1.02387576, 1.12397796],
                 [-1.62328545, 0.64667545],
                 [-1.74314104, -0.59664964]])
    parameters = {
        'W1': np.array([[ 1.62434536, -0.61175641, -0.52817175],
                        [-1.07296862,  0.86540763, -2.3015387 ]]),
        'W2': np.array([[ 1.74481176, -0.7612069 ]]),
        'b1': np.array([[ 0.], [ 0.]]),
        'b2': np.array([[ 0.]])
    }
    """
    np.random.seed(1)
    X = np.random.randn(4,2)
    W1 = np.random.randn(3,4)
    b1 = np.random.randn(3,1)
    W2 = np.random.randn(1,3)
    b2 = np.random.randn(1,1)
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return X, parameters

def func_compute_cost_test_case():
    Y = np.asarray([[1, 1, 1]])
    aL = np.array([[.8,.9,0.4]])

    return Y, aL

def func_linear_backward_test_case():
    """
    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],
       [-1.62328545,  0.64667545],
       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))
    """
    np.random.seed(1)
    dZ = np.random.randn(1,2)
    A = np.random.randn(3,2)
    W = np.random.randn(1,3)
    b = np.random.randn(1,1)
    linear_cache = (A, W, b)
    return dZ, linear_cache

def func_linear_activation_backward_test_case():
    """
    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))
    """
    np.random.seed(2)
    dA = np.random.randn(1,2)
    A = np.random.randn(3,2)
    W = np.random.randn(1,3)
    b = np.random.randn(1,1)
    Z = np.random.randn(1,2)
    linear_cache = (A, W, b)
    activation_cache = Z
    linear_activation_cache = (linear_cache, activation_cache)

    return dA, linear_activation_cache

def func_L_model_backward_test_case():
    """
    X = np.random.rand(3,2)
    Y = np.array([[1, 1]])
    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}

    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],
           [ 0.02738759,  0.67046751],
           [ 0.4173048 ,  0.55868983]]),
    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),
    np.array([[ 0.]])),
   np.array([[ 0.41791293,  1.91720367]]))])
   """
    np.random.seed(3)
    AL = np.random.randn(1, 2)
    Y = np.array([[1, 0]])

    A1 = np.random.randn(4,2)
    W1 = np.random.randn(3,4)
    b1 = np.random.randn(3,1)
    Z1 = np.random.randn(3,2)
    linear_cache_activation_1 = ((A1, W1, b1), Z1)

    A2 = np.random.randn(3,2)
    W2 = np.random.randn(1,3)
    b2 = np.random.randn(1,1)
    Z2 = np.random.randn(1,2)
    linear_cache_activation_2 = ((A2, W2, b2), Z2)

    caches = (linear_cache_activation_1, linear_cache_activation_2)

    return AL, Y, caches

def func_update_parameters_test_case():
    """
    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747],
        [-1.8634927 , -0.2773882 , -0.35475898],
        [-0.08274148, -0.62700068, -0.04381817],
        [-0.47721803, -1.31386475,  0.88462238]]),
    'W2': np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],
        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],
        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),
        'W3': np.array([[-1.02378514, -0.7129932 ,  0.62524497],
        [-0.16051336, -0.76883635, -0.23003072]]),
    'b1': np.array([[ 0.],
        [ 0.],
        [ 0.],
        [ 0.]]),
    'b2': np.array([[ 0.],
        [ 0.],
        [ 0.]]),
    'b3': np.array([[ 0.],
        [ 0.]])}
    grads = {'dW1': np.array([[ 0.63070583,  0.66482653,  0.18308507],
        [ 0.        ,  0.        ,  0.        ],
        [ 0.        ,  0.        ,  0.        ],
        [ 0.        ,  0.        ,  0.        ]]),
    'dW2': np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],
        [ 0.        ,  0.        ,  0.        ,  0.        ],
        [ 0.        ,  0.        ,  0.        ,  0.        ]]),
    'dW3': np.array([[-1.40260776,  0.        ,  0.        ]]),
    'da1': np.array([[ 0.70760786,  0.65063504],
        [ 0.17268975,  0.15878569],
        [ 0.03817582,  0.03510211]]),
    'da2': np.array([[ 0.39561478,  0.36376198],
        [ 0.7674101 ,  0.70562233],
        [ 0.0224596 ,  0.02065127],
        [-0.18165561, -0.16702967]]),
    'da3': np.array([[ 0.44888991,  0.41274769],
        [ 0.31261975,  0.28744927],
        [-0.27414557, -0.25207283]]),
    'db1': 0.75937676204411464,
    'db2': 0.86163759922811056,
    'db3': -0.84161956022334572}
    """
    np.random.seed(2)
    W1 = np.random.randn(3,4)
    b1 = np.random.randn(3,1)
    W2 = np.random.randn(1,3)
    b2 = np.random.randn(1,1)
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    np.random.seed(3)
    dW1 = np.random.randn(3,4)
    db1 = np.random.randn(3,1)
    dW2 = np.random.randn(1,3)
    db2 = np.random.randn(1,1)
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return parameters, grads


def func_L_model_forward_test_case_2hidden():
    np.random.seed(6)
    X = np.random.randn(5,4)
    W1 = np.random.randn(4,5)
    b1 = np.random.randn(4,1)
    W2 = np.random.randn(3,4)
    b2 = np.random.randn(3,1)
    W3 = np.random.randn(1,3)
    b3 = np.random.randn(1,1)

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2,
                  "W3": W3,
                  "b3": b3}

    return X, parameters

def func_print_grads(grads):
    print ("dW1 = "+ str(grads["dW1"]))
    print ("db1 = "+ str(grads["db1"]))
    print ("dA1 = "+ str(grads["dA2"])) # this is done on purpose to be consistent with lecture where we normally start with A0
    # in this implementation we started with A1, hence we bump it up by 1.
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="2-2-dnn-utils-v2-py"><a href="#2-2-dnn-utils-v2-py" class="headerlink" title="2.2-dnn_utils_v2.py"></a>2.2-dnn_utils_v2.py</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_sigmoid(Z):
    """
    Implements the sigmoid activation in numpy

    Arguments:
    Z -- numpy array of any shape

    Returns:
    A -- output of sigmoid(z), same shape as Z
    cache -- returns Z as well, useful during backpropagation
    """

    A = 1/(1+np.exp(-Z))
    cache = Z

    return A, cache

def func_relu(Z):
    """
    Implement the RELU function.

    Arguments:
    Z -- Output of the linear layer, of any shape

    Returns:
    A -- Post-activation parameter, of the same shape as Z
    cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently
    """

    A = np.maximum(0,Z)

    assert(A.shape == Z.shape)

    cache = Z 
    return A, cache


def func_relu_backward(dA, cache):
    """
    Implement the backward propagation for a single RELU unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    Z = cache
    dZ = np.array(dA, copy=True) # just converting dz to a correct object.

    # When z <= 0, you should set dz to 0 as well. 
    dZ[Z <= 0] = 0

    assert (dZ.shape == Z.shape)

    return dZ

def func_sigmoid_backward(dA, cache):
    """
    Implement the backward propagation for a single SIGMOID unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    Z = cache

    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)

    assert (dZ.shape == Z.shape)

    return dZ
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="3-主要框架"><a href="#3-主要框架" class="headerlink" title="3-主要框架"></a>3-主要框架</h2><ol>
<li>初始化参数：$L$层神经网络</li>
<li>实现前向传播模块</li>
<li>计算损失</li>
<li>实现反向传播模块</li>
<li>更新参数</li>
</ol>
<ul>
<li>如下图</li>
</ul>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/final_outline.png" alt="final_outline.png"></p>
<ul>
<li>注意<ol>
<li>每一个前向传播对应一个后向传播，所以要将前向传播的值cache，以便后向传播计算梯度</li>
</ol>
</li>
</ul>
<h2 id="4-参数初始化"><a href="#4-参数初始化" class="headerlink" title="4-参数初始化"></a>4-参数初始化</h2><ol>
<li>实现2层神经网络的参数初始化</li>
<li>实现$L$层神经网络的参数初始化</li>
</ol>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/relu.png" alt="relu"></p>
<h3 id="4-1-2层神经网络"><a href="#4-1-2层神经网络" class="headerlink" title="4.1-2层神经网络"></a>4.1-2层神经网络</h3><ul>
<li>模型结构是：<em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em></li>
<li>用<code>np.random.randn(shape)*0.01</code>来随机初始化参数</li>
<li>用<code>np.zeros(shape)</code>来初始化bias</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_2_layers_initialize_parameters(n_x, n_h, n_y):
    """
    2层神经网络的参数初始化
    :param n_x: size of the input layer
    :param n_h: size of the hidden layer
    :param n_y: size of the output layer
    :return parameters: python dictionary containing your parameters:
        W1 -- weight matrix of shape (n_h, n_x)
        b1 -- bias vector of shape (n_h, 1)
        W2 -- weight matrix of shape (n_y, n_h)
        b2 -- bias vector of shape (n_y, 1)
    """
    np.random.seed(1)

    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))

    parameters = {
        'W1': W1,
        'b1': b1,
        'W2': W2,
        'b2': b2
    }
    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_2_layers_initialize_parameters(3, 2, 1)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[ 0.01624345, -0.00611756, -0.00528172],
        [-0.01072969,  0.00865408, -0.02301539]]),
 &#39;b1&#39;: array([[0.],
        [0.]]),
 &#39;W2&#39;: array([[ 0.01744812, -0.00761207]]),
 &#39;b2&#39;: array([[0.]])}
</code></pre><h3 id="3-2-L层神经网络"><a href="#3-2-L层神经网络" class="headerlink" title="3.2-L层神经网络"></a>3.2-L层神经网络</h3><ul>
<li>以输入$X$大小为(12288,209)(m=209)为例，各层参数如下表：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">-</th>
<th style="text-align:left">shape of W</th>
<th style="text-align:left">shape of b</th>
<th style="text-align:left">activation</th>
<th style="text-align:left">shape of activation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">layer 1</td>
<td style="text-align:left">$(n^{[1]},12288)$</td>
<td style="text-align:left">$(n^{[1]},1)$</td>
<td style="text-align:left">$Z^{[1]} = W^{[1]}  X + b^{[1]}$</td>
<td style="text-align:left">$(n^{[1]},209)$</td>
</tr>
<tr>
<td style="text-align:left">layer 2</td>
<td style="text-align:left">$(n^{[2]}, n^{[1]})$</td>
<td style="text-align:left">$(n^{[2]},1)$</td>
<td style="text-align:left">$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$</td>
<td style="text-align:left">$(n^{[2]}, 209)$</td>
</tr>
<tr>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
<tr>
<td style="text-align:left">layer L-1</td>
<td style="text-align:left">$(n^{[L-1]}, n^{[L-2]})$</td>
<td style="text-align:left">$(n^{[L-1]}, 1)$</td>
<td style="text-align:left">$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$</td>
<td style="text-align:left">$(n^{[L-1]}, 209)$</td>
</tr>
<tr>
<td style="text-align:left">layer L</td>
<td style="text-align:left">$(n^{[L]}, n^{[L-1]})$</td>
<td style="text-align:left">$(n^{[L]}, 1)$</td>
<td style="text-align:left">$Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>
<td style="text-align:left">$(n^{[L]}, 209)$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>在python中，合理运用broadcast机制，如下：</li>
</ul>
<script type="math/tex; mode=display">
W = \begin{bmatrix}
    j  & k  & l\\
    m  & n & o \\
    p  & q & r 
\end{bmatrix}\;\;\; X = \begin{bmatrix}
    a  & b  & c\\
    d  & e & f \\
    g  & h & i 
\end{bmatrix} \;\;\; b =\begin{bmatrix}
    s  \\
    t  \\
    u
\end{bmatrix}\tag{2}</script><p>Then $WX + b$ will be:</p>
<script type="math/tex; mode=display">
WX + b = \begin{bmatrix}
(ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\
(ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\
(pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u
\end{bmatrix}\tag{3}</script><ul>
<li>练习<ol>
<li>实现L层神经网络参数初始化</li>
</ol>
</li>
<li>说明<ol>
<li>模型结构为：<em> [LINEAR -&gt; RELU] $ \times$ (L-1) -&gt; LINEAR -&gt; SIGMOID </em></li>
<li>使用<code>np.random.randn(shape)*0.01</code>初始化$W$</li>
<li>使用<code>np.zeros(shape)</code>初始化$b$</li>
<li>将$n^{[l]}$存储在<code>layer_dims</code>中，例如<code>layer_dims=[2,4,1]</code>意味着2个输入特征，一个隐藏层4个units，1个输出</li>
</ol>
</li>
<li><font size="5" color="red">强调</font>：<ol>
<li>这里<code>/ np.sqrt(layer_dims[layer-1])</code>很重要，如果还是<code>*0.01</code>，会导致模型cost降不下去</li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layers_initialize_parameters(layer_dims):
    """
    L层神经网络参数初始化
    :param layer_dims: python array (list) containing the dimensions of each layer in our network

    :return: 
        parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
        Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
        bl -- bias vector of shape (layer_dims[l], 1)
    """
    np.random.seed(1)
    parameters = {}
    L = len(layer_dims)

    # 这里/ np.sqrt(layer_dims[layer-1])很重要，如果还是*0.01，会导致模型cost降不下去
    for layer in range(1, L):
        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1]) / np.sqrt(layer_dims[layer-1]) # * 0.01
        parameters['b' + str(layer)] = np.zeros((layer_dims[layer], 1))

        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))
        assert(parameters['b' + str(layer)].shape == (layer_dims[layer], 1))
    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_L_layers_initialize_parameters(layer_dims=[5, 4, 3])
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[ 0.72642933, -0.27358579, -0.23620559, -0.47984616,  0.38702206],
        [-1.0292794 ,  0.78030354, -0.34042208,  0.14267862, -0.11152182],
        [ 0.65387455, -0.92132293, -0.14418936, -0.17175433,  0.50703711],
        [-0.49188633, -0.07711224, -0.39259022,  0.01887856,  0.26064289]]),
 &#39;b1&#39;: array([[0.],
        [0.],
        [0.],
        [0.]]),
 &#39;W2&#39;: array([[-0.55030959,  0.57236185,  0.45079536,  0.25124717],
        [ 0.45042797, -0.34186393, -0.06144511, -0.46788472],
        [-0.13394404,  0.26517773, -0.34583038, -0.19837676]]),
 &#39;b2&#39;: array([[0.],
        [0.],
        [0.]])}
</code></pre><h2 id="5-forward-propagation"><a href="#5-forward-propagation" class="headerlink" title="5-forward propagation"></a>5-forward propagation</h2><h3 id="5-1-linear-forward"><a href="#5-1-linear-forward" class="headerlink" title="5.1-linear forward"></a>5.1-linear forward</h3><ul>
<li>公式<script type="math/tex; mode=display">
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[0]} = X</script></li>
<li>练习<ol>
<li>实现<code>func_linear_forward()</code>函数</li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_forward(A, W, b):
    """
    linear forward
    :param A:
    :param W:
    :param b:
    :return Z,chche:
        Z -- the input of the activation function, also called pre-activation parameter 
        cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """
    Z = np.dot(W, A) + b

    assert(Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b)
    return Z, cache
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A, W, b = func_linear_forward_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">print(A.shape, W.shape, b.shape)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(3, 2) (1, 3) (1, 1)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">Z, linear_cache = func_linear_forward(A, W, b)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">Z
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[ 3.26295337, -1.23429987]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_cache
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(array([[ 1.62434536, -0.61175641],
        [-0.52817175, -1.07296862],
        [ 0.86540763, -2.3015387 ]]),
 array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]),
 array([[-0.24937038]]))
</code></pre><h3 id="5-2-linear-activation-forward"><a href="#5-2-linear-activation-forward" class="headerlink" title="5.2-linear activation forward"></a>5.2-linear activation forward</h3><ul>
<li>activation method:<ol>
<li>sigmoid</li>
<li>relu</li>
</ol>
</li>
<li>公式<script type="math/tex; mode=display">
  A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})</script></li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_activation_forward(A_prev, W, b, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)
    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)
    :param b: bias vector, numpy array of shape (size of the current layer, 1)
    :param activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
    :return:
        A -- the output of the activation function, also called the post-activation value 
        cache -- a python dictionary containing "linear_cache" and "activation_cache";
                 stored for computing the backward pass efficiently
    """
    Z, linear_cache = func_linear_forward(A_prev, W, b)
    if activation == 'sigmoid':
        A, activation_cache = func_sigmoid(Z)
    elif activation == 'relu':
        A, activation_cache = func_relu(Z)
    else:
        raise ValueError('activation param')

    assert(A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache)
    return A, cache
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A_prev, W, b = func_linear_activation_forward_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">print(A_prev.shape, W.shape, b.shape)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(3, 2) (1, 3) (1, 1)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, linear_activation_cache = func_linear_activation_forward(A_prev, W, b, activation='sigmoid')
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[0.96890023, 0.11013289]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_activation_cache
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>((array([[-0.41675785, -0.05626683],
         [-2.1361961 ,  1.64027081],
         [-1.79343559, -0.84174737]]),
  array([[ 0.50288142, -1.24528809, -1.05795222]]),
  array([[-0.90900761]])),
 array([[ 3.43896131, -2.08938436]]))
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, linear_activation_cache = func_linear_activation_forward(A_prev, W, b, activation='relu')
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[3.43896131, 0.        ]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_activation_cache
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>((array([[-0.41675785, -0.05626683],
         [-2.1361961 ,  1.64027081],
         [-1.79343559, -0.84174737]]),
  array([[ 0.50288142, -1.24528809, -1.05795222]]),
  array([[-0.90900761]])),
 array([[ 3.43896131, -2.08938436]]))
</code></pre><h3 id="5-3-L-layer-forward"><a href="#5-3-L-layer-forward" class="headerlink" title="5.3-L layer forward"></a>5.3-L layer forward</h3><ul>
<li>如图：</li>
</ul>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/model_architecture_kiank.png" alt="model_architecture_ki.png"></p>
<ul>
<li>练习：<ol>
<li>实现<code>func_L_model_forward()</code></li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_model_forward(X, parameters):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation

    :param X -- data, numpy array of shape (input size, number of examples)
    :param parameters -- output of initialize_parameters_deep()

    :return:
        AL -- last post-activation value
        caches -- list of caches containing:
                    every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                    the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """
    caches = []
    A = X
    L = len(parameters) // 2

    for layer in range(1, L):
        A_prev = A
        W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]
        A, cache = func_linear_activation_forward(A_prev, W, b, 'relu')
        caches.append(cache)

    A_prev = A
    layer = L
    W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]
    A, cache = func_linear_activation_forward(A_prev, W, b, 'sigmoid')
    caches.append(cache)

    assert(A.shape == (1, X.shape[1]))
    return A, caches
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">X, parameters = func_L_model_forward_test_case_2hidden()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">X
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],
       [-2.48678065,  0.91325152,  1.12706373, -1.51409323],
       [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],
       [-0.33588161,  1.23773784,  0.11112817,  0.12915125],
       [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],
        [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],
        [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],
        [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),
 &#39;b1&#39;: array([[ 1.38503523],
        [-0.51962709],
        [-0.78015214],
        [ 0.95560959]]),
 &#39;W2&#39;: array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],
        [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],
        [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]),
 &#39;b2&#39;: array([[ 1.50278553],
        [-0.59545972],
        [ 0.52834106]]),
 &#39;W3&#39;: array([[ 0.9398248 ,  0.42628539, -0.75815703]]),
 &#39;b3&#39;: array([[-0.16236698]])}
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, caches = func_L_model_forward(X, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">A
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[0.03921668, 0.70498921, 0.19734387, 0.04728177]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">caches
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>[((array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],
          [-2.48678065,  0.91325152,  1.12706373, -1.51409323],
          [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],
          [-0.33588161,  1.23773784,  0.11112817,  0.12915125],
          [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]]),
   array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],
          [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],
          [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],
          [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),
   array([[ 1.38503523],
          [-0.51962709],
          [-0.78015214],
          [ 0.95560959]])),
  array([[-5.23825714,  3.18040136,  0.4074501 , -1.88612721],
         [-2.77358234, -0.56177316,  3.18141623, -0.99209432],
         [ 4.18500916, -1.78006909, -0.14502619,  2.72141638],
         [ 5.05850802, -1.25674082, -3.54566654,  3.82321852]])),
 ((array([[0.        , 3.18040136, 0.4074501 , 0.        ],
          [0.        , 0.        , 3.18141623, 0.        ],
          [4.18500916, 0.        , 0.        , 2.72141638],
          [5.05850802, 0.        , 0.        , 3.82321852]]),
   array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],
          [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],
          [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]),
   array([[ 1.50278553],
          [-0.59545972],
          [ 0.52834106]])),
  array([[ 2.2644603 ,  1.09971298, -2.90298027,  1.54036335],
         [ 6.33722569, -2.38116246, -4.11228806,  4.48582383],
         [10.37508342, -0.66591468,  1.63635185,  8.17870169]])),
 ((array([[ 2.2644603 ,  1.09971298,  0.        ,  1.54036335],
          [ 6.33722569,  0.        ,  0.        ,  4.48582383],
          [10.37508342,  0.        ,  1.63635185,  8.17870169]]),
   array([[ 0.9398248 ,  0.42628539, -0.75815703]]),
   array([[-0.16236698]])),
  array([[-3.19864676,  0.87117055, -1.40297864, -3.00319435]]))]
</code></pre><h2 id="6-损失函数"><a href="#6-损失函数" class="headerlink" title="6-损失函数"></a>6-损失函数</h2><ul>
<li>cross entropy公式</li>
</ul>
<script type="math/tex; mode=display">
-\frac{1}{m} \sum\limits_{i = 1}^{m} \left(y^{(i)}\log(a^{[L] (i)}) + (1-y^{(i)})\log(1- a^{[L](i)})\right)</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_compute_cost(AL, Y):
    """
    Implement the cost function defined by equation (7).

    :param AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    :param Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    :return:
        cost -- cross-entropy cost
    """
    m = Y.shape[1]

#     cost = -1 / m * np.sum(np.multiply(Y, np.log(AL))+np.multiply(1-Y, np.log(1-AL)))
    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))

    cost = np.squeeze(cost)

    assert(cost.shape == ())
    return cost
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">Y, AL = func_compute_cost_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">cost = func_compute_cost(AL, Y)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">cost
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array(0.4149316)
</code></pre><h2 id="7-backward-propagation"><a href="#7-backward-propagation" class="headerlink" title="7-backward propagation"></a>7-backward propagation</h2><ul>
<li>反向传播模块是为了计算梯度，从而更新参数</li>
<li>反向传播图</li>
</ul>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/backprop.png"></p>
<caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>

<ul>
<li>反向传播公式</li>
</ul>
<script type="math/tex; mode=display">
\frac{d \mathcal{L}(a^{[2]},y)}{ {dz^{[1]}} } = \frac{d\mathcal{L}(a^{[2]},y)}{ {da^{[2]}} }\frac{ {da^{[2]} }}{ {dz^{[2]}} }\frac{ {dz^{[2]}} }{ {da^{[1]}} }\frac{ {da^{[1]}} }{ {dz^{[1]}} }  \\
dW^{[1]} = \frac{\partial \mathcal{L}}{\partial W^{[1]}} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial W^{[1]}}  \\
db^{[1]} = \frac{\partial \mathcal{L}}{\partial b^{[1]}} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial b^{[1]}}</script><ul>
<li>练习<ol>
<li>实现linear backward</li>
<li>实现激活函数的backward</li>
<li>实现L层的backward</li>
</ol>
</li>
</ul>
<h3 id="7-1-linear-backward"><a href="#7-1-linear-backward" class="headerlink" title="7.1-linear backward"></a>7.1-linear backward</h3><ul>
<li>对于layer l，<ol>
<li>linear forward是：$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$</li>
<li>linear backward是：$dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$</li>
</ol>
</li>
<li><p>如下图</p>
<p>  <img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/linearback_kiank.png" style="width:250px;height:300px;"></p>
  <caption><center> **Figure 4** </center></caption>
</li>
<li><p>求解$(dW^{[l]}, db^{[l]} dA^{[l-1]})$如下：</p>
<script type="math/tex; mode=display">
  dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \\
  db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)} \\
  dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}</script></li>
<li>解释：<ol>
<li>$dW^{[l]}$携带$1/m$是因为一共$m$个样本，每个样本都会计算一次损失以及$dW^{[l]}$</li>
<li>$W^{[l]}$的维度为$(n^{[l]}, n^{[l-1]})$</li>
<li>$A^{[l-1]}$的维度为$(n^{[l-1]}, m)$，故其导数不携带$1/m$</li>
</ol>
</li>
<li>练习：<ol>
<li>实现<code>func_linear_backward()</code>函数</li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_backward(dZ, cache):
    """
    Implement the linear portion of backward propagation for a single layer (layer l)

    :param dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    :param cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1 / m * np.dot(dZ, A_prev.T)
    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)  # axis=1是行记录求和
    dA_prev = np.dot(W.T, dZ)

    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)
    assert(dA_prev.shape == A_prev.shape)
    return dA_prev, dW, db
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">dZ, linear_cache = func_linear_backward_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">dZ.shape
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(1, 2)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for i in linear_cache:
    print(i.shape)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>(3, 2)
(1, 3)
(1, 1)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db = func_linear_backward(dZ, linear_cache)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[ 0.51822968, -0.19517421],
       [-0.40506361,  0.15255393],
       [ 2.37496825, -0.89445391]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dW
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[-0.10076895,  1.40685096,  1.64992505]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">db
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[0.50629448]])
</code></pre><h3 id="7-2-linear-activation-backward"><a href="#7-2-linear-activation-backward" class="headerlink" title="7.2-linear activation backward"></a>7.2-linear activation backward</h3><ul>
<li>公式</li>
</ul>
<script type="math/tex; mode=display">
dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})</script><ul>
<li>练习<ol>
<li>实现<code>func_linear_activation_backward()</code>函数</li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_activation_backward(dA, cache, activation):
    """
    Implement the backward propagation for the LINEAR->ACTIVATION layer.

    :param dA -- post-activation gradient for current layer l 
    :param cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    :param activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    :returns
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache = cache

    if activation == 'relu':
        dZ = func_relu_backward(dA, activation_cache)
    elif activation == 'sigmoid':
        dZ = func_sigmoid_backward(dA, activation_cache)
    else:
        raise ValueError('activation param')
    dA_prev, dW, db = func_linear_backward(dZ, linear_cache)
    return dA_prev, dW, db
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">AL, linear_activation_cache = func_linear_activation_backward_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">AL
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([[-0.41675785, -0.05626683]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_activation_cache
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>((array([[-2.1361961 ,  1.64027081],
         [-1.79343559, -0.84174737],
         [ 0.50288142, -1.24528809]]),
  array([[-1.05795222, -0.90900761,  0.55145404]]),
  array([[2.29220801]])),
 array([[ 0.04153939, -1.11792545]]))
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db = func_linear_activation_backward(AL, linear_activation_cache, activation='sigmoid')
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(array([[ 0.11017994,  0.01105339],
        [ 0.09466817,  0.00949723],
        [-0.05743092, -0.00576154]]),
 array([[ 0.10266786,  0.09778551, -0.01968084]]),
 array([[-0.05729622]]))
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db = func_linear_activation_backward(AL, linear_activation_cache, activation='relu')
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(array([[ 0.44090989,  0.        ],
        [ 0.37883606,  0.        ],
        [-0.2298228 ,  0.        ]]),
 array([[ 0.44513824,  0.37371418, -0.10478989]]),
 array([[-0.20837892]]))
</code></pre><h3 id="7-3-L-layer-backward"><a href="#7-3-L-layer-backward" class="headerlink" title="7.3-L layer backward"></a>7.3-L layer backward</h3><ul>
<li>结构图</li>
</ul>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/mn_backward.png" style="width:450px;height:300px;"></p>
<caption><center>  **Figure 5** : Backward pass  </center></caption>

<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/mn_backward1.jpg" style="width:450px;height:300px;"></p>
<caption><center>  **Figure 5** : Backward pass 截图  </center></caption>

<ul>
<li><p>output层的梯度</p>
<p>  <code>dAL</code> $= \frac{\partial \mathcal{L}}{\partial A^{[L]}}$=<code>- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></p>
</li>
<li><p>练习</p>
<ol>
<li>实现<code>func_L_model_backward()</code>函数</li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group

    :param AL -- probability vector, output of the forward propagation (L_model_forward())
    :param Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
    :param caches -- list of caches containing:
                    every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                    the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])

    :return:
        grads -- A dictionary with the gradients
                 grads["dA" + str(l)] = ... 
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ... 
    """
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)

    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))

    cur_cache = caches[L-1]
    grads['dA' + str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = func_linear_activation_backward(dAL, cur_cache, activation='sigmoid')

    for layer in reversed(range(L-1)):
        cur_cache = caches[layer]
        dA_prev_tmp, dW_tmp, db_tmp = func_linear_activation_backward(grads["dA"+str(layer+1)], cur_cache, activation='relu')
        grads['dA'+str(layer)] = dA_prev_tmp
        grads['dW'+str(layer+1)] = dW_tmp
        grads['db'+str(layer+1)] = db_tmp

#     current_cache = caches[L-1]
#     grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = func_linear_activation_backward(dAL, current_cache, activation = "sigmoid")

#     for l in reversed(range(L-1)):
#         # lth layer: (RELU -> LINEAR) gradients.
#         current_cache = caches[l]
#         dA_prev_temp, dW_temp, db_temp = func_linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation = "relu")
#         grads["dA" + str(l + 1)] = dA_prev_temp
#         grads["dW" + str(l + 1)] = dW_temp
#         grads["db" + str(l + 1)] = db_temp

    return grads
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">AL, Y_assess, caches = func_L_model_backward_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">AL, Y_assess, caches
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>(array([[1.78862847, 0.43650985]]),
 array([[1, 0]]),
 (((array([[ 0.09649747, -1.8634927 ],
           [-0.2773882 , -0.35475898],
           [-0.08274148, -0.62700068],
           [-0.04381817, -0.47721803]]),
    array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],
           [ 0.05003364, -0.40467741, -0.54535995, -1.54647732],
           [ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]),
    array([[ 1.48614836],
           [ 0.23671627],
           [-1.02378514]])),
   array([[-0.7129932 ,  0.62524497],
          [-0.16051336, -0.76883635],
          [-0.23003072,  0.74505627]])),
  ((array([[ 1.97611078, -1.24412333],
           [-0.62641691, -0.80376609],
           [-2.41908317, -0.92379202]]),
    array([[-1.02387576,  1.12397796, -0.13191423]]),
    array([[-1.62328545]])),
   array([[ 0.64667545, -0.35627076]]))))
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">grads = func_L_model_backward(AL, Y_assess, caches)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">grads
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>{&#39;dA1&#39;: array([[ 0.12913162, -0.44014127],
        [-0.14175655,  0.48317296],
        [ 0.01663708, -0.05670698]]),
 &#39;dW2&#39;: array([[-0.39202432, -0.13325855, -0.04601089]]),
 &#39;db2&#39;: array([[0.15187861]]),
 &#39;dA0&#39;: array([[ 0.        ,  0.52257901],
        [ 0.        , -0.3269206 ],
        [ 0.        , -0.32070404],
        [ 0.        , -0.74079187]]),
 &#39;dW1&#39;: array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],
        [0.        , 0.        , 0.        , 0.        ],
        [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]),
 &#39;db1&#39;: array([[-0.22007063],
        [ 0.        ],
        [-0.02835349]])}
</code></pre><h3 id="7-4-更新参数"><a href="#7-4-更新参数" class="headerlink" title="7.4-更新参数"></a>7.4-更新参数</h3><ul>
<li>更新公式</li>
</ul>
<script type="math/tex; mode=display">
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \\
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><ul>
<li>练习<ol>
<li>实现<code>func_update_parameters()</code>函数</li>
</ol>
</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_update_parameters(parameters, grads, lr):
    """
    Update parameters using gradient descent

    :param parameters -- python dictionary containing your parameters 
    :param grads -- python dictionary containing your gradients, output of L_model_backward
    :param lr: learning rate
    :return:
        parameters -- python dictionary containing your updated parameters 
                      parameters["W" + str(l)] = ... 
                      parameters["b" + str(l)] = ...
    """
    L = len(parameters) // 2

    for layer in range(1, L+1):
        parameters['W'+str(layer)] = parameters['W'+str(layer)] - lr * grads['dW' + str(layer)]
        parameters['b'+str(layer)] = parameters['b'+str(layer)] - lr * grads['db' + str(layer)]

    return parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, grads = func_update_parameters_test_case()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, grads
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>({&#39;W1&#39;: array([[-0.41675785, -0.05626683, -2.1361961 ,  1.64027081],
         [-1.79343559, -0.84174737,  0.50288142, -1.24528809],
         [-1.05795222, -0.90900761,  0.55145404,  2.29220801]]),
  &#39;b1&#39;: array([[ 0.04153939],
         [-1.11792545],
         [ 0.53905832]]),
  &#39;W2&#39;: array([[-0.5961597 , -0.0191305 ,  1.17500122]]),
  &#39;b2&#39;: array([[-0.74787095]])},
 {&#39;dW1&#39;: array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],
         [-0.2773882 , -0.35475898, -0.08274148, -0.62700068],
         [-0.04381817, -0.47721803, -1.31386475,  0.88462238]]),
  &#39;db1&#39;: array([[0.88131804],
         [1.70957306],
         [0.05003364]]),
  &#39;dW2&#39;: array([[-0.40467741, -0.54535995, -1.54647732]]),
  &#39;db2&#39;: array([[0.98236743]])})
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_update_parameters(parameters, grads, 0.1)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>{&#39;W1&#39;: array([[-0.59562069, -0.09991781, -2.14584584,  1.82662008],
        [-1.76569676, -0.80627147,  0.51115557, -1.18258802],
        [-1.0535704 , -0.86128581,  0.68284052,  2.20374577]]),
 &#39;b1&#39;: array([[-0.04659241],
        [-1.28888275],
        [ 0.53405496]]),
 &#39;W2&#39;: array([[-0.55569196,  0.0354055 ,  1.32964895]]),
 &#39;b2&#39;: array([[-0.84610769]])}
</code></pre><h3 id="7-5-预测"><a href="#7-5-预测" class="headerlink" title="7.5-预测"></a>7.5-预测</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_predict(X, y, parameters):
    """
    This function is used to predict the results of a  L-layer neural network.

    :param X -- data set of examples you would like to label
    :param parameters -- parameters of the trained model
    :return:
        p -- predictions for the given dataset X
    """
    m = X.shape[1]
    L = len(parameters) // 2
    p = np.zeros((1, m))

    probas, caches = func_L_model_forward(X, parameters)

    for i in range(probas.shape[1]):
        if probas[0, i] > 0.5:
            p[0, i] = 1
        else:
            p[0, i] = 0

    print('acc: {}'.format(np.sum(p==y)/m))
    return p
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="8-整合到model"><a href="#8-整合到model" class="headerlink" title="8-整合到model"></a>8-整合到model</h2><h3 id="8-1-two-layer-neural-network"><a href="#8-1-two-layer-neural-network" class="headerlink" title="8.1-two layer neural network"></a>8.1-two layer neural network</h3><p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/2layerNN_kiank.png" style="width:650px;height:400px;"></p>
<caption><center> <u>Figure 2</u>: 2-layer neural network. <br> The model can be summarized as: ***INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT***. </center></caption>



<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_two_layer_nn_model(X, Y, layer_dims, lr=0.001, num_epochs=10000, print_cost=False):
    """
    双层神经网络模型
    :param X:
    :param Y:
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param lr: learning rate
    :param num_epochs:
    :param print_cost:
    :return params
    """
    np.random.seed(1)
    grads = {}
    costs = []
    m = X.shape[1]
    (n_x, n_h, n_y) = layer_dims

    # 参数初始化
    parameters = func_2_layers_initialize_parameters(n_x, n_h, n_y)

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    # loop
    for epoch in range(num_epochs):
        # 前向传播
        A1, cache1 = func_linear_activation_forward(X, W1, b1, 'relu')
        A2, cache2 = func_linear_activation_forward(A1, W2, b2, 'sigmoid')

        # 计算损失
        cost = func_compute_cost(A2, Y)

        # 后向传播
        dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))

        dA1, dW2, db2 = func_linear_activation_backward(dA2, cache2, 'sigmoid')
        dA0, dW1, db1 = func_linear_activation_backward(dA1, cache1, 'relu')

        grads['dW1'] = dW1
        grads['db1'] = db1
        grads['dW2'] = dW2
        grads['db2'] = db2

        # 更新参数
        parameters = func_update_parameters(parameters, grads, lr)

        W1 = parameters['W1']
        b1 = parameters['b1']
        W2 = parameters['W2']
        b2 = parameters['b2']

        # 打印信息
        if print_cost and epoch % 100 == 0:
            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))
        if epoch % 100 == 0:
            costs.append(cost)

    return parameters, costs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="8-2-L-layer-deep-neural-network"><a href="#8-2-L-layer-deep-neural-network" class="headerlink" title="8.2 L layer deep neural network"></a>8.2 L layer deep neural network</h3><p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/LlayerNN_kiank.png" style="width:650px;height:400px;"></p>
<caption><center> <u>Figure 3</u>: L-layer neural network. <br> The model can be summarized as: ***[LINEAR -> RELU] $\times$ (L-1) -> LINEAR -> SIGMOID***</center></caption>

<p><u>Detailed Architecture of figure 3</u>:</p>
<ul>
<li>The input is a (64,64,3) image which is flattened to a vector of size (12288,1).</li>
<li>The corresponding vector: $[x_0,x_1,…,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[1]}$. The result is called the linear unit.</li>
<li>Next, you take the relu of the linear unit. This process could be repeated several times for each $(W^{[l]}, b^{[l]})$ depending on the model architecture.</li>
<li>Finally, you take the sigmoid of the final linear unit. If it is greater than 0.5, you classify it to be a cat.</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layer_dnn_model(X, Y, layer_dims, lr=0.001, num_epochs=10000, print_cost=False):
    """
    多层神经网络模型
    :param X:
    :param Y:
    :param layer_dims: python array (list) containing the dimensions of each layer in our network
    :param lr: learning rate
    :param num_epochs:
    :param print_cost:
    :return params
    """
    np.random.seed(1)
    costs = []
#     m = X.shape[1]

    # 参数初始化
    parameters = func_L_layers_initialize_parameters(layer_dims)

    # loop
    for epoch in range(num_epochs):
        # 前向传播
        AL, caches = func_L_model_forward(X, parameters)

        # 计算损失
        cost = func_compute_cost(AL, Y)

        # 后向传播
        grads = func_L_model_backward(AL, Y, caches)

        # 更新参数
        parameters = func_update_parameters(parameters, grads, lr)

        # 打印信息
        if print_cost and epoch % 100 == 0:
            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))
        if epoch % 100 == 0:
            costs.append(cost)

    return parameters, costs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="9-应用-图片分类"><a href="#9-应用-图片分类" class="headerlink" title="9-应用=图片分类"></a>9-应用=图片分类</h2><ul>
<li>You will use use the functions you’d implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification.</li>
</ul>
<h3 id="9-1-一些函数"><a href="#9-1-一些函数" class="headerlink" title="9.1-一些函数"></a>9.1-一些函数</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_load_dataset():
    """
    from lr_utils import load_dataset
    :return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
    """
    train_dataset = h5py.File('./深度学习之吴恩达课程作业1/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('./深度学习之吴恩达课程作业1/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_print_mislabeled_images(classes, X, y, p):
    """
    Plots images where predictions and truth were different.
    :param X -- dataset
    :param y -- true labels
    :param p -- predictions
    """
    a = p + y
    mislabeled_indices = np.asarray(np.where(a == 1))
    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots
    num_images = len(mislabeled_indices[0])
    for i in range(num_images):
        index = mislabeled_indices[1][i]

        plt.subplot(2, num_images, i + 1)
        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')
        plt.axis('off')
        plt.title("Prediction: " + classes[int(p[0,index])].decode("utf-8") + " \n Class: " + classes[y[0,index]].decode("utf-8"))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="9-2-导入数据集"><a href="#9-2-导入数据集" class="headerlink" title="9.2-导入数据集"></a>9.2-导入数据集</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = func_load_dataset()
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">print(train_set_x_orig.shape)
print(test_set_x_orig.shape)
print(train_set_y_orig.shape)
print(test_set_y_orig.shape)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>(209, 64, 64, 3)
(50, 64, 64, 3)
(1, 209)
(1, 50)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">index = 10
plt.imshow(train_set_x_orig[index])
print('y=' + str(train_set_y_orig[:, index]) + ' it is a ' + classes[np.squeeze(train_set_y_orig[:, index])].decode('utf-8'))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre><code>y=[0] it is a non-cat
</code></pre><p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/output_92_1.png" alt="output_92_1"></p>
<ul>
<li>重塑数据集，将大小(n, length, height, 3)的重塑为(length*height*3, n)</li>
<li>如下图</li>
</ul>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/imvectorkiank.png" alt="imvector"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

print(train_set_x_flatten.shape)
print(test_set_x_flatten.shape)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>(12288, 209)
(12288, 50)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_flatten[:5, 0]
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([17, 31, 56, 22, 33], dtype=uint8)
</code></pre><ul>
<li>数据预处理</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x = train_set_x_flatten / 255.
test_set_x = test_set_x_flatten / 255.
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x[:5, 0]
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>array([0.06666667, 0.12156863, 0.21960784, 0.08627451, 0.12941176])
</code></pre><h3 id="9-3-使用two-layer-neural-network"><a href="#9-3-使用two-layer-neural-network" class="headerlink" title="9.3-使用two layer neural network"></a>9.3-使用two layer neural network</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">n_x, n_h, n_y = train_set_x.shape[0], 7, 1
layer_dims = (n_x, n_h, n_y)
print(layer_dims)

lr = 0.0075
num_epochs = 2500
print_cost=True
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>(12288, 7, 1)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%time
parameters, costs = func_two_layer_nn_model(train_set_x, train_set_y_orig, layer_dims, lr, num_epochs, print_cost)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>cost after epoch 0: 0.693049735659989
cost after epoch 100: 0.6464320953428849
cost after epoch 200: 0.6325140647912678
cost after epoch 300: 0.6015024920354665
cost after epoch 400: 0.5601966311605748
cost after epoch 500: 0.5158304772764731
cost after epoch 600: 0.47549013139433266
cost after epoch 700: 0.433916315122575
cost after epoch 800: 0.40079775362038844
cost after epoch 900: 0.3580705011323798
cost after epoch 1000: 0.3394281538366413
cost after epoch 1100: 0.30527536361962654
cost after epoch 1200: 0.2749137728213015
cost after epoch 1300: 0.24681768210614827
cost after epoch 1400: 0.1985073503746611
cost after epoch 1500: 0.17448318112556638
cost after epoch 1600: 0.17080762978096792
cost after epoch 1700: 0.11306524562164719
cost after epoch 1800: 0.09629426845937153
cost after epoch 1900: 0.08342617959726863
cost after epoch 2000: 0.07439078704319084
cost after epoch 2100: 0.06630748132267933
cost after epoch 2200: 0.05919329501038171
cost after epoch 2300: 0.053361403485605585
cost after epoch 2400: 0.04855478562877018
CPU times: user 19min 11s, sys: 13min 2s, total: 32min 14s
Wall time: 34.9 s
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot the cost

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per tens)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/output_102_0.png" alt="output_102_0"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_train = func_predict(train_set_x, train_set_y_orig, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>acc: 1.0
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_test = func_predict(test_set_x, test_set_y_orig, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>acc: 0.72
</code></pre><h3 id="9-4-使用L层深度神经网络"><a href="#9-4-使用L层深度神经网络" class="headerlink" title="9.4-使用L层深度神经网络"></a>9.4-使用L层深度神经网络</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">layer_dims = [train_set_x.shape[0], 7, 1]
print(layer_dims)

lr = 0.0075
num_epochs = 2500
print_cost=True
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>[12288, 7, 1]
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%time
parameters, costs = func_L_layer_dnn_model(train_set_x, train_set_y_orig, layer_dims, lr, num_epochs, print_cost)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>cost after epoch 0: 0.6950464961800915
cost after epoch 100: 0.5892596054583805
cost after epoch 200: 0.5232609173622991
cost after epoch 300: 0.4497686396221906
cost after epoch 400: 0.42090021618838985
cost after epoch 500: 0.3724640306174595
cost after epoch 600: 0.34742051870201895
cost after epoch 700: 0.3171919198737027
cost after epoch 800: 0.26643774347746585
cost after epoch 900: 0.21991432807842595
cost after epoch 1000: 0.14357898893623774
cost after epoch 1100: 0.45309212623221046
cost after epoch 1200: 0.09499357670093511
cost after epoch 1300: 0.08014128076781372
cost after epoch 1400: 0.06940234005536465
cost after epoch 1500: 0.06021664023174592
cost after epoch 1600: 0.05327415758001879
cost after epoch 1700: 0.04762903262098435
cost after epoch 1800: 0.04297588879436871
cost after epoch 1900: 0.039036074365138215
cost after epoch 2000: 0.03568313638049029
cost after epoch 2100: 0.03291526373054677
cost after epoch 2200: 0.030472193059120623
cost after epoch 2300: 0.028387859212946124
cost after epoch 2400: 0.026615212372776063
CPU times: user 18min 20s, sys: 11min 49s, total: 30min 10s
Wall time: 32.4 s
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot the cost

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per tens)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/output_108_0.png" alt="output_108_0"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_train = func_predict(train_set_x, train_set_y_orig, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>acc: 1.0
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_test = func_predict(test_set_x, test_set_y_orig, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>acc: 0.74
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">layer_dims = [train_set_x.shape[0], 20, 7, 5, 1]
print(layer_dims)

lr = 0.0075
num_epochs = 2500
print_cost=True
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>[12288, 20, 7, 5, 1]
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%time
parameters, costs = func_L_layer_dnn_model(train_set_x, train_set_y_orig, layer_dims, lr, num_epochs, print_cost)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>cost after epoch 0: 0.7717493284237686
cost after epoch 100: 0.6720534400822914
cost after epoch 200: 0.6482632048575212
cost after epoch 300: 0.6115068816101354
cost after epoch 400: 0.5670473268366111
cost after epoch 500: 0.5401376634547801
cost after epoch 600: 0.5279299569455267
cost after epoch 700: 0.46547737717668514
cost after epoch 800: 0.36912585249592794
cost after epoch 900: 0.39174697434805344
cost after epoch 1000: 0.3151869888600617
cost after epoch 1100: 0.2726998441789385
cost after epoch 1200: 0.23741853400268137
cost after epoch 1300: 0.19960120532208644
cost after epoch 1400: 0.18926300388463305
cost after epoch 1500: 0.16118854665827748
cost after epoch 1600: 0.14821389662363316
cost after epoch 1700: 0.13777487812972944
cost after epoch 1800: 0.1297401754919012
cost after epoch 1900: 0.12122535068005211
cost after epoch 2000: 0.1138206066863371
cost after epoch 2100: 0.10783928526254132
cost after epoch 2200: 0.10285466069352679
cost after epoch 2300: 0.10089745445261786
cost after epoch 2400: 0.09287821526472395
CPU times: user 28min 23s, sys: 17min 29s, total: 45min 53s
Wall time: 49.3 s
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot the cost

plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per tens)')
plt.title("Learning rate =" + str(lr))
plt.show()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/output_113_0.png" alt="output_113_0"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_train = func_predict(train_set_x, train_set_y_orig, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>acc: 0.9856459330143541
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_test = func_predict(test_set_x, test_set_y_orig, parameters)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>acc: 0.8
</code></pre><h3 id="9-5-结果分析"><a href="#9-5-结果分析" class="headerlink" title="9.5-结果分析"></a>9.5-结果分析</h3><ul>
<li>让我们看看一些被错误分类的图片</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">func_print_mislabeled_images(classes, test_set_x, test_set_y_orig, pred_test)
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/output_117_0.png" alt="output_117_0"></p>
<p><strong>一些模型在识别上犯错误的地方</strong></p>
<ul>
<li>Cat body in an unusual position</li>
<li>Cat appears against a background of a similar color</li>
<li>Unusual cat color and species</li>
<li>Camera Angle</li>
<li>Brightness of the picture</li>
<li>Scale variation (cat is very large or small in image) </li>
</ul>
<h3 id="9-6-识别自己的图片"><a href="#9-6-识别自己的图片" class="headerlink" title="9.6-识别自己的图片"></a>9.6-识别自己的图片</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">image_path = './深度学习之吴恩达课程作业1/cat_in_iran.jpg'
my_label_y = [1]
image = np.array(plt.imread(image_path))
image.shape
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre><code>(1115, 1114, 3)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">num_px = train_set_x_orig.shape[1]
my_image = np.array(Image.fromarray(image).resize((num_px, num_px))).reshape((1, num_px*num_px*3)).T
my_image.shape
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<pre><code>(12288, 1)
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">my_pred_image = func_predict(my_image, my_label_y, parameters)
my_pred_image
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>acc: 1.0





array([[1.]])
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.figure(figsize=(9, 9))
plt.imshow(image)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f02f60edeb0&gt;
</code></pre><p><img src="/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/output_123_1.png" alt="output_123_1"></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">print('y_pred=', np.squeeze(my_pred_image), ", your algorithm predicts a \"" + classes[int(np.squeeze(my_pred_image)),].decode("utf-8") +  "\" picture.")
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre><code>y_pred= 1.0 , your algorithm predicts a &quot;cat&quot; picture.
</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">

<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io" rel="external nofollow noreferrer">Myhaa</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/">https://myhaa.github.io/2021/09/03/shen-du-xue-xi-ng-zhi-duo-ceng-yin-cang-ceng-shen-jing-wang-luo/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                                    <span class="chip bg-color">神经网络</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">感谢您的赏识！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'a1900fd4b8fb7a569ef7',
        clientSecret: 'd1176a5ad242e4887008f5d4389ea5a35f199c44',
        repo: 'myhaa.github.io',
        owner: 'myhaa',
        admin: ["myhaa"],
        id: '2021-09-03T09-19-18',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/09/06/shi-yong-gong-ju-zhi-ffmpeg/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/12.jpg" class="responsive-img" alt="实用工具之FFmpeg">
                        
                        <span class="card-title">实用工具之FFmpeg</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            FFmpeg视频处理入门教程
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-09-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/" class="post-category">
                                    实用工具
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/FFmpeg/">
                        <span class="chip bg-color">FFmpeg</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/08/11/shen-du-xue-xi-ng-zhi-yi-ceng-yin-cang-ceng-shen-jing-wang-luo/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/22.jpg" class="responsive-img" alt="深度学习之吴恩达课程作业2">
                        
                        <span class="card-title">深度学习之吴恩达课程作业2</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            一层隐藏层神经网络
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-08-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
                        <span class="chip bg-color">神经网络</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Myhaa's Blog<br />'
            + '文章作者: Myhaa<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>

    
<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">年份</span>
            <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    window.setTimeout("siteTime()", 1000);
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2019";
                    var startMonth = "11";
                    var startDate = "11";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">














    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="/libs/instantpage/instantpage.js" type="module"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
    

</body>

</html>
