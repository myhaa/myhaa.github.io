<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="推荐系统之算法介绍, 统计学 数据挖掘 机器学习 计算广告">
    <meta name="description" content="一、CF(Collaborative-Filtering)主要思想：
协同过滤推荐方法的主要思想是：利用已有用户群过去的行为或意见预测当前用户最可能喜欢哪些东西或对哪些东西感兴趣。
1、基于用户的协同过滤潜在假设：

如果用户过去有相似的偏">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>推荐系统之算法介绍 | Myhaa&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    
    <script src="/libs/jquery/jquery.min.js"></script>
    
<link rel="alternate" href="/atom.xml" title="Myhaa's Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Myhaa's Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Myhaa's Blog</div>
        <div class="logo-desc">
            
            要么孤独，要么庸俗
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/myhaa" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/myhaa" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        推荐系统之算法介绍
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D/">
                                <span class="chip bg-color">算法介绍</span>
                            </a>
                        
                            <a href="/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/">
                                <span class="chip bg-color">协同过滤</span>
                            </a>
                        
                            <a href="/tags/CTR%E9%A2%84%E4%BC%B0/">
                                <span class="chip bg-color">CTR预估</span>
                            </a>
                        
                            <a href="/tags/LR/">
                                <span class="chip bg-color">LR</span>
                            </a>
                        
                            <a href="/tags/FM/">
                                <span class="chip bg-color">FM</span>
                            </a>
                        
                            <a href="/tags/FFM/">
                                <span class="chip bg-color">FFM</span>
                            </a>
                        
                            <a href="/tags/DEEP-WIDE/">
                                <span class="chip bg-color">DEEP &amp; WIDE</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" class="post-category">
                                推荐系统
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-10-17
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                
				
                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="一、CF-Collaborative-Filtering"><a href="#一、CF-Collaborative-Filtering" class="headerlink" title="一、CF(Collaborative-Filtering)"></a>一、CF(Collaborative-Filtering)</h1><p>主要思想：</p>
<p>协同过滤推荐方法的主要思想是：利用已有用户群过去的行为或意见预测当前用户最可能喜欢哪些东西或对哪些东西感兴趣。</p>
<h2 id="1、基于用户的协同过滤"><a href="#1、基于用户的协同过滤" class="headerlink" title="1、基于用户的协同过滤"></a>1、基于用户的协同过滤</h2><p>潜在假设：</p>
<ul>
<li>如果用户过去有相似的偏好，那么他们未来也会有相似的偏好；</li>
<li>用户偏好不会随时间而变化</li>
</ul>
<p>主要思想：</p>
<ul>
<li>首先，给定一个评分数据集和当前用户的ID作为输入，找出与当前用户过去有相似偏好的其他用户；</li>
<li>然后，对当前用户没有评价过的物品，利用最近邻对物品的评分计算预测值。</li>
</ul>
<p>（1）协同推荐的评分数据库（例子1）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>物品1</th>
<th>物品2</th>
<th>物品3</th>
</tr>
</thead>
<tbody>
<tr>
<td>用户1</td>
<td>5</td>
<td>4</td>
<td>？</td>
</tr>
<tr>
<td>用户2</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>用户3</td>
<td>4</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>（2）算法步骤</p>
<ul>
<li>利用用户已有的物品评分<code>rate</code>计算用户之间的相似度$sim(a, b)$，相似度计算方法可以是<strong><code>pearson</code>相关系数、改进余弦相似度、<code>spearman</code>秩相关系数、均方差等</strong>。（计算相似度是用两个用户拥有共同行为物品的评分来计算）</li>
<li>针对要预测的每个用户，选择与其相似度排名最靠前的<code>k</code>个用户对指定物品的评分做加权计算来得到该用户对该物品的评分。</li>
</ul>
<p>（3）面临的挑战</p>
<ul>
<li>用户量和物品量很大时，算法性能不高</li>
<li>当需要扫描大量潜在近邻时，很难做到实时计算预测值</li>
</ul>
<h2 id="2、基于物品的协同过滤"><a href="#2、基于物品的协同过滤" class="headerlink" title="2、基于物品的协同过滤"></a>2、基于物品的协同过滤</h2><p>主要思想：</p>
<ul>
<li>利用物品间的相似度来计算预测值</li>
</ul>
<h2 id="3、疑难解答"><a href="#3、疑难解答" class="headerlink" title="3、疑难解答"></a>3、疑难解答</h2><h3 id="1-相似度的改进-热门item或者活跃user带来的问题"><a href="#1-相似度的改进-热门item或者活跃user带来的问题" class="headerlink" title="1. 相似度的改进=热门item或者活跃user带来的问题"></a>1. 相似度的改进=热门item或者活跃user带来的问题</h3><ul>
<li><a href="chrome-extension://cdonnmffkdaoajfknoeeecmchibpmkmg/assets/pdf/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fftp%2Farxiv%2Fpapers%2F1301%2F1301.7363.pdf" target="_blank" rel="noopener">参考论文: Empirical Analysis of Predictive Algorithm for Collaborative Filtering</a></li>
<li><a href="https://blog.csdn.net/songbinxu/article/details/79917267" target="_blank" rel="noopener">改进办法参考博客链接</a></li>
</ul>
<h1 id="二、LR-Logistic-Regression-FTRL"><a href="#二、LR-Logistic-Regression-FTRL" class="headerlink" title="二、LR(Logistic Regression)+FTRL"></a>二、LR(Logistic Regression)+FTRL</h1><h2 id="1、逻辑回归"><a href="#1、逻辑回归" class="headerlink" title="1、逻辑回归"></a>1、逻辑回归</h2><p>FTRL本质上是一种优化方法，最早由google提出并用于CTR预估。常被用于逻辑回归的优化，因此先简单介绍一下逻辑回归的内容。</p>
<h3 id="1-1-sigmoid函数"><a href="#1-1-sigmoid函数" class="headerlink" title="1.1 sigmoid函数"></a>1.1 sigmoid函数</h3><p>由于二分类结果是1或者0，这与数学的阶跃函数很类似，但是阶跃函数在x=0的位置会发生突变，这个突变在数学上很难处理。所以一般使用sigmoid函数来拟合：</p>
<script type="math/tex; mode=display">
g(z)={\frac 1{1+e^{-z}}} \qquad(1)</script><p>具体应用到逻辑回归算法中：</p>
<script type="math/tex; mode=display">
z={\omega}_0+{\omega}_1x_1+{\omega}_2x_2+......+{\omega}_nx_n=\sum_{i=0}^n{\omega}_ix_i=\mathbf{\omega^TX}   \qquad(2)</script><p>其中<script type="math/tex">x_i</script>表示样本属性（对于我们而言，就是标签IP）的值， <script type="math/tex">\omega_i</script>表示这个属性对应的系数（也就是算法需要计算的内容）。注意这里将<script type="math/tex">x_0</script>与<script type="math/tex">\omega_0</script>也代入了上述公式，其中前者恒为1。于是问题就变成了在训练样本中，已知属性x与最终分类结果y（1或者0）时，如何求得这些系数 <script type="math/tex">\omega_i</script>，使得损失最小。</p>
<h3 id="1-2-极大似然估计MLE与损失函数"><a href="#1-2-极大似然估计MLE与损失函数" class="headerlink" title="1.2 极大似然估计MLE与损失函数"></a>1.2 极大似然估计MLE与损失函数</h3><p>在机器学习理论中，损失函数（loss function）是用来衡量模型的预测值<script type="math/tex">f(x)</script>与真实值<script type="math/tex">Y</script>的不一致程度，它是一个非负实值函数，损失函数越小，模型越优（还需考虑过拟合等问题）。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子</p>
<script type="math/tex; mode=display">
\omega^* = \arg \min_\omega \frac{1}{m}{}\sum_{i=1}^{m} L(y_i, f(x_i; \omega)) + \lambda\  \Phi(\omega)  \qquad(3)</script><p>其中m表示样本的数量。对于逻辑回归，其loss function是log损失，这可以通过极大似然估计进行推导得到。</p>
<p>首先，给定一个样本<script type="math/tex">x</script>，可以使用一个线性函数对自变量进行线性组合，即上述的（2）式子：</p>
<script type="math/tex; mode=display">
z={\omega}_0+{\omega}_1x_1+{\omega}_2x_2+......+{\omega}_nx_n=\sum_{i=0}^n{\omega}_ix_i=\mathbf{\omega^TX} \qquad(4)</script><p>根据sigmoid函数，我们可以得出预测函数的表达式为：</p>
<script type="math/tex; mode=display">
h_{\omega}(x) = g(\omega^Tx) = \frac{1}{1 + e^{-\omega^Tx}}  \qquad(5)</script><p>上式表示<script type="math/tex">y=1</script>的预测函数为<script type="math/tex">h_{\omega}(x)</script>。在这里，假设因变量<script type="math/tex">y</script>服从伯努利分布，那么可以得到下列两个式子：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y=1 | x) &= h_{\omega} (x)         \quad\qquad(6)\\
p(y=0 | x) &= 1 - h_{\omega} (x)      \qquad(7)
\end{aligned}</script><p>而对于上面的两个表达式，通过观察，我们发现，可以将其合并为以下表达式：</p>
<script type="math/tex; mode=display">
p(y | x) = h_{\omega} (x)^y (1-h_{\omega} (x))^{1-y} \qquad(8)</script><p>根据上面的式子，给定一定的样本之后，我们可以构造出似然函数，然后可以使用极大似然估计MLE的思想来求解参数。但是，为了满足最小化风险理论，我们可以将MLE的思想转化为最小化风险化理论，最大化似然函数其实就等价于最小化负的似然函数。对于MLE，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说是什么样的参数才能使我们观测到目前这组数据的概率最大。使用MLE推导LR的loss function的过程如下。<br>首先，根据上面的假设，写出相应的极大似然函数（假定有<script type="math/tex">m</script>个样本）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  \prod_{i=1}^{m} h_{\omega} (x_i)^{y_i} (1-h_{\omega} (x_i)^{1-y_i} \\
\end{aligned}   \qquad(9)</script><p>上述式子中的<script type="math/tex">\omega</script>及<script type="math/tex">x_i</script>均为向量，并未显示其转置。</p>
<p>直接对上面的式子求导会不方便，因此，为了便于计算，我们可以对似然函数取对数，经过化简可以得到下式的推导结果：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\log L(\omega)&= \sum_{i=1}^{m} \log \left [ (h_{\omega} (x_i)^{y_i} (1-h_{\omega} (x_i))^{1-y_i}) \right ] \\
&= \sum_{i=1}^{m} \left [ y_i \log h_{\omega} (x_i) +  (1-y_i) \log(1-h_{\omega} (x_i)) \right ]  \\
\end{aligned}  \qquad(10)</script><p>因此，损失函数可以通过最小化负的似然函数得到，即下式：</p>
<script type="math/tex; mode=display">
J(\omega) = - \frac{1}{m} \sum_{i=1}^m \left [ y_i \log h_{\omega}(x_i) + (1-y_i) \log(1-h_{\omega}(x_i)  \right ]   \qquad(11)</script><p>在周志华版的机器学习中，将sigmiod函数代入<script type="math/tex">h_{\omega}(x_i)</script>，并使用ln代替log，上述公式表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\omega) &= - \frac{1}{m} \sum_{i=1}^m \left [ y_i \ln h_{\omega}(x_i) + (1-y_i) \ln(1-h_{\omega}(x_i)  \right ]\\
&=- \frac{1}{m} \sum_{i=1}^m \left [ y_i\ln  \frac{1}{1+e^{-\omega x_i}}+(1-y_i)\ln \frac{e^{-\omega x_i}}{1+e^{-\omega x_i}}\right ]\\
&=- \frac{1}{m} \sum_{i=1}^m \left [ \ln \frac{1}{1+e^{\omega x_i}} + y_i \ln \frac{1}{e^{-\omega x_i}}\right ]\\
&= \frac{1}{m} \sum_{i=1}^m \left [ -y_iwx_i + \ln(1+e^{\omega x_i})\right ]
\end{aligned}  \qquad(12)</script><p>在某些资料上，还有另一种损失函数的表达形式，但本质是一样的，如下【推导见下面1.4】：</p>
<script type="math/tex; mode=display">
J(\omega) = \frac{1}{m} \sum_{i=1}^m log(1 + e^{-y_i \omega x}) \qquad(13)</script><h3 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h3><p>我们以梯度下降为例对逻辑回归进行求解，其迭代公式的推导过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{ \partial J(\omega)} {\partial \omega_j}&= -\frac{1}{m} \sum_{i}^{m} \left [ y_i(1 - h_{\omega}(x_i)) \cdot (-x_{i,j}) + (1 - y_i) h_{\omega} (x_i)  \cdot (x_{i,j}) \right ]\\
& = - \frac{1}{m} \sum_{i}^{m}  (-y_i \cdot x_{i,j} + h_{\omega}(x_i) \cdot x_{i,j})  \\
& = -\frac{1}{m} \sum_{i}^{m} (h_{\omega}(x_i) - y_i) x_{i,j}
\end{aligned}  \qquad(14)</script><p>上述中<script type="math/tex">x_{i,j}</script>表示第<script type="math/tex">i</script>个样本的第<script type="math/tex">j</script>个属性的取值。<br>于是，<script type="math/tex">\omega</script>的更新方式为：</p>
<script type="math/tex; mode=display">
\omega_{j+1} = \omega_j - \alpha \sum_{i=1}^{m} (h_{\omega}(x_i) - y_i) x_{x,j}</script><p>对于随机梯度下降，每次只取一个样本，则<script type="math/tex">\omega</script>的更新方式为：</p>
<script type="math/tex; mode=display">
\omega_{j+1} = \omega_j - \alpha (h_{\omega}(x)- y) x_{j}</script><p>其中<script type="math/tex">x</script>为这个样本的特征值，<script type="math/tex">y</script>为这个样本的真实值，<script type="math/tex">x_j</script>为这个样本第<script type="math/tex">j</script>个属性的值。</p>
<p>使用周志华版的损失函数更容易得出这个结论。</p>
<h3 id="1-4-另一种形式的损失函数及其梯度"><a href="#1-4-另一种形式的损失函数及其梯度" class="headerlink" title="1.4 另一种形式的损失函数及其梯度"></a>1.4 另一种形式的损失函数及其梯度</h3><p>与上面相同，根据sigmoid函数，我们可以得出预测函数的表达式为：</p>
<script type="math/tex; mode=display">
h_{\omega}(x) = g(\omega^Tx) = \frac{1}{1 + e^{-\omega^Tx}}</script><p>上式表示<script type="math/tex">y=1</script>的预测函数为<script type="math/tex">h_{\omega}(x)</script>。<br>但与上面不同，我们假设样本的分布为{-1,1}，则</p>
<script type="math/tex; mode=display">
p(y=1 | x) = h_{\omega} (x)</script><script type="math/tex; mode=display">
p(y=-1 | x) = 1 - h_{\omega} (x)</script><p>对于sigmoid函数，有以下特性（简单推导一下就可以得到）：</p>
<script type="math/tex; mode=display">
h(-x) = 1 - h(x)</script><p>于是(14)(15)式可以表示为：</p>
<script type="math/tex; mode=display">
p(y|x) = h_\omega(yx)</script><p>同样，我们使用MLE作估计，</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  \prod_{i=1}^{m} h_\omega(y_i x_i)\\
&= \prod_{i=1}^{m} \frac{1}{1+e^{-y_iwx_i}}
\end{aligned}</script><p>对上式取对数及负值，得到损失为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
-\log L(\omega)&= -\log \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log \frac{1}{1+e^{-y_iwx_i}}\\
&=  \sum_{i=1}^{m} \log(1+e^{-y_iwx_i})\\
\end{aligned}</script><p>即对于每一个样本，损失函数为：</p>
<script type="math/tex; mode=display">
L(\omega)=\log(1+e^{-y_iwx_i})</script><p>对上式求梯度，容易得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{ \partial J(\omega)} {\partial \omega_j}&= \frac{-y_i x_i}{1+e^{y_i \omega x_i}}
\end{aligned}</script><h2 id="2、FOBOS与RDA"><a href="#2、FOBOS与RDA" class="headerlink" title="2、FOBOS与RDA"></a>2、FOBOS与RDA</h2><h3 id="2-1-FOBOS基本原理"><a href="#2-1-FOBOS基本原理" class="headerlink" title="2.1 FOBOS基本原理"></a>2.1 FOBOS基本原理</h3><p>FOBOS算法由John Duchi和Yoram Singer提出，是梯度下降的一个变种。<br>与梯度下降不同，它将权重的更新分为2个步骤：</p>
<script type="math/tex; mode=display">
\begin{aligned}
W_{t+\frac{1}{2}}&=W_t-\eta_tG_t   \\n
W_{t+1}&=\arg\min\{\frac{1}{2}\|W-W_{t+\frac{1}{2}}\|^2+\eta_{(t+\frac{1}{2})}\psi(W)\}
\end{aligned}</script><p>从上面的2个步骤可以看出：<br>第一个步骤是一个标准的梯度下降。<br>第二个步骤是对梯度下降的结果进行微调。这里可以分为2部分：（1）前半部分保证了微调发生在第一个步骤结果（取梯度下降结果）的附近。（2）后半部分用于处理正则化，产生稀疏性。</p>
<p>根据次梯度理论的推断，可以得出<script type="math/tex">W_{(t+1)}</script>不仅仅与迭代前的状态<script type="math/tex">W_t</script>有关，而且与迭代后的相关</p>
<h3 id="2-2-L1-FOBOS"><a href="#2-2-L1-FOBOS" class="headerlink" title="2.2 L1-FOBOS"></a>2.2 L1-FOBOS</h3><p>FOBOS在L1正则化条件下，特征权重的更新方式为：（推导过程暂略）</p>
<script type="math/tex; mode=display">
\omega_{t+1,i}=sgn(\omega_{t,i}-\eta_tg_{t,i})max\{0,|\omega_{t,i}-\eta_tg_{t,i}|-\eta_{t+\frac{1}{2}}\lambda\}</script><p>其中<script type="math/tex">g_{t,i}</script>为梯度在维度i上的取值</p>
<h3 id="2-3-RDA基本原理"><a href="#2-3-RDA基本原理" class="headerlink" title="2.3 RDA基本原理"></a>2.3 RDA基本原理</h3><p>简单截断、TG、FOBOS都是建立在SGD基础上的一个变种，属于梯度下降类型的方法，这类方法的精度比较高，同时也能得到一定的稀疏性。而RDA是从另一个方面来求解，它的精度比FOBOS等略差，但它更有效的提升了稀疏性。</p>
<p>在RDA中，特征权重的更新策略为：</p>
<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{\frac{1}{t}\sum_{r=1}^{t}<G^{r},W>+\psi(W)+\frac{\beta_t}{t}h(W)\}</script><p>其中<script type="math/tex"><G^{r},W></script>表示梯度<script type="math/tex">G^r</script>对W的积分平均值（积分中值）；<script type="math/tex">\psi(W)</script>为正则项；<script type="math/tex">h(W)</script>为一个辅助的严格的凸函数；<script type="math/tex">{\beta_t|t\ge1}</script>是一个非负且非自减序列。</p>
<h3 id="2-4-L1-RDA"><a href="#2-4-L1-RDA" class="headerlink" title="2.4 L1-RDA"></a>2.4 L1-RDA</h3><p>在L1正则化条件下，RDA的各个维度的权重更新方式为：</p>
<p><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.17.20.png" alt></p>
<p>亦即当某个维度上的累积梯度平均值的绝对值 <script type="math/tex">|\overline g_{t,i}|</script>小于阈值<script type="math/tex">\lambda</script>时，该维度权重被置为0。</p>
<h2 id="3、FTRL"><a href="#3、FTRL" class="headerlink" title="3、FTRL"></a>3、FTRL</h2><p>理论及实验均证明，L1-FOBOS这类基于梯度下降的算法有比较高的精度，但L1-RDA却能在损失一定精度的情况下产生更好的稀疏性。<br>把这二者的优点结合成一个算法，这就是FTRL算法的来源。</p>
<h3 id="3-1-从L1-FOBOS和L1-RDA推导FTRL"><a href="#3-1-从L1-FOBOS和L1-RDA推导FTRL" class="headerlink" title="3.1 从L1-FOBOS和L1-RDA推导FTRL"></a>3.1 从L1-FOBOS和L1-RDA推导FTRL</h3><p>我们令<script type="math/tex">\eta_{t+\frac{1}{2}}=\eta_t=\Theta(\frac{1}{\sqrt{t}})</script>是一个非增正序列，同时代入L1正则项，得到L1-FOBOS的形式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
W_{t+\frac{1}{2}}&=W_t-\eta_tG_t \\
W_{t+1}&=\arg\min\{\frac{1}{2}\|W-W_{t+\frac{1}{2}}\|^2+\eta_{t}\lambda\|W\|_1\}
\end{aligned}</script><p>将这2个公式合并到一起，得到L1-FOBOS的形式如下：</p>
<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{\frac{1}{2}\|W-W_t+\eta_tG_t\|^2+\eta_{t}\lambda\|W\|_1\}</script><p>将上式分解为N个独立的维度进行最优化求解：</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_i&=\arg\min\{\frac{1}{2}\|w_i-w_{t,i}+\eta_tg_{t,i}\|^2+\eta_{t}\lambda|w_{t,i}|_1\} \\
&=\arg\min\{\frac{1}{2}(w_i-w_{t,i})^2+\frac{1}{2}(\eta_t g_{t,i})^2+w_i\eta_tg_{t,i}-w_{t,i}\eta_tg_{t,i}+\eta_t\lambda|w_i|\}\\
&=\arg\min\{w_ig_{t,i}+\lambda|w_i|+\frac{1}{2\eta_t}(w_i-w_{t,i})^2+|\frac{\eta_t}{2}g_{t,i}^2-w_{t,i}g_{t,i}|\}
\end{aligned}</script><ul>
<li>上述推导的最后一步是通过除以<script type="math/tex">\eta_t</script>得到的。<br>由于上式中的最后一项<script type="math/tex">|\frac{\eta_t}{2}g_{t,i}^2-w_{t,i}g_{t,i}|</script>是一个与<script type="math/tex">w_i</script>无关的量，因此上式可简化为：<script type="math/tex; mode=display">
w_i=\arg\min\{w_ig_{t,i}+\lambda|w_i|+\frac{1}{2\eta_t}(w_i-w_{t,i})^2\}</script>把N个独立优化的维度重新合并，L1-FOBOS可写成以下形式：<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{G_tW+\lambda\|W\|_1+\frac{1}{2\eta_t}\|W-W_t\|_2^2\}</script></li>
</ul>
<p>另一方面，L1-RDA可以表达为：</p>
<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{G_{(1:t)}W+t\lambda\|W\|_1+\frac{1}{2\eta_t}\|W-0\|_2^2\}</script><p>其中<script type="math/tex">G_{(1:t)}=\sum_{s=1}{t}G_s</script>。<br>我们令<script type="math/tex">\sigma_s=\frac{1}{\eta_s}-\frac{1}{\eta_{s-1}}</script>，可以得到<script type="math/tex">\sigma_{(1:t)}=\frac{1}{\eta_t}</script>。那么L1-FOBOS与L1-RDA可以写为以下格式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
W_{t+1}&=\arg\min\{G_tW+\lambda\|W\|_1+\frac{1}{2}\sigma_{(1:t)}\|W-W_t\|_2^2\}\\
W_{t+1}&=\arg\min\{G_{(1:t)}W+t\lambda\|W\|_1+\frac{1}{2}\sigma_{(1:t)}\|W-0\|_2^2\}
\end{aligned}</script><p>比较以上2式的区别：</p>
<ul>
<li>（1）L1-FOBOS考虑的是当前梯度的影响，L1-RDA则考虑了累积影响。</li>
<li>（2）L1-FOBOS限制<script type="math/tex">W_{t+1}</script>不能离<script type="math/tex">W_t</script>太远，而L1-RDA的W则不能离0太远，因此后者更容易产生稀疏性。</li>
</ul>
<h3 id="3-2-FTRL权重更新的最终形式"><a href="#3-2-FTRL权重更新的最终形式" class="headerlink" title="3.2 FTRL权重更新的最终形式"></a>3.2 FTRL权重更新的最终形式</h3><p>在google2010公布的理论文章中并没有使用L2正则项，但在2013年公布工程实施方案时引入了L2。<br>因此，综合上述的L1-FOBOS及L1-RDA，FTRL算法的权重更新方式为：</p>
<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{G_{(1:t)}W+\lambda_1\|W\|_1+\lambda_2\|W\|_2^2+\frac{1}{2}\sum_{s=1}^t(\sigma_s\|W-W_s\|_2^2)\}</script><p>将上式展开，得到</p>
<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{(G_{(1:t)}-\sum_{s=1}^t\sigma_sW_s)W+\lambda_1\|W\|_1+\frac{1}{2}(\lambda_2+\sum_{s=1}^t\sigma_s)\|W\|^2+\frac{1}{2}\sum_{s=1}^{t}\sigma_s\|W_s\|_2^2\}</script><p>由于上式的最后一项相对于W来说是一个常数，同时令<script type="math/tex">Z_t=G_{(1:t)}-\sum_{(s=1)}^t\sigma_sW_s</script>，上式可表示为：</p>
<script type="math/tex; mode=display">
W_{t+1}=\arg\min\{Z_tW+\lambda_1\|W\|_1+\frac{1}{2}(\lambda_2+\sum_{s=1}^t\sigma_s)\|W\|^2\}</script><p>各个维度可以独立表示为：</p>
<script type="math/tex; mode=display">
w_{i+1}=\arg\min\{z_{t,i}w_i+\lambda_1|w_i|+\frac{1}{2}(\lambda_2+\sum_{s=1}^{t}\sigma_s)w_i^2\}</script><p>使用与L1-FOBOS相同的分析方法可以得到：<br><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.18.58.png" alt></p>
<p>根据上面的定义:</p>
<script type="math/tex; mode=display">
\sigma_{(1:t)}=\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}</script><p>我们使用下面介绍的学习率，以及令<script type="math/tex">n_i=\sum g_i^2</script>，则可以得到FTRL的最终形式：<br><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.19.50.png" alt></p>
<h3 id="3-3-学习率"><a href="#3-3-学习率" class="headerlink" title="3.3 学习率"></a>3.3 学习率</h3><p>1、per-coordinate learning rate<br>在FTRL中，学习率的定义如下：</p>
<script type="math/tex; mode=display">
\eta_{t,i}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^tg_{s,i}^2}}</script><p>其中<script type="math/tex">\alpha\beta</script>是自定义的参数。</p>
<p>在一般梯度下降算法中，使用的是一个全局的学习率策略：<script type="math/tex">\eta_t=\frac{1}{\sqrt{t}}</script>。这个策略保证了学习率是一个正的非增序列，这个值对于每一个特征都是一样的。</p>
<p>考虑一个极端的情况，我们有多个稀疏特征，其中一个特征<script type="math/tex">x_1</script>出现非常频繁，而另一个特征<script type="math/tex">x_2</script>很少出现。比如第一个样本，<script type="math/tex">x_1</script>和<script type="math/tex">x_2</script>同时出现了，但接下来的99个样本都只出现了<script type="math/tex">x_1</script>，然后第100个样本<script type="math/tex">x_2</script>又出来了，由于此时的学习率为<script type="math/tex">\frac{1}{\sqrt{100}}</script>，远小于第一个样的影响了。也就是说假如第一个样本是正样本，第10个样本是负样本，此时由于学习率的不同，模型会认为<script type="math/tex">x_2</script>会是一个有用的正相关的特征，即<script type="math/tex">\omega>0</script>。但事实上，这个特征只出现了2次，而且一正一负，因此这应该是一个无用的特征，即<script type="math/tex">\omega=0</script>。</p>
<p>在FTRL中，我们会使用一个特征的累积梯度，由于中间的99个数据没有这个特征，因此其对应的梯度为0，因此第二次的负样本对应的学习率只是略小于第一个正样本。</p>
<h3 id="3-4-工程实现计算过程"><a href="#3-4-工程实现计算过程" class="headerlink" title="3.4 工程实现计算过程"></a>3.4 工程实现计算过程</h3><h4 id="3-4-1-一些定义"><a href="#3-4-1-一些定义" class="headerlink" title="3.4.1 一些定义"></a>3.4.1 一些定义</h4><p>对于每一个样本，我们计算以下数值。</p>
<p>（1）<script type="math/tex">p_t</script></p>
<p>使用当前的<script type="math/tex">\omega</script>代入sigmoid函数得出的预测值，即：</p>
<script type="math/tex; mode=display">
p=\frac{1}{1+e^{-(\sum_{i=1}^n\omega_ix_i)}}</script><p>（2）<script type="math/tex">g_i</script></p>
<p>损失函数在某一个维度上的梯度，对于逻辑回归而言：</p>
<script type="math/tex; mode=display">
g_i=(p-y)x_i</script><p>其中y为当前样本的实际值，即0或者1。</p>
<p>（3）<script type="math/tex">n_i</script></p>
<p>这是该维度梯度<script type="math/tex">g_i^2</script>的累积值，即：</p>
<script type="math/tex; mode=display">
n_i=n_i+g_i^2</script><p>（4）<script type="math/tex">\eta_i</script></p>
<p>这是该维度的学习率，它与累积梯度有关。定义为：</p>
<script type="math/tex; mode=display">
\eta_i=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^t(g_i^s)^2}}=\frac{\alpha}{\beta+\sqrt{n_i}}</script><p>其中<script type="math/tex">\alpha\beta</script>为用户定义的2个参数。</p>
<p>（5）<script type="math/tex">\sigma_i</script></p>
<script type="math/tex; mode=display">\sigma_i$$是一个中间计算值，没有实际含义，其定义为：</script><p>\sigma_i=\frac{1}{\eta_{t,i}}-\frac{1}{\eta_{t-1,i}}=\frac{1}{\alpha}(\sqrt{n_i+g_i^2}-\sqrt{n_i})</p>
<script type="math/tex; mode=display">

（6）$$z_i</script><script type="math/tex; mode=display">z_i$$也是一个辅助计算的中间值，它的定义为：</script><p>z_{t,i}=g^{(1:t)}-\sum_{s=1}^t{\sigma_{s,i}\omega_{s,i}}</p>
<script type="math/tex; mode=display">
于是$$z_i$$的更新为：</script><p>z_t-z_{t-1}=g_t-\sigma_{t}\omega_{t}</p>
<script type="math/tex; mode=display">
即：</script><p>z_i=z_i+g_t-\sigma_i\omega_i</p>
<script type="math/tex; mode=display">


#### 3.4.2 FTRL算法

（1）设定以下4个输入参数，这些参数根据经验而定，可以参考以下数据：</script><p>\alpha=0.1,\beta=1,\lambda_1=1,\lambda_2=1</p>
<script type="math/tex; mode=display">

（2）初始化以下数值：</script><p>z_i=0,n_i=0</p>
<script type="math/tex; mode=display">

（3）对于**每一个样本**的所带的**每一个维度**,更新$$\omega</script><p><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.20.36.png" alt></p>
<p>（4）使用上面更新后的<script type="math/tex">\omega</script>，预测<strong>这个样本</strong>的值，即代入sigmoid函数计算<script type="math/tex">p_t</script></p>
<script type="math/tex; mode=display">
p=\frac{1}{1+e^{-(\sum_{i=1}^n\omega_ix_i)}}</script><p>（5）对于<strong>每一个样本</strong>的<strong>每一个维度</strong>，更新<script type="math/tex">g_i,\sigma_i,z_i,n_i</script>，即上面所说的：</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_i&=(p-y)x_i \\
\sigma_i&=\frac{1}{\alpha}(\sqrt{n_i+g_i^2}-\sqrt{n_i})\\
z_i&=z_i+g_t-\sigma_i\omega_i\\
n_i&=n_i+g_i^2
\end{aligned}</script><h4 id="3-4-3-特征的独立性"><a href="#3-4-3-特征的独立性" class="headerlink" title="3.4.3 特征的独立性"></a>3.4.3 特征的独立性</h4><p>如果特征是非独立的，会导致特征的权重不符合预期。<br>比如，有2个特征A和B，然后加了一个组合标签A&amp;B，假如A和B都是和label非常正相关的，A&amp;B效果更好。但结果会发现A&amp;B这个标签的权重值可能还不如A或者B的权重高。<br>这是因为，带了A&amp;B标签的，必然带A或者B，大概率被预测成高概率值，而LR梯度为：</p>
<script type="math/tex; mode=display">
(p-y)x_i</script><p>对于正样本，<script type="math/tex">y=1</script>，则梯度并不大，所以A&amp;B提升并不大；而对于负样本，<script type="math/tex">y=0</script>，则梯度非常大，导致惩罚很大，权重降低很多。</p>
<p>所以最终结果A和B的权重都可能比A&amp;B要大，但对于带A&amp;B标签的用户来说，他由于同时带3个特征，所以预测概率值还是比较高的。</p>
<p>当然最好的办法是尽量避免这种关联性太强的特征。</p>
<h2 id="4、FTRL的工程应用"><a href="#4、FTRL的工程应用" class="headerlink" title="4、FTRL的工程应用"></a>4、FTRL的工程应用</h2><p>这里只列出了google提供的一些建议，我们自身的工程应用并不包含在内。</p>
<p>（1）减少样本的数量</p>
<ul>
<li>Poisson Inclusion：以p的概率接受样本</li>
<li>Bloom Filter Inclusion：只有当特征出现次数达到一定数量后，特征才会被加入模型</li>
</ul>
<p>（2）使用更少的位来进行浮点数编码<br>一般而言，特征对应的<script type="math/tex">\omega</script>在(-2,2)之间，所以我们可以将32，或者64位的浮点数编码规则改为q2.13编码方式，即2位整数，1位小数点，13位小数。</p>
<p>（3）多个类似的模型<br>把多个同类型/相似的模型放在一起保存，可以不需要记录多个key，只需要记录一次Key，以及各个模型中的<script type="math/tex">\omega</script>。这可以节省内存、带宽、CPU、存储。</p>
<p>（4）只保存一个模型<br> 在上面的基础上更进一步，所有模型共用一份<script type="math/tex">\omega</script>，以及以一个bit来记录本模型有哪些特征。当模型更新某个<script type="math/tex">\omega</script>后，取它与之前那个值的平均值。<br>与共用一个模型对比，好处是记录了这个模型有哪些有效特征，共用模型就无法区分了。</p>
<p>（5）近似计算学习率<br>只使用正样本P与负样本数据N来近似估计梯度的平方和，前提是假设特征有类似的分布。</p>
<script type="math/tex; mode=display">
\sum{g_i^2}=\frac{PN}{P+N}</script><p>（6）减少负样本数量<br>减少负样本数量，然后在训练时弥补其损失。</p>
<h1 id="三、FM-Factorization-Machine"><a href="#三、FM-Factorization-Machine" class="headerlink" title="三、FM(Factorization Machine)"></a>三、FM(Factorization Machine)</h1><h2 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h2><h3 id="1-1-1-线性模型"><a href="#1-1-1-线性模型" class="headerlink" title="1.1.1 线性模型"></a>1.1.1 线性模型</h3><p>常见的线性模型，比如线性回归、逻辑回归等，它只考虑了每个特征对结果的单独影响，而没有考虑特征间的组合对结果的影响。</p>
<p>对于一个有n维特征的模型，线性回归的形式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f(x) &= \omega_0 + \omega_1x_1+\omega_2x_2+...+\omega_nx_n \\
 &=\omega_0+\sum_{i=1}^n{\omega_ix_i}
 \end{aligned}       \qquad (1)</script><p>其中<script type="math/tex">(\omega_0,\omega_1...\omega_n)</script>为模型参数，<script type="math/tex">(x_1,x_2...x_n)</script>为特征。<br>从(1)式可以看出来，模型的最终计算结果是各个特征的独立计算结果，并没有考虑特征之间的相互关系。</p>
<p>举个例子，我们认为“USA”与”Thanksgiving”，”China”与“Chinese new year”这样的组合特征是很有意义的，在这样的组合特征下，会对某些商品表现出更强的购买意愿，而单独考虑国家及节日都是没有意义的。</p>
<h2 id="1-1-2-二项式模型"><a href="#1-1-2-二项式模型" class="headerlink" title="1.1.2 二项式模型"></a>1.1.2 二项式模型</h2><p>我们在（1）式的基础上，考虑任意2个特征分量之间的关系，得出以下模型：</p>
<script type="math/tex; mode=display">
f(x)=\omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\omega_{ij}x_ix_j               \qquad (2)</script><p>这个模型考虑了任意2个特征分量之间的关系，但并未考虑更高阶的关系。<br>模型涉及的参数数量为：</p>
<script type="math/tex; mode=display">
1+n+\frac{n(n-1)}{2}=\frac{1}{2}(n^2+n+2)  \qquad (3)</script><p>对于参数<script type="math/tex">\omega_i</script>的训练，只要这个样本中对应的<script type="math/tex">x_i</script>不为0，则可以完成一次训练。<br>但对于参数<script type="math/tex">\omega_{ij}</script>的训练，需要这个样本中的<script type="math/tex">x_i</script>和<script type="math/tex">x_j</script>同时不为0，才可以完成一次训练。<br>在数据稀疏的实际应用场景中，二次项<script type="math/tex">\omega_{ij}</script>的训练是非常困难的。因为每个<script type="math/tex">\omega_{ij}</script>都需要大量<script type="math/tex">x_i</script>和<script type="math/tex">x_j</script>都不为0的样本。但在数据稀疏性比较明显的样本中，<script type="math/tex">x_i</script>和<script type="math/tex">x_j</script>都不为0的样本会非常稀少，这会导致<script type="math/tex">\omega_{ij}</script>不能得到足够的训练，从而不准确。</p>
<h2 id="1-2-FM"><a href="#1-2-FM" class="headerlink" title="1.2 FM"></a>1.2 FM</h2><h3 id="1-2-1-FM基本原理"><a href="#1-2-1-FM基本原理" class="headerlink" title="1.2.1 FM基本原理"></a>1.2.1 FM基本原理</h3><p>为了解决二项式模型中由于数据稀疏引起的训练不足的问题，我们为每个特征维度<script type="math/tex">x_i</script>引入一个辅助向量：</p>
<script type="math/tex; mode=display">
V_i = (v_{i1},v_{i2},v_{i3},...,v_{ik})^T\in \mathbb R^k, i=1,2,3,...,n  \qquad(4)</script><p>其中<script type="math/tex">k</script>为辅助变量的维度，依经验而定，一般而言，对于特征维度足够多的样本，<script type="math/tex">k<<n</script>。<br>将<script type="math/tex">\omega_{ij}</script>表示为：</p>
<script type="math/tex; mode=display">
\omega_{ij}=V_i^TV_j=\sum_{l=1}^kv_{il}v_{jl} \qquad(5)</script><p>简单的说，我们不再简单的使用样本训练具体的<script type="math/tex">\omega_{ij}</script>，而是先训练2个隐变量<script type="math/tex">V_i</script>以及<script type="math/tex">V_j</script>，然后使用式（5）求出最终的<script type="math/tex">\omega_{ij}</script>。</p>
<p>具体而言，<script type="math/tex">\omega_{ij}=V_i^TV_j</script>与<script type="math/tex">\omega_{hi}=V_h^TV_i</script>有相同的项<script type="math/tex">V_i</script>，也就是只要样本中的<script type="math/tex">x_i</script>不为0，且最少具有一个其它特征，则这个样本则可用于训练<script type="math/tex">V_i</script>，这就解决了数据稀疏性导致的问题。</p>
<p>于是，在FM中，模型可以表达为：</p>
<script type="math/tex; mode=display">
f(x) = \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j \qquad(6)</script><h3 id="1-2-2-数据分析"><a href="#1-2-2-数据分析" class="headerlink" title="1.2.2 数据分析"></a>1.2.2 数据分析</h3><p>我们的目标是要求得以下交互矩阵W：</p>
<script type="math/tex; mode=display">
W=
        \begin{pmatrix}
        \omega_{11} & \omega_{12}& ... &\omega_{1n} \\
        \omega_{21} & \omega_{22}& ... &\omega_{2n} \\
        \vdots &\vdots &\ddots &\vdots\\
       \omega_{n1} & \omega_{n2}& ... &\omega_{nn} \\
        \end{pmatrix}_{n\times n} \qquad(7)</script><p>由于直接求解W不方便，因此我们引入隐变量V：</p>
<script type="math/tex; mode=display">
V=
        \begin{pmatrix}
        v_{11} & v_{12}& ... &v_{1k} \\
        v_{21} & v_{22}& ... &v_{2k} \\
        \vdots &\vdots &\ddots &\vdots\\
       v_{n1} & v_{n2}& ... &v_{nk} \\
        \end{pmatrix}_{n\times k}=\begin{pmatrix}
V_1^T\\
V_2^T\\
\cdots \\
V_n^T\\
\end{pmatrix} \qquad(8)</script><p>令</p>
<script type="math/tex; mode=display">
VV^T = W \qquad(9)</script><p>如果我们先得到V，则可以得到W了。<br>现在只剩下一个问题了，是否一个存在V，使得上述式（9）成立。<br>理论研究表明：当k足够大时，对于任意对称正定的实矩阵<script type="math/tex">W\in \mathbb R^{n \times  n}</script>，均存在实矩阵<script type="math/tex">V\in \mathbb R^{n \times  k}</script>，使得<script type="math/tex">W=VV^T</script>。</p>
<p>理论分析中要求参数k足够的大，但在高度稀疏数据的场景中，由于 没有足够的样本，因此k通常取较小的值。事实上，对参数<script type="math/tex">k</script>的限制，在一定程度上可以提高模型的泛化能力。</p>
<h3 id="1-2-3参数个数"><a href="#1-2-3参数个数" class="headerlink" title="1.2.3参数个数"></a>1.2.3参数个数</h3><p>假设样本中有n个特征，每个特征对应的隐变量维度为k，则参数个数为<script type="math/tex">1+n+nk</script>。<br>正如上面所言，对于特征维度足够多的样本，<script type="math/tex">k<<n</script>。</p>
<h3 id="1-2-4-计算时间复杂度"><a href="#1-2-4-计算时间复杂度" class="headerlink" title="1.2.4 计算时间复杂度"></a>1.2.4 计算时间复杂度</h3><p>下面我们分析一下已经知道所有参数，代入式（6）计算预测值时的时间复杂度。从式（6）中一看，</p>
<script type="math/tex; mode=display">
f(x) = \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j \qquad(6)</script><p>可以看出时间复杂度是<script type="math/tex">O(kn^2)</script>。但我们对上述式子的最后一项作变换后，可以得出一个<script type="math/tex">O(kn)</script>的时间复杂度表达式。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j
&= \frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n(V_i^TV_j)x_ix_j-\sum_{i=1}^n(V_i^TV_i)x_ix_i\right) \\
&=\frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n\sum_{l=1}^kv_{il}v_{jl}x_ix_j-\sum_{i=1}^n\sum_{l=1}^k v_{il}^2x_i^2\right)\\
&=\frac{1}{2}\sum_{l=1}^k\left(\sum_{i=1}^n(v_{il}x_i)\sum_{j=1}^n(v_{jl}x_j)-\sum_{i=1}^nv_{il}^2x_i^2\right)\\
&=\frac{1}{2}\sum_{l=1}^k\left(\left(\sum_{i=1}^n(v_{il}x_i)\right)^2-\sum_{i=1}^nv_{il}^2x_i^2\right)
\end{aligned} \qquad(10)</script><p>上述式子中的<script type="math/tex">\sum_{i=1}^n(v_{il}x_i)</script>只需要计算一次就好，因此，可以看出上述模型的复杂度为<script type="math/tex">O(kn)</script>。<br>也就是说我们不要直接使用式（6）来计算预测结果，而应该使用式（10），这样的计算效率更高。</p>
<h3 id="1-2-5-梯度"><a href="#1-2-5-梯度" class="headerlink" title="1.2.5 梯度"></a>1.2.5 梯度</h3><p>FM有一个重要的性质：multilinearity：若记<script type="math/tex">\Theta=(\omega_0,\omega_1,\omega_2,...,\omega_n,v_{11},v_{12},...,v_{nk})</script>表示FM模型的所有参数，则对于任意的<script type="math/tex">\theta \in \Theta</script>，存在与<script type="math/tex">\theta</script>无关的<script type="math/tex">g(x)</script>与<script type="math/tex">h(x)</script>，使得式（6）可以表示为：</p>
<script type="math/tex; mode=display">
f(x) = g(x) + \theta h(x) \qquad(11)</script><p>从式（11）中可以看出，如果我们得到了<script type="math/tex">g(x)</script>与<script type="math/tex">h(x)</script>，则对于参数<script type="math/tex">\theta</script>的梯度为<script type="math/tex">h(x)</script>。下面我们分情况讨论。</p>
<ul>
<li><p>当 <script type="math/tex">\theta=\omega_0</script>时，式（6）可以表示为：</p>
<script type="math/tex; mode=display">
f(x) = \color{blue}{\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j} +\omega_0 \times \color{red}{1} \qquad(12)</script><p>上述中的蓝色表示<script type="math/tex">g(x)</script>，红色表示<script type="math/tex">h(x)</script>。下同。<br>从上述式子可以看出此时的梯度为1.</p>
</li>
<li><p>当<script type="math/tex">\theta=\omega_l, l \in (1,2,...,n)</script>时，</p>
<script type="math/tex; mode=display">
f(x) = \color{blue}{\omega_0+\sum_{i=1,\ i \ne l}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j}+\omega_l \times \color{red}{x_l} \qquad(13)</script><p>此时梯度为<script type="math/tex">x_l</script>。</p>
</li>
<li><p>当<script type="math/tex">\theta=v_{lm}</script>时</p>
<script type="math/tex; mode=display">
f(x) =\color{blue}{ \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\sum_{s=1 , is \ne lm ,js \ne lm}^k v_{is}v_{js})x_ix_j }+v_{lm}\times \color{red}{x_l\sum_{i=1,i \ne l }^n v_{im}x_i} \qquad(14)</script><p>此时梯度为<script type="math/tex">x_l\sum_{i \ne l } v_{im}x_i</script>。</p>
</li>
</ul>
<p>综合上述结论，<script type="math/tex">f(x)</script>关于<script type="math/tex">\theta</script>的偏导数为：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x)}{\partial \theta}  =
\begin{cases}
1,  & \theta=\omega_0 \\
x_l, & \theta=\omega_l, l \in (1,2,...,n) \\
x_l\sum_{i=1,i \ne l }^n v_{im}x_i & \theta=v_{lm}
\end{cases}   \qquad(15)</script><h3 id="1-2-6-训练时间复杂度"><a href="#1-2-6-训练时间复杂度" class="headerlink" title="1.2.6 训练时间复杂度"></a>1.2.6 训练时间复杂度</h3><p>由上述式（15）可以得到：</p>
<script type="math/tex; mode=display">
x_l\sum_{i=1,i \ne l }^n v_{im}x_i = x_l\sum_{i=1}^n v_{im}x_i-v_{lm}x_l^2 \qquad(16)</script><p>对于上式中的前半部分<script type="math/tex">\sum_{i=1}^n v_{im}x_i</script>，对于每个样本只需要计算一次，所以时间复杂度为<script type="math/tex">O(n)</script>，对于k个隐变量的维度分别计算一次，则复杂度为<script type="math/tex">O(kn)</script>。其它项的时间复杂度都小于这一项，因此，模型训练的时间复杂度为<script type="math/tex">O(kn)</script>。</p>
<p>详细一点解释：<br>（1）我们首先计算<script type="math/tex">\sum_{i=1}^n v_{im}x_i</script>，时间复杂度为n，这个值对于所有特征对应的隐变量的某一个维度是相同的。我们设这值为C。</p>
<p>（2）计算每一个特征对应的<script type="math/tex">x_l\sum_{i=1}^n v_{im}x_i-v_{lm}x_l^2 =Cx_l-v_{lm}x_l^2</script>，由于总共有n个特征，因此时间复杂度为n，至此，总的时间复杂度为n+n。</p>
<p>（3）上述只是计算了隐变量的其中一个维度，我们总共有k个维度，因此总的时间复杂度为<script type="math/tex">k(n+n)=O(kn)</script>.</p>
<h1 id="四、FFM-Field-Factorization-Machine"><a href="#四、FFM-Field-Factorization-Machine" class="headerlink" title="四、FFM(Field Factorization Machine)"></a>四、FFM(Field Factorization Machine)</h1><h2 id="2-1-背景及基本原理"><a href="#2-1-背景及基本原理" class="headerlink" title="2.1 背景及基本原理"></a>2.1 背景及基本原理</h2><p>在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。</p>
<p>举个例子，我们的样本有3种类型的字段：publisher, advertiser, gender，分别可以代表媒体，广告主或者是具体的商品，性别。其中publisher有5种数据，advertiser有10种数据，gender有男女2种，经过one-hot编码以后，每个样本有17个特征，其中只有3个特征非空。</p>
<p>如果使用FM模型，则17个特征，每个特征对应一个隐变量。<br>如果使用FFM模型，则17个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，具体而言，就是对应publisher, advertiser, gender三个field各有一个隐变量。</p>
<h2 id="2-2模型与最优化问题"><a href="#2-2模型与最优化问题" class="headerlink" title="2.2模型与最优化问题"></a>2.2模型与最优化问题</h2><h3 id="2-2-1-模型"><a href="#2-2-1-模型" class="headerlink" title="2.2.1 模型"></a>2.2.1 模型</h3><p>根据上面的描述，可以得出FFM的模型为：</p>
<script type="math/tex; mode=display">
f(x) = \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{j1=1}^{n-1}\sum_{j2=j1+1}^n(V_{j1,f2}^TV_{j2,f1})x_{j1}x_{j2} \qquad(17)</script><p>其中<script type="math/tex">j1,j2</script>表示特征的索引。我们假设<script type="math/tex">j1</script>特征属于<script type="math/tex">f1</script>这个field，<script type="math/tex">j2</script>特征属于<script type="math/tex">f2</script>这个field，则<script type="math/tex">V_{j1,f2}</script>表示<script type="math/tex">j1</script>这个特征对应<script type="math/tex">f2</script>(<script type="math/tex">j2</script>所属的field)的隐变量，同时<script type="math/tex">V_{j2,f1}</script>表示<script type="math/tex">j2</script>这个特征对应<script type="math/tex">f1</script>(<script type="math/tex">j1</script>所属的field)的隐变量。</p>
<p>事实上，在大多数情况下，FFM模型只保留了二次项，即：</p>
<script type="math/tex; mode=display">
\phi(V,x) =  \sum_{j1=1}^{n-1}\sum_{j2=j1+1}^n(V_{j1,f2}^TV_{j2,f1})x_{j1}x_{j2} \qquad(18)</script><h3 id="2-2-2-最优化问题"><a href="#2-2-2-最优化问题" class="headerlink" title="2.2.2 最优化问题"></a>2.2.2 最优化问题</h3><p>根据逻辑回归的损失函数及分析，可以得出FFM的最优化问题为：</p>
<script type="math/tex; mode=display">
\min \frac{\lambda}{2}||V||_2^2+\sum_{i=1}^{m}\log(1+exp(-y_i\phi(V,x))) \qquad(19)</script><p>上面加号的前面部分使用了L2范式，后面部分是逻辑回归的损失函数。m表示样本的数量，<script type="math/tex">y_i</script>表示训练样本的真实值（如是否点击的-1/1），<script type="math/tex">\phi(V,x)</script>表示使用当前的V代入式（18）计算得到的值。</p>
<p><strong>注意，以上的损失函数适用于样本分布为{-1,1}的情况。</strong></p>
<h3 id="2-2-3-自适应学习率"><a href="#2-2-3-自适应学习率" class="headerlink" title="2.2.3 自适应学习率"></a>2.2.3 自适应学习率</h3><p>与FTRL一样，FFM也使用了累积梯度作为学习率的一部分，即：</p>
<script type="math/tex; mode=display">
V_{j1,f2} = V_{j1,f2} - \frac{\eta}{\sqrt{1+\sum_t(g_{v_{j1,f2}}^t)^2}}g_{v_{j1,f2}} \qquad(20)</script><p>其中<script type="math/tex">g_{v_{j1,f2}}</script>表示对于<script type="math/tex">V_{j1,f2}</script>这个变量的梯度向量，因为<script type="math/tex">V_{j1,f2}</script>是一个向量，因此<script type="math/tex">g_{v_{j1,f2}}</script>也是一个向量，尺寸为隐变量的维度大小，即k。<br>而<script type="math/tex">\sum_t(g_{v_{j1,f2}}^t)^2</script>表示从第一个样本到当前样本一直以来的累积梯度平方和。</p>
<h3 id="2-2-4-FFM算法的最终形式"><a href="#2-2-4-FFM算法的最终形式" class="headerlink" title="2.2.4 FFM算法的最终形式"></a>2.2.4 FFM算法的最终形式</h3><script type="math/tex; mode=display">
\begin{aligned}
(V_{j1,f2})_d &=(V_{j1,f2})_{d-1}-\frac{\eta}{\sqrt{(G_{j1,f2})_d}} \cdot (g_{j1,f2})_d \\
(V_{j2,f1})_d &=(V_{j2,f1})_{d-1}-\frac{\eta}{\sqrt{(G_{j2,f1})_d}} \cdot (g_{j2,f1})_d
\end{aligned}</script><p>其中G为累积梯度平方：</p>
<script type="math/tex; mode=display">
(G_{j1,f2})_d=(G_{j1,f2})_{d-1}+(g_{j1,f2})_d^2

(G_{j2,f1})_d=(G_{j2,f1})_{d-1}+(g_{j2,f1})_d^2</script><p>g为梯度，比如<script type="math/tex">g_{j1,f2}</script>为<script type="math/tex">j1</script>这个特征对应<script type="math/tex">f2</script>这个field的梯度向量：</p>
<script type="math/tex; mode=display">
g_{ji,f2}=\lambda \cdot V_{ji,f2} + \kappa \cdot V_{j2,f1}

g_{j2,f1}=\lambda \cdot V_{j2,f1} + \kappa \cdot V_{j1,f2}</script><p>其中<script type="math/tex">\kappa</script>为：</p>
<script type="math/tex; mode=display">
\kappa=\frac{\partial\log(1+exp(-y_i\phi(V,x))) }{\partial \phi(V,x) }=\frac{-y}{1+\exp(y\phi(V,x) )}</script><h2 id="2-3完整算法流程"><a href="#2-3完整算法流程" class="headerlink" title="2.3完整算法流程"></a>2.3完整算法流程</h2><p>使用随机梯度下降（SGD）训练FFM模型的完整过程如下：</p>
<h3 id="2-3-1-计算梯度"><a href="#2-3-1-计算梯度" class="headerlink" title="2.3.1 计算梯度"></a>2.3.1 计算梯度</h3><p>对于每一个样本的每一对特征组合都要计算以下梯度向量。</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_{ji,f2}=\lambda \cdot V_{ji,f2} + \kappa \cdot V_{j2,f1} \\
g_{j2,f1}=\lambda \cdot V_{j2,f1} + \kappa \cdot V_{j1,f2}
\end{aligned}
\qquad(21)</script><p>其中<script type="math/tex">\kappa</script>为式(19)后半部分对应的梯度，即：</p>
<script type="math/tex; mode=display">
\kappa=\frac{\partial\log(1+exp(-y_i\phi(V,x))) }{\partial \phi(V,x) }=\frac{-y}{1+\exp(y\phi(V,x) )} \qquad(22)</script><p>再重申一次，<script type="math/tex">g</script>与<script type="math/tex">V</script>都是k维的向量，在python中可以作为一个向量计算，在java/c++等需要通过一个循环进行计算。</p>
<p>详细推导（21）式如下：<br>（1）在SGD中，式（19）可以转化为：</p>
<script type="math/tex; mode=display">
\min \frac{\lambda}{2}||V||_2^2+\log(1+exp(-y_i\phi(V,x))) \qquad(23)</script><p>（2）上式对<script type="math/tex">V_{j1,f2}</script>求偏导，可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial \frac{\lambda}{2}||V||_2^2+\log(1+exp(-y_i\phi(V,x)))}{\partial V_{j1,f2}} \\
&=\lambda \cdot V_{j1,f2} + \frac{\partial \log(1+exp(-y_i\phi(V,x)))}{\partial V_{j1,f2}}\\
&=\lambda \cdot V_{j1,f2} + \frac{\partial \log(1+exp(-y_i\phi(V,x)))}{\partial \phi} \cdot  \frac{\partial \phi}{V_{j1,f2}}\\
&=\lambda \cdot V_{j1,f2} + \frac{-y}{1+\exp(y\phi(V,x) )}  \cdot V_{j2,f1}
\end{aligned}
\qquad(24)</script><h3 id="2-3-2-计算累积梯度平方和"><a href="#2-3-2-计算累积梯度平方和" class="headerlink" title="2.3.2 计算累积梯度平方和"></a>2.3.2 计算累积梯度平方和</h3><p>计算从第一个样本，到当前样本（第d个）以来的累积梯度平方和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
(G_{j1,f2})_d=(G_{j1,f2})_{d-1}+(g_{j1,f2})_d^2 \\
(G_{j2,f1})_d=(G_{j2,f1})_{d-1}+(g_{j2,f1})_d^2
\end{aligned}
\qquad(26)</script><h3 id="2-3-3-更新隐变量"><a href="#2-3-3-更新隐变量" class="headerlink" title="2.3.3 更新隐变量"></a>2.3.3 更新隐变量</h3><script type="math/tex; mode=display">
\begin{aligned}
(V_{j1,f2})_d=(V_{j1,f2})_{d-1}-\frac{\eta}{\sqrt{(G_{j1,f2})_d}} \cdot (g_{j1,f2})_d\\
(V_{j2,f1})_d=(V_{j2,f1})_{d-1}-\frac{\eta}{\sqrt{(G_{j2,f1})_d}} \cdot (g_{j2,f1})_d
\end{aligned}
\qquad(26)</script><h3 id="2-3-4-关于初始参数的设定"><a href="#2-3-4-关于初始参数的设定" class="headerlink" title="2.3.4 关于初始参数的设定"></a>2.3.4 关于初始参数的设定</h3><p>文献1中如此建议：<br>（1）<script type="math/tex">\eta</script>：没有具体的建议，用户根据经验指定即可，一般会取0.1，0.01，0.001。</p>
<p>（2）<script type="math/tex">V</script>：在区间<script type="math/tex">[0,1/\sqrt{k}]</script>间的随机值，均匀分布即可。</p>
<p>（3）<script type="math/tex">G</script>：设置为1，以避免<script type="math/tex">(G_{j1,f2})_d^{-\frac{1}{2}}</script>出现很大的值。</p>
<h2 id="2-4-时间复杂度"><a href="#2-4-时间复杂度" class="headerlink" title="2.4 时间复杂度"></a>2.4 时间复杂度</h2><h3 id="2-4-1-计算时间复杂度"><a href="#2-4-1-计算时间复杂度" class="headerlink" title="2.4.1 计算时间复杂度"></a>2.4.1 计算时间复杂度</h3><p>由于式(18)无法做类似于式（10）的简化，因此FFM的计算时间复杂度为<script type="math/tex">O(kn^2)</script>。</p>
<h3 id="2-4-2-训练时间复杂度"><a href="#2-4-2-训练时间复杂度" class="headerlink" title="2.4.2 训练时间复杂度"></a>2.4.2 训练时间复杂度</h3><p>由于训练时，需要先根据式（18）计算<script type="math/tex">\phi</script>，复杂度为<script type="math/tex">O(kn^2)</script>，计算得到<script type="math/tex">\phi</script>后，还需要按照式（22）计算1次，按照式（21）计算2k次，按照式（23）计算2k次，按照式（24）计算2k次，也就是说，总的训练时间复杂度为：</p>
<script type="math/tex; mode=display">
O(kn^2) + 1 + 2k + 2k + 2k = O(kn^2)</script><p>因此，训练时间复杂度为<script type="math/tex">O(kn^2)</script>。</p>
<h2 id="2-5-计算速度优化"><a href="#2-5-计算速度优化" class="headerlink" title="2.5 计算速度优化"></a>2.5 计算速度优化</h2><h3 id="2-5-1-openMP"><a href="#2-5-1-openMP" class="headerlink" title="2.5.1 openMP"></a>2.5.1 openMP</h3><p>OpenMP提供的这种对于并行描述的高层抽象降低了并行编程的难度和复杂度，这样程序员可以把更多的精力投入到并行算法本身，而非其具体实现细节。对基于数据分集的多线程程序设计，OpenMP是一个很好的选择。同时，使用OpenMP也提供了更强的灵活性，可以较容易的适应不同的并行系统配置。线程粒度和负载平衡等是传统多线程程序设计中的难题，但在OpenMP中，OpenMP库从程序员手中接管了部分这两方面的工作。</p>
<p>openPM原生支持C/C++/Fortran，但java可以通过jomp等引入，未测试。</p>
<h3 id="2-5-2-SSE3"><a href="#2-5-2-SSE3" class="headerlink" title="2.5.2 SSE3"></a>2.5.2 SSE3</h3><p>SSE3 中13个新指令的主要目的是改进线程同步和特定应用程序领域，例如媒体和游戏。这些新增指令强化了处理器在浮点转换至整数、复杂算法、视频编码、SIMD浮点寄存器操作以及线程同步等五个方面的表现，最终达到提升多媒体和游戏性能的目的。Intel是从Prescott核心的Pentium 4开始支持SSE3指令集的，而AMD则是从2005年下半年Troy核心的Opteron开始才支持SSE3的。但是需要注意的是，AMD所支持的SSE3与Intel的SSE3并不完全相同，主要是删除了针对Intel超线程技术优化的部分指令。<br>SSE3指令采用128位的寄存器，可以同时操作4个单精度浮点数或者整数，因此非常类似于向量运算。这对于有大量向量计算的的FFM模型是有用的。<br>但事实上，计算<script type="math/tex">\phi</script>是几乎无用，而这是最耗时间的部分。</p>
<h3 id="2-5-3-ParameterServer"><a href="#2-5-3-ParameterServer" class="headerlink" title="2.5.3 ParameterServer"></a>2.5.3 ParameterServer</h3><p><a href="https://www.zybuluo.com/Dounm/note/517675" target="_blank" rel="noopener">https://www.zybuluo.com/Dounm/note/517675</a><br>Paraeter Server框架中，每个server都只负责分到的部分参数（server共同维持一个全局共享参数）。server节点可以和其他server节点通信，每个server负责自己分到的参数，server group共同维持所有参数的更新。server manage node负责维护一些元数据的一致性，例如各个节点的状态，参数的分配情况。</p>
<h2 id="2-6模型优化"><a href="#2-6模型优化" class="headerlink" title="2.6模型优化"></a>2.6模型优化</h2><h3 id="2-6-1-特征编码连续"><a href="#2-6-1-特征编码连续" class="headerlink" title="2.6.1 特征编码连续"></a>2.6.1 特征编码连续</h3><p>如果特征的编码不连续，比如编码是有意义的，或者预留空间给之后的编码。如果直接使用最大编码值的作为参数数据尺寸，则会导致大量内存空间的浪费，因此有2种解决方案：<br>（1）使用hashmap，而非数组。<br>（2）将有意义的编码映射到一个连续的编码空间。<br>目前我们使用方式（1），理论上方式（2）的计算速度会更快。</p>
<h3 id="2-6-2-一次项缺失的影响"><a href="#2-6-2-一次项缺失的影响" class="headerlink" title="2.6.2 一次项缺失的影响"></a>2.6.2 一次项缺失的影响</h3><p>正如上面式（18）所言，我们经常会忽略了一次项的影响，因此我们可以为每个样本加上一个辅助特征，这项特征的值恒为1，这相当于引入了一次项。</p>
<h3 id="2-6-3-样本归一化"><a href="#2-6-3-样本归一化" class="headerlink" title="2.6.3 样本归一化"></a>2.6.3 样本归一化</h3><p>文献1还建议，将样本向量的长度归一化后，性能有少量的提升。</p>
<script type="math/tex; mode=display">
R[i]=\frac{1}{||X||}</script><h3 id="2-6-4-特征归一化"><a href="#2-6-4-特征归一化" class="headerlink" title="2.6.4 特征归一化"></a>2.6.4 特征归一化</h3><p>某些特征（如购买个数）有可能很大，而一些类别参数则恒为1，这将导致不同特征最终对模型的影响相关很大，这很不合理。</p>
<h1 id="五、DEEP-amp-WIDE"><a href="#五、DEEP-amp-WIDE" class="headerlink" title="五、DEEP &amp; WIDE"></a>五、DEEP &amp; WIDE</h1>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io" rel="external nofollow noreferrer">Myhaa</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://myhaa.github.io/2019/10/17/tui-jian-xi-tong-zhi-suan-fa-jie-shao/">https://myhaa.github.io/2019/10/17/tui-jian-xi-tong-zhi-suan-fa-jie-shao/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D/">
                                    <span class="chip bg-color">算法介绍</span>
                                </a>
                            
                                <a href="/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/">
                                    <span class="chip bg-color">协同过滤</span>
                                </a>
                            
                                <a href="/tags/CTR%E9%A2%84%E4%BC%B0/">
                                    <span class="chip bg-color">CTR预估</span>
                                </a>
                            
                                <a href="/tags/LR/">
                                    <span class="chip bg-color">LR</span>
                                </a>
                            
                                <a href="/tags/FM/">
                                    <span class="chip bg-color">FM</span>
                                </a>
                            
                                <a href="/tags/FFM/">
                                    <span class="chip bg-color">FFM</span>
                                </a>
                            
                                <a href="/tags/DEEP-WIDE/">
                                    <span class="chip bg-color">DEEP &amp; WIDE</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">感谢您的赏识！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'a1900fd4b8fb7a569ef7',
        clientSecret: 'd1176a5ad242e4887008f5d4389ea5a35f199c44',
        repo: 'myhaa.github.io',
        owner: 'myhaa',
        admin: ["myhaa"],
        id: '2019-10-17T15-21-11',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2019/11/12/shi-yong-gong-ju-zhi-hexo/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="实用工具之hexo">
                        
                        <span class="card-title">实用工具之hexo</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            运用Hexo+GitHub搭建个人博客详细教程
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2019-11-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/" class="post-category">
                                    实用工具
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Hexo/">
                        <span class="chip bg-color">Hexo</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/10/15/shi-yong-gong-ju-zhi-python/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/19.jpg" class="responsive-img" alt="实用工具之python">
                        
                        <span class="card-title">实用工具之python</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            有关Python的笔记
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2019-10-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/" class="post-category">
                                    实用工具
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">
                        <span class="chip bg-color">编程语言</span>
                    </a>
                    
                    <a href="/tags/python/">
                        <span class="chip bg-color">python</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Myhaa's Blog<br />'
            + '文章作者: Myhaa<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>

    
<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">年份</span>
            <a href="https://myhaa.github.io" target="_blank">Myhaa</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    window.setTimeout("siteTime()", 1000);
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2019";
                    var startMonth = "11";
                    var startDate = "11";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">














    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->


    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    
    
    
    <script src="/libs/instantpage/instantpage.js" type="module"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
    

</body>

</html>
