<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>实用工具之FFmpeg</title>
      <link href="/2021/09/06/shi-yong-gong-ju-zhi-ffmpeg/"/>
      <url>/2021/09/06/shi-yong-gong-ju-zhi-ffmpeg/</url>
      
        <content type="html"><![CDATA[<h1 id="FFmpeg视频处理工具"><a href="#FFmpeg视频处理工具" class="headerlink" title="FFmpeg视频处理工具"></a>FFmpeg视频处理工具</h1><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">conda install ffmpeg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h4 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h4><p>视频文件本身是一个<code>容器</code>，里面包含<code>视频</code>、<code>音频</code>、<code>字幕</code>等等</p><p>常见的容器格式有以下几种（一般后缀能反映其格式）</p><ol><li>mp4</li><li>mkv</li><li>webm</li><li>avi</li></ol><h5 id="查看ffmpeg支持容器格式"><a href="#查看ffmpeg支持容器格式" class="headerlink" title="查看ffmpeg支持容器格式"></a>查看<code>ffmpeg</code>支持容器格式</h5><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -formats<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="编码格式"><a href="#编码格式" class="headerlink" title="编码格式"></a>编码格式</h4><p>视频、音频都需要编码才能保存成文件。不同的编码格式，有不同的压缩率，会导致文件大小和清晰度的差异</p><p>常见的视频编码格式有以下几种</p><ul><li>有版权，免费使用<ul><li>H.262</li><li>H.264</li><li>H.265</li></ul></li><li>无版权<ul><li>VP8</li><li>VP9</li><li>AV1</li></ul></li></ul><p>常见的音频编码格式</p><ul><li>无损<ul><li>FLAC</li><li>ALAC</li></ul></li><li>有损<ul><li>MP3</li><li>AAC</li></ul></li></ul><h5 id="查看ffmpeg支持编码格式"><a href="#查看ffmpeg支持编码格式" class="headerlink" title="查看ffmpeg支持编码格式"></a>查看ffmpeg支持编码格式</h5><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -codecs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p>编码器是实现某种编码格式的库文件，只有安装了对应编码格式的编码器，才能实现该格式视频、音频编码和解码</p><p>常见的视频编码器</p><ol><li>libx264：最流行的开源H.264编码器</li><li>NVENC：基于NVIDIA GPU的H.264编码器</li><li>libx265：开源的HEVC编码器</li><li>libvpx：谷歌的VP8和VP9编码器</li><li>libaom：AV1编码器</li></ol><p>常见音频编码器</p><ol><li>libfdk-aac：当前最高质量的AAC编码，编码模式分为CBR和VBR</li><li>aac</li></ol><h5 id="查看ffmpeg已安装编码器"><a href="#查看ffmpeg已安装编码器" class="headerlink" title="查看ffmpeg已安装编码器"></a>查看ffmpeg已安装编码器</h5><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -encoders<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="使用格式"><a href="#使用格式" class="headerlink" title="使用格式"></a>使用格式</h3><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg {1} {2} -i {3} {4} {5}<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>其中：</p><ol><li>全局参数</li><li>输入文件参数</li><li>输入文件</li><li>输出文件参数</li><li>输出文件</li></ol><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-y \  # 全局参数-c \  # 输入文件参数-i input.mp4 \  # 输入文件-c:v libvpx-vp9 -c:a libvorbis \  # 输出文件参数output.webm  # 输出文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的命令是将mp4文件转成webm文件，两个都是容器格式。输入的mp4文件的音频编码格式是aac，视频编码格式是H.264；输出的webm文件的视频编码格式是vp9，音频格式是vorbis</p><ul><li>如果不指明编码格式，FFmpeg会自己判断输入文件编码</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -i input.avi output.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="常用命令行参数"><a href="#常用命令行参数" class="headerlink" title="常用命令行参数"></a>常用命令行参数</h3><ul><li><code>-c</code>：指定编码器</li><li><code>-c copy</code>：直接复制，不经过重新编码（这样比较快）</li><li><code>-c:v</code>：指定视频编码器</li><li><code>-c:a</code>：指定音频编码器</li><li><code>-i</code>：指定输入文件</li><li><code>-an</code>：去除音频流</li><li><code>-vn</code>：去除视频流</li><li><code>-preset</code>：指定输出视频的视频质量，会影响文件生成速度，以下几个可用值：<ul><li>ultrafast</li><li>superfast</li><li>veryfast</li><li>faster</li><li>fast</li><li>medium</li><li>slow</li><li>slower</li><li>veryslow</li></ul></li><li><code>-y</code>：不经过确认，直接覆盖同名文件</li></ul><h3 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h3><h4 id="查看文件信息"><a href="#查看文件信息" class="headerlink" title="查看文件信息"></a>查看文件信息</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -i input.mp4ffmpeg -i input.mp4 -hide_banner<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="转换编码格式"><a href="#转换编码格式" class="headerlink" title="转换编码格式"></a>转换编码格式</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -i input.mp4 -c:v libx265 outpu.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="转换容器格式"><a href="#转换容器格式" class="headerlink" title="转换容器格式"></a>转换容器格式</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -i input.mp4 -c copy output.webm<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="调整码率"><a href="#调整码率" class="headerlink" title="调整码率"></a>调整码率</h4><p>调整码率指的是改变编码的比特率，一般用来将视频文件的体积变小</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-i input.mp4 \-minrate 964K -maxrate 3856K -bufsize 2000K \output.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="改变分辨率"><a href="#改变分辨率" class="headerlink" title="改变分辨率"></a>改变分辨率</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-i input.mp4 \-vf scale=480:-1 \output.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="提取音频"><a href="#提取音频" class="headerlink" title="提取音频"></a>提取音频</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-i input.mp4\-vn -c:a copy \output.aac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="添加音轨"><a href="#添加音轨" class="headerlink" title="添加音轨"></a>添加音轨</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-i input.aac -i input.mp4 \output.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h4><ul><li>指定时间开始，连续对1秒钟的视频进行截图</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-y \-i input.mp4 \-ss 00:01:24 -t 00:00:01 \output_%3d.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>只需要截一张图</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-ss 01:23:45 \-i input.mp4 \-vframes 1 -q:v 2 \  # 只截取一帧，输出图片质量为2，1最高output.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg -ss [start] -i [input] -t [duration] -c copy [output]ffmpeg -ss [start] -i [input] -to [end] -c copy [output]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="为音频添加封面"><a href="#为音频添加封面" class="headerlink" title="为音频添加封面"></a>为音频添加封面</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">ffmpeg \-loop 1 \  # 表示图片无限循环-i cover.jpg -i input.mp3 \-c:v libx264 -c:a aac -b:a 192k -shortest \ # 表示音频文件结束，输出视频就结束output.mp4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><h2 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.ruanyifeng.com/blog/2020/01/ffmpeg.html" target="_blank" rel="noopener">阮一峰的网络日志</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FFmpeg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之吴恩达课程作业3</title>
      <link href="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/"/>
      <url>/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/</url>
      
        <content type="html"><![CDATA[<h1 id="吴恩达深度学习课程作业L1W4"><a href="#吴恩达深度学习课程作业L1W4" class="headerlink" title="吴恩达深度学习课程作业L1W4"></a>吴恩达深度学习课程作业L1W4</h1><ul><li>构建多层隐藏层神经网络</li></ul><h2 id="HW1参考"><a href="#HW1参考" class="headerlink" title="HW1参考"></a>HW1参考</h2><ol><li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li><li><a href="https://www.heywhale.com/mw/project/5dd798fbf41512002ceb38de" target="_blank" rel="noopener">作业链接</a></li><li><a href="https://github.com/suqi/deeplearning_andrewng" target="_blank" rel="noopener">作业链接2</a></li></ol><h2 id="HW2参考"><a href="#HW2参考" class="headerlink" title="HW2参考"></a>HW2参考</h2><ol><li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li><li><a href="https://github.com/JudasDie/deeplearning.ai" target="_blank" rel="noopener">作业链接</a></li><li><a href="https://github.com/suqi/deeplearning_andrewng" target="_blank" rel="noopener">作业链接2</a></li></ol><h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1-导入模块"></a>1-导入模块</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import timeimport numpy as npimport h5pyimport matplotlib.pyplot as pltimport scipyfrom PIL import Imagefrom scipy import ndimage%matplotlib inlineplt.rcParams['figure.figsize'] = (5.0, 4.0)plt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# load_extension%load_ext autoreload  # Then your module will be auto-reloaded by default%autoreload 2  np.random.seed(1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">root_path = './深度学习之吴恩达课程作业3/'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-一些函数"><a href="#2-一些函数" class="headerlink" title="2-一些函数"></a>2-一些函数</h2><h3 id="2-1-testCases-v3-py"><a href="#2-1-testCases-v3-py" class="headerlink" title="2.1-testCases_v3.py"></a>2.1-testCases_v3.py</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_forward_test_case():    np.random.seed(1)    """    X = np.array([[-1.02387576, 1.12397796],                 [-1.62328545, 0.64667545],                 [-1.74314104, -0.59664964]])    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])    b = np.array([[1]])    """    A = np.random.randn(3,2)    W = np.random.randn(1,3)    b = np.random.randn(1,1)    return A, W, bdef func_linear_activation_forward_test_case():    """    X = np.array([[-1.02387576, 1.12397796],                 [-1.62328545, 0.64667545],                 [-1.74314104, -0.59664964]])    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])    b = 5    """    np.random.seed(2)    A_prev = np.random.randn(3,2)    W = np.random.randn(1,3)    b = np.random.randn(1,1)    return A_prev, W, bdef func_L_model_forward_test_case():    """    X = np.array([[-1.02387576, 1.12397796],                 [-1.62328545, 0.64667545],                 [-1.74314104, -0.59664964]])    parameters = {        'W1': np.array([[ 1.62434536, -0.61175641, -0.52817175],                        [-1.07296862,  0.86540763, -2.3015387 ]]),        'W2': np.array([[ 1.74481176, -0.7612069 ]]),        'b1': np.array([[ 0.], [ 0.]]),        'b2': np.array([[ 0.]])    }    """    np.random.seed(1)    X = np.random.randn(4,2)    W1 = np.random.randn(3,4)    b1 = np.random.randn(3,1)    W2 = np.random.randn(1,3)    b2 = np.random.randn(1,1)    parameters = {"W1": W1,                  "b1": b1,                  "W2": W2,                  "b2": b2}    return X, parametersdef func_compute_cost_test_case():    Y = np.asarray([[1, 1, 1]])    aL = np.array([[.8,.9,0.4]])    return Y, aLdef func_linear_backward_test_case():    """    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],       [-1.62328545,  0.64667545],       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))    """    np.random.seed(1)    dZ = np.random.randn(1,2)    A = np.random.randn(3,2)    W = np.random.randn(1,3)    b = np.random.randn(1,1)    linear_cache = (A, W, b)    return dZ, linear_cachedef func_linear_activation_backward_test_case():    """    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))    """    np.random.seed(2)    dA = np.random.randn(1,2)    A = np.random.randn(3,2)    W = np.random.randn(1,3)    b = np.random.randn(1,1)    Z = np.random.randn(1,2)    linear_cache = (A, W, b)    activation_cache = Z    linear_activation_cache = (linear_cache, activation_cache)    return dA, linear_activation_cachedef func_L_model_backward_test_case():    """    X = np.random.rand(3,2)    Y = np.array([[1, 1]])    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],           [ 0.02738759,  0.67046751],           [ 0.4173048 ,  0.55868983]]),    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),    np.array([[ 0.]])),   np.array([[ 0.41791293,  1.91720367]]))])   """    np.random.seed(3)    AL = np.random.randn(1, 2)    Y = np.array([[1, 0]])    A1 = np.random.randn(4,2)    W1 = np.random.randn(3,4)    b1 = np.random.randn(3,1)    Z1 = np.random.randn(3,2)    linear_cache_activation_1 = ((A1, W1, b1), Z1)    A2 = np.random.randn(3,2)    W2 = np.random.randn(1,3)    b2 = np.random.randn(1,1)    Z2 = np.random.randn(1,2)    linear_cache_activation_2 = ((A2, W2, b2), Z2)    caches = (linear_cache_activation_1, linear_cache_activation_2)    return AL, Y, cachesdef func_update_parameters_test_case():    """    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747],        [-1.8634927 , -0.2773882 , -0.35475898],        [-0.08274148, -0.62700068, -0.04381817],        [-0.47721803, -1.31386475,  0.88462238]]),    'W2': np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),        'W3': np.array([[-1.02378514, -0.7129932 ,  0.62524497],        [-0.16051336, -0.76883635, -0.23003072]]),    'b1': np.array([[ 0.],        [ 0.],        [ 0.],        [ 0.]]),    'b2': np.array([[ 0.],        [ 0.],        [ 0.]]),    'b3': np.array([[ 0.],        [ 0.]])}    grads = {'dW1': np.array([[ 0.63070583,  0.66482653,  0.18308507],        [ 0.        ,  0.        ,  0.        ],        [ 0.        ,  0.        ,  0.        ],        [ 0.        ,  0.        ,  0.        ]]),    'dW2': np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],        [ 0.        ,  0.        ,  0.        ,  0.        ],        [ 0.        ,  0.        ,  0.        ,  0.        ]]),    'dW3': np.array([[-1.40260776,  0.        ,  0.        ]]),    'da1': np.array([[ 0.70760786,  0.65063504],        [ 0.17268975,  0.15878569],        [ 0.03817582,  0.03510211]]),    'da2': np.array([[ 0.39561478,  0.36376198],        [ 0.7674101 ,  0.70562233],        [ 0.0224596 ,  0.02065127],        [-0.18165561, -0.16702967]]),    'da3': np.array([[ 0.44888991,  0.41274769],        [ 0.31261975,  0.28744927],        [-0.27414557, -0.25207283]]),    'db1': 0.75937676204411464,    'db2': 0.86163759922811056,    'db3': -0.84161956022334572}    """    np.random.seed(2)    W1 = np.random.randn(3,4)    b1 = np.random.randn(3,1)    W2 = np.random.randn(1,3)    b2 = np.random.randn(1,1)    parameters = {"W1": W1,                  "b1": b1,                  "W2": W2,                  "b2": b2}    np.random.seed(3)    dW1 = np.random.randn(3,4)    db1 = np.random.randn(3,1)    dW2 = np.random.randn(1,3)    db2 = np.random.randn(1,1)    grads = {"dW1": dW1,             "db1": db1,             "dW2": dW2,             "db2": db2}    return parameters, gradsdef func_L_model_forward_test_case_2hidden():    np.random.seed(6)    X = np.random.randn(5,4)    W1 = np.random.randn(4,5)    b1 = np.random.randn(4,1)    W2 = np.random.randn(3,4)    b2 = np.random.randn(3,1)    W3 = np.random.randn(1,3)    b3 = np.random.randn(1,1)    parameters = {"W1": W1,                  "b1": b1,                  "W2": W2,                  "b2": b2,                  "W3": W3,                  "b3": b3}    return X, parametersdef func_print_grads(grads):    print ("dW1 = "+ str(grads["dW1"]))    print ("db1 = "+ str(grads["db1"]))    print ("dA1 = "+ str(grads["dA2"])) # this is done on purpose to be consistent with lecture where we normally start with A0    # in this implementation we started with A1, hence we bump it up by 1.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2-dnn-utils-v2-py"><a href="#2-2-dnn-utils-v2-py" class="headerlink" title="2.2-dnn_utils_v2.py"></a>2.2-dnn_utils_v2.py</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_sigmoid(Z):    """    Implements the sigmoid activation in numpy    Arguments:    Z -- numpy array of any shape    Returns:    A -- output of sigmoid(z), same shape as Z    cache -- returns Z as well, useful during backpropagation    """    A = 1/(1+np.exp(-Z))    cache = Z    return A, cachedef func_relu(Z):    """    Implement the RELU function.    Arguments:    Z -- Output of the linear layer, of any shape    Returns:    A -- Post-activation parameter, of the same shape as Z    cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently    """    A = np.maximum(0,Z)    assert(A.shape == Z.shape)    cache = Z     return A, cachedef func_relu_backward(dA, cache):    """    Implement the backward propagation for a single RELU unit.    Arguments:    dA -- post-activation gradient, of any shape    cache -- 'Z' where we store for computing backward propagation efficiently    Returns:    dZ -- Gradient of the cost with respect to Z    """    Z = cache    dZ = np.array(dA, copy=True) # just converting dz to a correct object.    # When z <= 0, you should set dz to 0 as well.     dZ[Z <= 0] = 0    assert (dZ.shape == Z.shape)    return dZdef func_sigmoid_backward(dA, cache):    """    Implement the backward propagation for a single SIGMOID unit.    Arguments:    dA -- post-activation gradient, of any shape    cache -- 'Z' where we store for computing backward propagation efficiently    Returns:    dZ -- Gradient of the cost with respect to Z    """    Z = cache    s = 1/(1+np.exp(-Z))    dZ = dA * s * (1-s)    assert (dZ.shape == Z.shape)    return dZ<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-主要框架"><a href="#3-主要框架" class="headerlink" title="3-主要框架"></a>3-主要框架</h2><ol><li>初始化参数：$L$层神经网络</li><li>实现前向传播模块</li><li>计算损失</li><li>实现反向传播模块</li><li>更新参数</li></ol><ul><li>如下图</li></ul><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/final_outline.png" alt="final_outline.png"></p><ul><li>注意<ol><li>每一个前向传播对应一个后向传播，所以要将前向传播的值cache，以便后向传播计算梯度</li></ol></li></ul><h2 id="4-参数初始化"><a href="#4-参数初始化" class="headerlink" title="4-参数初始化"></a>4-参数初始化</h2><ol><li>实现2层神经网络的参数初始化</li><li>实现$L$层神经网络的参数初始化</li></ol><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/relu.png" alt="relu"></p><h3 id="4-1-2层神经网络"><a href="#4-1-2层神经网络" class="headerlink" title="4.1-2层神经网络"></a>4.1-2层神经网络</h3><ul><li>模型结构是：<em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em></li><li>用<code>np.random.randn(shape)*0.01</code>来随机初始化参数</li><li>用<code>np.zeros(shape)</code>来初始化bias</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_2_layers_initialize_parameters(n_x, n_h, n_y):    """    2层神经网络的参数初始化    :param n_x: size of the input layer    :param n_h: size of the hidden layer    :param n_y: size of the output layer    :return parameters: python dictionary containing your parameters:        W1 -- weight matrix of shape (n_h, n_x)        b1 -- bias vector of shape (n_h, 1)        W2 -- weight matrix of shape (n_y, n_h)        b2 -- bias vector of shape (n_y, 1)    """    np.random.seed(1)    W1 = np.random.randn(n_h, n_x) * 0.01    b1 = np.zeros((n_h, 1))    W2 = np.random.randn(n_y, n_h) * 0.01    b2 = np.zeros((n_y, 1))    parameters = {        'W1': W1,        'b1': b1,        'W2': W2,        'b2': b2    }    return parameters<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_2_layers_initialize_parameters(3, 2, 1)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>{&#39;W1&#39;: array([[ 0.01624345, -0.00611756, -0.00528172],        [-0.01072969,  0.00865408, -0.02301539]]), &#39;b1&#39;: array([[0.],        [0.]]), &#39;W2&#39;: array([[ 0.01744812, -0.00761207]]), &#39;b2&#39;: array([[0.]])}</code></pre><h3 id="3-2-L层神经网络"><a href="#3-2-L层神经网络" class="headerlink" title="3.2-L层神经网络"></a>3.2-L层神经网络</h3><ul><li>以输入$X$大小为(12288,209)(m=209)为例，各层参数如下表：</li></ul><div class="table-container"><table><thead><tr><th style="text-align:left">-</th><th style="text-align:left">shape of W</th><th style="text-align:left">shape of b</th><th style="text-align:left">activation</th><th style="text-align:left">shape of activation</th></tr></thead><tbody><tr><td style="text-align:left">layer 1</td><td style="text-align:left">$(n^{[1]},12288)$</td><td style="text-align:left">$(n^{[1]},1)$</td><td style="text-align:left">$Z^{[1]} = W^{[1]}  X + b^{[1]}$</td><td style="text-align:left">$(n^{[1]},209)$</td></tr><tr><td style="text-align:left">layer 2</td><td style="text-align:left">$(n^{[2]}, n^{[1]})$</td><td style="text-align:left">$(n^{[2]},1)$</td><td style="text-align:left">$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$</td><td style="text-align:left">$(n^{[2]}, 209)$</td></tr><tr><td style="text-align:left">…</td><td style="text-align:left">…</td><td style="text-align:left">…</td><td style="text-align:left">…</td><td style="text-align:left">…</td></tr><tr><td style="text-align:left">layer L-1</td><td style="text-align:left">$(n^{[L-1]}, n^{[L-2]})$</td><td style="text-align:left">$(n^{[L-1]}, 1)$</td><td style="text-align:left">$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$</td><td style="text-align:left">$(n^{[L-1]}, 209)$</td></tr><tr><td style="text-align:left">layer L</td><td style="text-align:left">$(n^{[L]}, n^{[L-1]})$</td><td style="text-align:left">$(n^{[L]}, 1)$</td><td style="text-align:left">$Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td><td style="text-align:left">$(n^{[L]}, 209)$</td></tr></tbody></table></div><ul><li>在python中，合理运用broadcast机制，如下：</li></ul><script type="math/tex; mode=display">W = \begin{bmatrix}    j  & k  & l\\    m  & n & o \\    p  & q & r \end{bmatrix}\;\;\; X = \begin{bmatrix}    a  & b  & c\\    d  & e & f \\    g  & h & i \end{bmatrix} \;\;\; b =\begin{bmatrix}    s  \\    t  \\    u\end{bmatrix}\tag{2}</script><p>Then $WX + b$ will be:</p><script type="math/tex; mode=display">WX + b = \begin{bmatrix}(ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\(ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\(pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\end{bmatrix}\tag{3}</script><ul><li>练习<ol><li>实现L层神经网络参数初始化</li></ol></li><li>说明<ol><li>模型结构为：<em> [LINEAR -&gt; RELU] $ \times$ (L-1) -&gt; LINEAR -&gt; SIGMOID </em></li><li>使用<code>np.random.randn(shape)*0.01</code>初始化$W$</li><li>使用<code>np.zeros(shape)</code>初始化$b$</li><li>将$n^{[l]}$存储在<code>layer_dims</code>中，例如<code>layer_dims=[2,4,1]</code>意味着2个输入特征，一个隐藏层4个units，1个输出</li></ol></li><li><font size="5" color="red">强调</font>：<ol><li>这里<code>/ np.sqrt(layer_dims[layer-1])</code>很重要，如果还是<code>*0.01</code>，会导致模型cost降不下去</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layers_initialize_parameters(layer_dims):    """    L层神经网络参数初始化    :param layer_dims: python array (list) containing the dimensions of each layer in our network    :return:         parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":        Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])        bl -- bias vector of shape (layer_dims[l], 1)    """    np.random.seed(1)    parameters = {}    L = len(layer_dims)    # 这里/ np.sqrt(layer_dims[layer-1])很重要，如果还是*0.01，会导致模型cost降不下去    for layer in range(1, L):        parameters['W' + str(layer)] = np.random.randn(layer_dims[layer], layer_dims[layer-1]) / np.sqrt(layer_dims[layer-1]) # * 0.01        parameters['b' + str(layer)] = np.zeros((layer_dims[layer], 1))        assert(parameters['W' + str(layer)].shape == (layer_dims[layer], layer_dims[layer-1]))        assert(parameters['b' + str(layer)].shape == (layer_dims[layer], 1))    return parameters<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_L_layers_initialize_parameters(layer_dims=[5, 4, 3])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>{&#39;W1&#39;: array([[ 0.72642933, -0.27358579, -0.23620559, -0.47984616,  0.38702206],        [-1.0292794 ,  0.78030354, -0.34042208,  0.14267862, -0.11152182],        [ 0.65387455, -0.92132293, -0.14418936, -0.17175433,  0.50703711],        [-0.49188633, -0.07711224, -0.39259022,  0.01887856,  0.26064289]]), &#39;b1&#39;: array([[0.],        [0.],        [0.],        [0.]]), &#39;W2&#39;: array([[-0.55030959,  0.57236185,  0.45079536,  0.25124717],        [ 0.45042797, -0.34186393, -0.06144511, -0.46788472],        [-0.13394404,  0.26517773, -0.34583038, -0.19837676]]), &#39;b2&#39;: array([[0.],        [0.],        [0.]])}</code></pre><h2 id="5-forward-propagation"><a href="#5-forward-propagation" class="headerlink" title="5-forward propagation"></a>5-forward propagation</h2><h3 id="5-1-linear-forward"><a href="#5-1-linear-forward" class="headerlink" title="5.1-linear forward"></a>5.1-linear forward</h3><ul><li>公式<script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\A^{[0]} = X</script></li><li>练习<ol><li>实现<code>func_linear_forward()</code>函数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_forward(A, W, b):    """    linear forward    :param A:    :param W:    :param b:    :return Z,chche:        Z -- the input of the activation function, also called pre-activation parameter         cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently    """    Z = np.dot(W, A) + b    assert(Z.shape == (W.shape[0], A.shape[1]))    cache = (A, W, b)    return Z, cache<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, W, b = func_linear_forward_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(A.shape, W.shape, b.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(3, 2) (1, 3) (1, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">Z, linear_cache = func_linear_forward(A, W, b)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">Z<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[ 3.26295337, -1.23429987]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_cache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(array([[ 1.62434536, -0.61175641],        [-0.52817175, -1.07296862],        [ 0.86540763, -2.3015387 ]]), array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]), array([[-0.24937038]]))</code></pre><h3 id="5-2-linear-activation-forward"><a href="#5-2-linear-activation-forward" class="headerlink" title="5.2-linear activation forward"></a>5.2-linear activation forward</h3><ul><li>activation method:<ol><li>sigmoid</li><li>relu</li></ol></li><li>公式<script type="math/tex; mode=display">  A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})</script></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_activation_forward(A_prev, W, b, activation):    """    Implement the forward propagation for the LINEAR->ACTIVATION layer    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)    :param b: bias vector, numpy array of shape (size of the current layer, 1)    :param activation: the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"    :return:        A -- the output of the activation function, also called the post-activation value         cache -- a python dictionary containing "linear_cache" and "activation_cache";                 stored for computing the backward pass efficiently    """    Z, linear_cache = func_linear_forward(A_prev, W, b)    if activation == 'sigmoid':        A, activation_cache = func_sigmoid(Z)    elif activation == 'relu':        A, activation_cache = func_relu(Z)    else:        raise ValueError('activation param')    assert(A.shape == (W.shape[0], A_prev.shape[1]))    cache = (linear_cache, activation_cache)    return A, cache<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A_prev, W, b = func_linear_activation_forward_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(A_prev.shape, W.shape, b.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(3, 2) (1, 3) (1, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, linear_activation_cache = func_linear_activation_forward(A_prev, W, b, activation='sigmoid')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[0.96890023, 0.11013289]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_activation_cache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>((array([[-0.41675785, -0.05626683],         [-2.1361961 ,  1.64027081],         [-1.79343559, -0.84174737]]),  array([[ 0.50288142, -1.24528809, -1.05795222]]),  array([[-0.90900761]])), array([[ 3.43896131, -2.08938436]]))</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, linear_activation_cache = func_linear_activation_forward(A_prev, W, b, activation='relu')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[3.43896131, 0.        ]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_activation_cache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>((array([[-0.41675785, -0.05626683],         [-2.1361961 ,  1.64027081],         [-1.79343559, -0.84174737]]),  array([[ 0.50288142, -1.24528809, -1.05795222]]),  array([[-0.90900761]])), array([[ 3.43896131, -2.08938436]]))</code></pre><h3 id="5-3-L-layer-forward"><a href="#5-3-L-layer-forward" class="headerlink" title="5.3-L layer forward"></a>5.3-L layer forward</h3><ul><li>如图：</li></ul><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/model_architecture_kiank.png" alt="model_architecture_ki.png"></p><ul><li>练习：<ol><li>实现<code>func_L_model_forward()</code></li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_model_forward(X, parameters):    """    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation    :param X -- data, numpy array of shape (input size, number of examples)    :param parameters -- output of initialize_parameters_deep()    :return:        AL -- last post-activation value        caches -- list of caches containing:                    every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)                    the cache of linear_sigmoid_forward() (there is one, indexed L-1)    """    caches = []    A = X    L = len(parameters) // 2    for layer in range(1, L):        A_prev = A        W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]        A, cache = func_linear_activation_forward(A_prev, W, b, 'relu')        caches.append(cache)    A_prev = A    layer = L    W, b = parameters['W'+str(layer)], parameters['b'+str(layer)]    A, cache = func_linear_activation_forward(A_prev, W, b, 'sigmoid')    caches.append(cache)    assert(A.shape == (1, X.shape[1]))    return A, caches<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">X, parameters = func_L_model_forward_test_case_2hidden()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">X<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],       [-2.48678065,  0.91325152,  1.12706373, -1.51409323],       [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],       [-0.33588161,  1.23773784,  0.11112817,  0.12915125],       [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>{&#39;W1&#39;: array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],        [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],        [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],        [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]), &#39;b1&#39;: array([[ 1.38503523],        [-0.51962709],        [-0.78015214],        [ 0.95560959]]), &#39;W2&#39;: array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],        [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],        [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), &#39;b2&#39;: array([[ 1.50278553],        [-0.59545972],        [ 0.52834106]]), &#39;W3&#39;: array([[ 0.9398248 ,  0.42628539, -0.75815703]]), &#39;b3&#39;: array([[-0.16236698]])}</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A, caches = func_L_model_forward(X, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[0.03921668, 0.70498921, 0.19734387, 0.04728177]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">caches<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>[((array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],          [-2.48678065,  0.91325152,  1.12706373, -1.51409323],          [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],          [-0.33588161,  1.23773784,  0.11112817,  0.12915125],          [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]]),   array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],          [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],          [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],          [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),   array([[ 1.38503523],          [-0.51962709],          [-0.78015214],          [ 0.95560959]])),  array([[-5.23825714,  3.18040136,  0.4074501 , -1.88612721],         [-2.77358234, -0.56177316,  3.18141623, -0.99209432],         [ 4.18500916, -1.78006909, -0.14502619,  2.72141638],         [ 5.05850802, -1.25674082, -3.54566654,  3.82321852]])), ((array([[0.        , 3.18040136, 0.4074501 , 0.        ],          [0.        , 0.        , 3.18141623, 0.        ],          [4.18500916, 0.        , 0.        , 2.72141638],          [5.05850802, 0.        , 0.        , 3.82321852]]),   array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],          [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],          [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]),   array([[ 1.50278553],          [-0.59545972],          [ 0.52834106]])),  array([[ 2.2644603 ,  1.09971298, -2.90298027,  1.54036335],         [ 6.33722569, -2.38116246, -4.11228806,  4.48582383],         [10.37508342, -0.66591468,  1.63635185,  8.17870169]])), ((array([[ 2.2644603 ,  1.09971298,  0.        ,  1.54036335],          [ 6.33722569,  0.        ,  0.        ,  4.48582383],          [10.37508342,  0.        ,  1.63635185,  8.17870169]]),   array([[ 0.9398248 ,  0.42628539, -0.75815703]]),   array([[-0.16236698]])),  array([[-3.19864676,  0.87117055, -1.40297864, -3.00319435]]))]</code></pre><h2 id="6-损失函数"><a href="#6-损失函数" class="headerlink" title="6-损失函数"></a>6-损失函数</h2><ul><li>cross entropy公式</li></ul><script type="math/tex; mode=display">-\frac{1}{m} \sum\limits_{i = 1}^{m} \left(y^{(i)}\log(a^{[L] (i)}) + (1-y^{(i)})\log(1- a^{[L](i)})\right)</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_compute_cost(AL, Y):    """    Implement the cost function defined by equation (7).    :param AL -- probability vector corresponding to your label predictions, shape (1, number of examples)    :param Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)    :return:        cost -- cross-entropy cost    """    m = Y.shape[1]#     cost = -1 / m * np.sum(np.multiply(Y, np.log(AL))+np.multiply(1-Y, np.log(1-AL)))    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))    cost = np.squeeze(cost)    assert(cost.shape == ())    return cost<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">Y, AL = func_compute_cost_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">cost = func_compute_cost(AL, Y)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">cost<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array(0.4149316)</code></pre><h2 id="7-backward-propagation"><a href="#7-backward-propagation" class="headerlink" title="7-backward propagation"></a>7-backward propagation</h2><ul><li>反向传播模块是为了计算梯度，从而更新参数</li><li>反向传播图</li></ul><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/backprop.png"></p><caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption><ul><li>反向传播公式</li></ul><script type="math/tex; mode=display">\frac{d \mathcal{L}(a^{[2]},y)}{ {dz^{[1]}} } = \frac{d\mathcal{L}(a^{[2]},y)}{ {da^{[2]}} }\frac{ {da^{[2]} }}{ {dz^{[2]}} }\frac{ {dz^{[2]}} }{ {da^{[1]}} }\frac{ {da^{[1]}} }{ {dz^{[1]}} }  \\dW^{[1]} = \frac{\partial \mathcal{L}}{\partial W^{[1]}} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial W^{[1]}}  \\db^{[1]} = \frac{\partial \mathcal{L}}{\partial b^{[1]}} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial b^{[1]}}</script><ul><li>练习<ol><li>实现linear backward</li><li>实现激活函数的backward</li><li>实现L层的backward</li></ol></li></ul><h3 id="7-1-linear-backward"><a href="#7-1-linear-backward" class="headerlink" title="7.1-linear backward"></a>7.1-linear backward</h3><ul><li>对于layer l，<ol><li>linear forward是：$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$</li><li>linear backward是：$dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$</li></ol></li><li><p>如下图</p><p>  <img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/linearback_kiank.png" style="width:250px;height:300px;"></p>  <caption><center> **Figure 4** </center></caption></li><li><p>求解$(dW^{[l]}, db^{[l]} dA^{[l-1]})$如下：</p><script type="math/tex; mode=display">  dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \\  db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)} \\  dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}</script></li><li>解释：<ol><li>$dW^{[l]}$携带$1/m$是因为一共$m$个样本，每个样本都会计算一次损失以及$dW^{[l]}$</li><li>$W^{[l]}$的维度为$(n^{[l]}, n^{[l-1]})$</li><li>$A^{[l-1]}$的维度为$(n^{[l-1]}, m)$，故其导数不携带$1/m$</li></ol></li><li>练习：<ol><li>实现<code>func_linear_backward()</code>函数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_backward(dZ, cache):    """    Implement the linear portion of backward propagation for a single layer (layer l)    :param dZ -- Gradient of the cost with respect to the linear output (of current layer l)    :param cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer    :return:        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev        dW -- Gradient of the cost with respect to W (current layer l), same shape as W        db -- Gradient of the cost with respect to b (current layer l), same shape as b    """    A_prev, W, b = cache    m = A_prev.shape[1]    dW = 1 / m * np.dot(dZ, A_prev.T)    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)  # axis=1是行记录求和    dA_prev = np.dot(W.T, dZ)    assert(dW.shape == W.shape)    assert(db.shape == b.shape)    assert(dA_prev.shape == A_prev.shape)    return dA_prev, dW, db<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dZ, linear_cache = func_linear_backward_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dZ.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(1, 2)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for i in linear_cache:    print(i.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>(3, 2)(1, 3)(1, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db = func_linear_backward(dZ, linear_cache)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[ 0.51822968, -0.19517421],       [-0.40506361,  0.15255393],       [ 2.37496825, -0.89445391]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dW<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[-0.10076895,  1.40685096,  1.64992505]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">db<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[0.50629448]])</code></pre><h3 id="7-2-linear-activation-backward"><a href="#7-2-linear-activation-backward" class="headerlink" title="7.2-linear activation backward"></a>7.2-linear activation backward</h3><ul><li>公式</li></ul><script type="math/tex; mode=display">dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})</script><ul><li>练习<ol><li>实现<code>func_linear_activation_backward()</code>函数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_linear_activation_backward(dA, cache, activation):    """    Implement the backward propagation for the LINEAR->ACTIVATION layer.    :param dA -- post-activation gradient for current layer l     :param cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently    :param activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"    :returns        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev        dW -- Gradient of the cost with respect to W (current layer l), same shape as W        db -- Gradient of the cost with respect to b (current layer l), same shape as b    """    linear_cache, activation_cache = cache    if activation == 'relu':        dZ = func_relu_backward(dA, activation_cache)    elif activation == 'sigmoid':        dZ = func_sigmoid_backward(dA, activation_cache)    else:        raise ValueError('activation param')    dA_prev, dW, db = func_linear_backward(dZ, linear_cache)    return dA_prev, dW, db<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">AL, linear_activation_cache = func_linear_activation_backward_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">AL<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[-0.41675785, -0.05626683]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">linear_activation_cache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>((array([[-2.1361961 ,  1.64027081],         [-1.79343559, -0.84174737],         [ 0.50288142, -1.24528809]]),  array([[-1.05795222, -0.90900761,  0.55145404]]),  array([[2.29220801]])), array([[ 0.04153939, -1.11792545]]))</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db = func_linear_activation_backward(AL, linear_activation_cache, activation='sigmoid')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(array([[ 0.11017994,  0.01105339],        [ 0.09466817,  0.00949723],        [-0.05743092, -0.00576154]]), array([[ 0.10266786,  0.09778551, -0.01968084]]), array([[-0.05729622]]))</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db = func_linear_activation_backward(AL, linear_activation_cache, activation='relu')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">dA_prev, dW, db<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(array([[ 0.44090989,  0.        ],        [ 0.37883606,  0.        ],        [-0.2298228 ,  0.        ]]), array([[ 0.44513824,  0.37371418, -0.10478989]]), array([[-0.20837892]]))</code></pre><h3 id="7-3-L-layer-backward"><a href="#7-3-L-layer-backward" class="headerlink" title="7.3-L layer backward"></a>7.3-L layer backward</h3><ul><li>结构图</li></ul><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/mn_backward.png" style="width:450px;height:300px;"></p><caption><center>  **Figure 5** : Backward pass  </center></caption><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/mn_backward1.jpg" style="width:450px;height:300px;"></p><caption><center>  **Figure 5** : Backward pass 截图  </center></caption><ul><li><p>output层的梯度</p><p>  <code>dAL</code> $= \frac{\partial \mathcal{L}}{\partial A^{[L]}}$=<code>- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></p></li><li><p>练习</p><ol><li>实现<code>func_L_model_backward()</code>函数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_model_backward(AL, Y, caches):    """    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group    :param AL -- probability vector, output of the forward propagation (L_model_forward())    :param Y -- true "label" vector (containing 0 if non-cat, 1 if cat)    :param caches -- list of caches containing:                    every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)                    the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])    :return:        grads -- A dictionary with the gradients                 grads["dA" + str(l)] = ...                  grads["dW" + str(l)] = ...                 grads["db" + str(l)] = ...     """    grads = {}    L = len(caches)    m = AL.shape[1]    Y = Y.reshape(AL.shape)    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))    cur_cache = caches[L-1]    grads['dA' + str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = func_linear_activation_backward(dAL, cur_cache, activation='sigmoid')    for layer in reversed(range(L-1)):        cur_cache = caches[layer]        dA_prev_tmp, dW_tmp, db_tmp = func_linear_activation_backward(grads["dA"+str(layer+1)], cur_cache, activation='relu')        grads['dA'+str(layer)] = dA_prev_tmp        grads['dW'+str(layer+1)] = dW_tmp        grads['db'+str(layer+1)] = db_tmp#     current_cache = caches[L-1]#     grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = func_linear_activation_backward(dAL, current_cache, activation = "sigmoid")#     for l in reversed(range(L-1)):#         # lth layer: (RELU -> LINEAR) gradients.#         current_cache = caches[l]#         dA_prev_temp, dW_temp, db_temp = func_linear_activation_backward(grads["dA" + str(l + 2)], current_cache, activation = "relu")#         grads["dA" + str(l + 1)] = dA_prev_temp#         grads["dW" + str(l + 1)] = dW_temp#         grads["db" + str(l + 1)] = db_temp    return grads<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">AL, Y_assess, caches = func_L_model_backward_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">AL, Y_assess, caches<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(array([[1.78862847, 0.43650985]]), array([[1, 0]]), (((array([[ 0.09649747, -1.8634927 ],           [-0.2773882 , -0.35475898],           [-0.08274148, -0.62700068],           [-0.04381817, -0.47721803]]),    array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],           [ 0.05003364, -0.40467741, -0.54535995, -1.54647732],           [ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]),    array([[ 1.48614836],           [ 0.23671627],           [-1.02378514]])),   array([[-0.7129932 ,  0.62524497],          [-0.16051336, -0.76883635],          [-0.23003072,  0.74505627]])),  ((array([[ 1.97611078, -1.24412333],           [-0.62641691, -0.80376609],           [-2.41908317, -0.92379202]]),    array([[-1.02387576,  1.12397796, -0.13191423]]),    array([[-1.62328545]])),   array([[ 0.64667545, -0.35627076]]))))</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">grads = func_L_model_backward(AL, Y_assess, caches)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">grads<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>{&#39;dA1&#39;: array([[ 0.12913162, -0.44014127],        [-0.14175655,  0.48317296],        [ 0.01663708, -0.05670698]]), &#39;dW2&#39;: array([[-0.39202432, -0.13325855, -0.04601089]]), &#39;db2&#39;: array([[0.15187861]]), &#39;dA0&#39;: array([[ 0.        ,  0.52257901],        [ 0.        , -0.3269206 ],        [ 0.        , -0.32070404],        [ 0.        , -0.74079187]]), &#39;dW1&#39;: array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],        [0.        , 0.        , 0.        , 0.        ],        [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]), &#39;db1&#39;: array([[-0.22007063],        [ 0.        ],        [-0.02835349]])}</code></pre><h3 id="7-4-更新参数"><a href="#7-4-更新参数" class="headerlink" title="7.4-更新参数"></a>7.4-更新参数</h3><ul><li>更新公式</li></ul><script type="math/tex; mode=display">W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \\b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><ul><li>练习<ol><li>实现<code>func_update_parameters()</code>函数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_update_parameters(parameters, grads, lr):    """    Update parameters using gradient descent    :param parameters -- python dictionary containing your parameters     :param grads -- python dictionary containing your gradients, output of L_model_backward    :param lr: learning rate    :return:        parameters -- python dictionary containing your updated parameters                       parameters["W" + str(l)] = ...                       parameters["b" + str(l)] = ...    """    L = len(parameters) // 2    for layer in range(1, L+1):        parameters['W'+str(layer)] = parameters['W'+str(layer)] - lr * grads['dW' + str(layer)]        parameters['b'+str(layer)] = parameters['b'+str(layer)] - lr * grads['db' + str(layer)]    return parameters<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, grads = func_update_parameters_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, grads<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>({&#39;W1&#39;: array([[-0.41675785, -0.05626683, -2.1361961 ,  1.64027081],         [-1.79343559, -0.84174737,  0.50288142, -1.24528809],         [-1.05795222, -0.90900761,  0.55145404,  2.29220801]]),  &#39;b1&#39;: array([[ 0.04153939],         [-1.11792545],         [ 0.53905832]]),  &#39;W2&#39;: array([[-0.5961597 , -0.0191305 ,  1.17500122]]),  &#39;b2&#39;: array([[-0.74787095]])}, {&#39;dW1&#39;: array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],         [-0.2773882 , -0.35475898, -0.08274148, -0.62700068],         [-0.04381817, -0.47721803, -1.31386475,  0.88462238]]),  &#39;db1&#39;: array([[0.88131804],         [1.70957306],         [0.05003364]]),  &#39;dW2&#39;: array([[-0.40467741, -0.54535995, -1.54647732]]),  &#39;db2&#39;: array([[0.98236743]])})</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_update_parameters(parameters, grads, 0.1)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>{&#39;W1&#39;: array([[-0.59562069, -0.09991781, -2.14584584,  1.82662008],        [-1.76569676, -0.80627147,  0.51115557, -1.18258802],        [-1.0535704 , -0.86128581,  0.68284052,  2.20374577]]), &#39;b1&#39;: array([[-0.04659241],        [-1.28888275],        [ 0.53405496]]), &#39;W2&#39;: array([[-0.55569196,  0.0354055 ,  1.32964895]]), &#39;b2&#39;: array([[-0.84610769]])}</code></pre><h3 id="7-5-预测"><a href="#7-5-预测" class="headerlink" title="7.5-预测"></a>7.5-预测</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_predict(X, y, parameters):    """    This function is used to predict the results of a  L-layer neural network.    :param X -- data set of examples you would like to label    :param parameters -- parameters of the trained model    :return:        p -- predictions for the given dataset X    """    m = X.shape[1]    L = len(parameters) // 2    p = np.zeros((1, m))    probas, caches = func_L_model_forward(X, parameters)    for i in range(probas.shape[1]):        if probas[0, i] > 0.5:            p[0, i] = 1        else:            p[0, i] = 0    print('acc: {}'.format(np.sum(p==y)/m))    return p<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="8-整合到model"><a href="#8-整合到model" class="headerlink" title="8-整合到model"></a>8-整合到model</h2><h3 id="8-1-two-layer-neural-network"><a href="#8-1-two-layer-neural-network" class="headerlink" title="8.1-two layer neural network"></a>8.1-two layer neural network</h3><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/2layerNN_kiank.png" style="width:650px;height:400px;"></p><caption><center> <u>Figure 2</u>: 2-layer neural network. <br> The model can be summarized as: ***INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT***. </center></caption><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_two_layer_nn_model(X, Y, layer_dims, lr=0.001, num_epochs=10000, print_cost=False):    """    双层神经网络模型    :param X:    :param Y:    :param layer_dims: python array (list) containing the dimensions of each layer in our network    :param lr: learning rate    :param num_epochs:    :param print_cost:    :return params    """    np.random.seed(1)    grads = {}    costs = []    m = X.shape[1]    (n_x, n_h, n_y) = layer_dims    # 参数初始化    parameters = func_2_layers_initialize_parameters(n_x, n_h, n_y)    W1 = parameters['W1']    b1 = parameters['b1']    W2 = parameters['W2']    b2 = parameters['b2']    # loop    for epoch in range(num_epochs):        # 前向传播        A1, cache1 = func_linear_activation_forward(X, W1, b1, 'relu')        A2, cache2 = func_linear_activation_forward(A1, W2, b2, 'sigmoid')        # 计算损失        cost = func_compute_cost(A2, Y)        # 后向传播        dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))        dA1, dW2, db2 = func_linear_activation_backward(dA2, cache2, 'sigmoid')        dA0, dW1, db1 = func_linear_activation_backward(dA1, cache1, 'relu')        grads['dW1'] = dW1        grads['db1'] = db1        grads['dW2'] = dW2        grads['db2'] = db2        # 更新参数        parameters = func_update_parameters(parameters, grads, lr)        W1 = parameters['W1']        b1 = parameters['b1']        W2 = parameters['W2']        b2 = parameters['b2']        # 打印信息        if print_cost and epoch % 100 == 0:            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))        if epoch % 100 == 0:            costs.append(cost)    return parameters, costs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="8-2-L-layer-deep-neural-network"><a href="#8-2-L-layer-deep-neural-network" class="headerlink" title="8.2 L layer deep neural network"></a>8.2 L layer deep neural network</h3><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/LlayerNN_kiank.png" style="width:650px;height:400px;"></p><caption><center> <u>Figure 3</u>: L-layer neural network. <br> The model can be summarized as: ***[LINEAR -> RELU] $\times$ (L-1) -> LINEAR -> SIGMOID***</center></caption><p><u>Detailed Architecture of figure 3</u>:</p><ul><li>The input is a (64,64,3) image which is flattened to a vector of size (12288,1).</li><li>The corresponding vector: $[x_0,x_1,…,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[1]}$. The result is called the linear unit.</li><li>Next, you take the relu of the linear unit. This process could be repeated several times for each $(W^{[l]}, b^{[l]})$ depending on the model architecture.</li><li>Finally, you take the sigmoid of the final linear unit. If it is greater than 0.5, you classify it to be a cat.</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_L_layer_dnn_model(X, Y, layer_dims, lr=0.001, num_epochs=10000, print_cost=False):    """    多层神经网络模型    :param X:    :param Y:    :param layer_dims: python array (list) containing the dimensions of each layer in our network    :param lr: learning rate    :param num_epochs:    :param print_cost:    :return params    """    np.random.seed(1)    costs = []#     m = X.shape[1]    # 参数初始化    parameters = func_L_layers_initialize_parameters(layer_dims)    # loop    for epoch in range(num_epochs):        # 前向传播        AL, caches = func_L_model_forward(X, parameters)        # 计算损失        cost = func_compute_cost(AL, Y)        # 后向传播        grads = func_L_model_backward(AL, Y, caches)        # 更新参数        parameters = func_update_parameters(parameters, grads, lr)        # 打印信息        if print_cost and epoch % 100 == 0:            print('cost after epoch {}: {}'.format(epoch, np.squeeze(cost)))        if epoch % 100 == 0:            costs.append(cost)    return parameters, costs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="9-应用-图片分类"><a href="#9-应用-图片分类" class="headerlink" title="9-应用=图片分类"></a>9-应用=图片分类</h2><ul><li>You will use use the functions you’d implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification.</li></ul><h3 id="9-1-一些函数"><a href="#9-1-一些函数" class="headerlink" title="9.1-一些函数"></a>9.1-一些函数</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_load_dataset():    """    from lr_utils import load_dataset    :return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes    """    train_dataset = h5py.File('./深度学习之吴恩达课程作业1/train_catvnoncat.h5', "r")    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels    test_dataset = h5py.File('./深度学习之吴恩达课程作业1/test_catvnoncat.h5', "r")    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels    classes = np.array(test_dataset["list_classes"][:]) # the list of classes    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_print_mislabeled_images(classes, X, y, p):    """    Plots images where predictions and truth were different.    :param X -- dataset    :param y -- true labels    :param p -- predictions    """    a = p + y    mislabeled_indices = np.asarray(np.where(a == 1))    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots    num_images = len(mislabeled_indices[0])    for i in range(num_images):        index = mislabeled_indices[1][i]        plt.subplot(2, num_images, i + 1)        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')        plt.axis('off')        plt.title("Prediction: " + classes[int(p[0,index])].decode("utf-8") + " \n Class: " + classes[y[0,index]].decode("utf-8"))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="9-2-导入数据集"><a href="#9-2-导入数据集" class="headerlink" title="9.2-导入数据集"></a>9.2-导入数据集</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = func_load_dataset()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(train_set_x_orig.shape)print(test_set_x_orig.shape)print(train_set_y_orig.shape)print(test_set_y_orig.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(209, 64, 64, 3)(50, 64, 64, 3)(1, 209)(1, 50)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">index = 10plt.imshow(train_set_x_orig[index])print('y=' + str(train_set_y_orig[:, index]) + ' it is a ' + classes[np.squeeze(train_set_y_orig[:, index])].decode('utf-8'))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>y=[0] it is a non-cat</code></pre><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/output_92_1.png" alt="output_92_1"></p><ul><li>重塑数据集，将大小(n, length, height, 3)的重塑为(length*height*3, n)</li><li>如下图</li></ul><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/imvectorkiank.png" alt="imvector"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).Ttest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).Tprint(train_set_x_flatten.shape)print(test_set_x_flatten.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(12288, 209)(12288, 50)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_flatten[:5, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([17, 31, 56, 22, 33], dtype=uint8)</code></pre><ul><li>数据预处理</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x = train_set_x_flatten / 255.test_set_x = test_set_x_flatten / 255.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x[:5, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([0.06666667, 0.12156863, 0.21960784, 0.08627451, 0.12941176])</code></pre><h3 id="9-3-使用two-layer-neural-network"><a href="#9-3-使用two-layer-neural-network" class="headerlink" title="9.3-使用two layer neural network"></a>9.3-使用two layer neural network</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">n_x, n_h, n_y = train_set_x.shape[0], 7, 1layer_dims = (n_x, n_h, n_y)print(layer_dims)lr = 0.0075num_epochs = 2500print_cost=True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(12288, 7, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%timeparameters, costs = func_two_layer_nn_model(train_set_x, train_set_y_orig, layer_dims, lr, num_epochs, print_cost)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>cost after epoch 0: 0.693049735659989cost after epoch 100: 0.6464320953428849cost after epoch 200: 0.6325140647912678cost after epoch 300: 0.6015024920354665cost after epoch 400: 0.5601966311605748cost after epoch 500: 0.5158304772764731cost after epoch 600: 0.47549013139433266cost after epoch 700: 0.433916315122575cost after epoch 800: 0.40079775362038844cost after epoch 900: 0.3580705011323798cost after epoch 1000: 0.3394281538366413cost after epoch 1100: 0.30527536361962654cost after epoch 1200: 0.2749137728213015cost after epoch 1300: 0.24681768210614827cost after epoch 1400: 0.1985073503746611cost after epoch 1500: 0.17448318112556638cost after epoch 1600: 0.17080762978096792cost after epoch 1700: 0.11306524562164719cost after epoch 1800: 0.09629426845937153cost after epoch 1900: 0.08342617959726863cost after epoch 2000: 0.07439078704319084cost after epoch 2100: 0.06630748132267933cost after epoch 2200: 0.05919329501038171cost after epoch 2300: 0.053361403485605585cost after epoch 2400: 0.04855478562877018CPU times: user 19min 11s, sys: 13min 2s, total: 32min 14sWall time: 34.9 s</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot the costplt.plot(np.squeeze(costs))plt.ylabel('cost')plt.xlabel('iterations (per tens)')plt.title("Learning rate =" + str(lr))plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/output_102_0.png" alt="output_102_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_train = func_predict(train_set_x, train_set_y_orig, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>acc: 1.0</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_test = func_predict(test_set_x, test_set_y_orig, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>acc: 0.72</code></pre><h3 id="9-4-使用L层深度神经网络"><a href="#9-4-使用L层深度神经网络" class="headerlink" title="9.4-使用L层深度神经网络"></a>9.4-使用L层深度神经网络</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">layer_dims = [train_set_x.shape[0], 7, 1]print(layer_dims)lr = 0.0075num_epochs = 2500print_cost=True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[12288, 7, 1]</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%timeparameters, costs = func_L_layer_dnn_model(train_set_x, train_set_y_orig, layer_dims, lr, num_epochs, print_cost)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>cost after epoch 0: 0.6950464961800915cost after epoch 100: 0.5892596054583805cost after epoch 200: 0.5232609173622991cost after epoch 300: 0.4497686396221906cost after epoch 400: 0.42090021618838985cost after epoch 500: 0.3724640306174595cost after epoch 600: 0.34742051870201895cost after epoch 700: 0.3171919198737027cost after epoch 800: 0.26643774347746585cost after epoch 900: 0.21991432807842595cost after epoch 1000: 0.14357898893623774cost after epoch 1100: 0.45309212623221046cost after epoch 1200: 0.09499357670093511cost after epoch 1300: 0.08014128076781372cost after epoch 1400: 0.06940234005536465cost after epoch 1500: 0.06021664023174592cost after epoch 1600: 0.05327415758001879cost after epoch 1700: 0.04762903262098435cost after epoch 1800: 0.04297588879436871cost after epoch 1900: 0.039036074365138215cost after epoch 2000: 0.03568313638049029cost after epoch 2100: 0.03291526373054677cost after epoch 2200: 0.030472193059120623cost after epoch 2300: 0.028387859212946124cost after epoch 2400: 0.026615212372776063CPU times: user 18min 20s, sys: 11min 49s, total: 30min 10sWall time: 32.4 s</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot the costplt.plot(np.squeeze(costs))plt.ylabel('cost')plt.xlabel('iterations (per tens)')plt.title("Learning rate =" + str(lr))plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/output_108_0.png" alt="output_108_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_train = func_predict(train_set_x, train_set_y_orig, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>acc: 1.0</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_test = func_predict(test_set_x, test_set_y_orig, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>acc: 0.74</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">layer_dims = [train_set_x.shape[0], 20, 7, 5, 1]print(layer_dims)lr = 0.0075num_epochs = 2500print_cost=True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[12288, 20, 7, 5, 1]</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%timeparameters, costs = func_L_layer_dnn_model(train_set_x, train_set_y_orig, layer_dims, lr, num_epochs, print_cost)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>cost after epoch 0: 0.7717493284237686cost after epoch 100: 0.6720534400822914cost after epoch 200: 0.6482632048575212cost after epoch 300: 0.6115068816101354cost after epoch 400: 0.5670473268366111cost after epoch 500: 0.5401376634547801cost after epoch 600: 0.5279299569455267cost after epoch 700: 0.46547737717668514cost after epoch 800: 0.36912585249592794cost after epoch 900: 0.39174697434805344cost after epoch 1000: 0.3151869888600617cost after epoch 1100: 0.2726998441789385cost after epoch 1200: 0.23741853400268137cost after epoch 1300: 0.19960120532208644cost after epoch 1400: 0.18926300388463305cost after epoch 1500: 0.16118854665827748cost after epoch 1600: 0.14821389662363316cost after epoch 1700: 0.13777487812972944cost after epoch 1800: 0.1297401754919012cost after epoch 1900: 0.12122535068005211cost after epoch 2000: 0.1138206066863371cost after epoch 2100: 0.10783928526254132cost after epoch 2200: 0.10285466069352679cost after epoch 2300: 0.10089745445261786cost after epoch 2400: 0.09287821526472395CPU times: user 28min 23s, sys: 17min 29s, total: 45min 53sWall time: 49.3 s</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot the costplt.plot(np.squeeze(costs))plt.ylabel('cost')plt.xlabel('iterations (per tens)')plt.title("Learning rate =" + str(lr))plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/output_113_0.png" alt="output_113_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_train = func_predict(train_set_x, train_set_y_orig, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>acc: 0.9856459330143541</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_test = func_predict(test_set_x, test_set_y_orig, parameters)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>acc: 0.8</code></pre><h3 id="9-5-结果分析"><a href="#9-5-结果分析" class="headerlink" title="9.5-结果分析"></a>9.5-结果分析</h3><ul><li>让我们看看一些被错误分类的图片</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_print_mislabeled_images(classes, test_set_x, test_set_y_orig, pred_test)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/output_117_0.png" alt="output_117_0"></p><p><strong>一些模型在识别上犯错误的地方</strong></p><ul><li>Cat body in an unusual position</li><li>Cat appears against a background of a similar color</li><li>Unusual cat color and species</li><li>Camera Angle</li><li>Brightness of the picture</li><li>Scale variation (cat is very large or small in image) </li></ul><h3 id="9-6-识别自己的图片"><a href="#9-6-识别自己的图片" class="headerlink" title="9.6-识别自己的图片"></a>9.6-识别自己的图片</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">image_path = './深度学习之吴恩达课程作业1/cat_in_iran.jpg'my_label_y = [1]image = np.array(plt.imread(image_path))image.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(1115, 1114, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">num_px = train_set_x_orig.shape[1]my_image = np.array(Image.fromarray(image).resize((num_px, num_px))).reshape((1, num_px*num_px*3)).Tmy_image.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(12288, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">my_pred_image = func_predict(my_image, my_label_y, parameters)my_pred_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>acc: 1.0array([[1.]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.figure(figsize=(9, 9))plt.imshow(image)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&lt;matplotlib.image.AxesImage at 0x7f02f60edeb0&gt;</code></pre><p><img src="/2021/09/03/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-3/output_123_1.png" alt="output_123_1"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">print('y_pred=', np.squeeze(my_pred_image), ", your algorithm predicts a \"" + classes[int(np.squeeze(my_pred_image)),].decode("utf-8") +  "\" picture.")<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>y_pred= 1.0 , your algorithm predicts a &quot;cat&quot; picture.</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之吴恩达课程作业2</title>
      <link href="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/"/>
      <url>/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/</url>
      
        <content type="html"><![CDATA[<h1 id="吴恩达深度学习课程作业L1W3一层隐藏层神经网络"><a href="#吴恩达深度学习课程作业L1W3一层隐藏层神经网络" class="headerlink" title="吴恩达深度学习课程作业L1W3一层隐藏层神经网络"></a>吴恩达深度学习课程作业L1W3一层隐藏层神经网络</h1><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li><li><a href="https://www.heywhale.com/mw/project/5dd3946900b0b900365f3a48" target="_blank" rel="noopener">作业链接</a></li></ol><h2 id="本节目的"><a href="#本节目的" class="headerlink" title="本节目的"></a>本节目的</h2><ol><li>建立一个含一层隐藏层的神经网络，观察其和逻辑回归实现之间的差异</li><li>实现单个隐藏层的2分类神经网络</li><li>使用具有非线性激活函数的神经元，例如tanh</li><li>计算交叉熵损失</li><li>实现前向和后向传播</li></ol><h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1-导入模块"></a>1-导入模块</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport sklearnimport sklearn.datasetsimport sklearn.linear_model%matplotlib inlinenp.random.seed(2030)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-1-planar-utils-py"><a href="#1-1-planar-utils-py" class="headerlink" title="1.1-planar_utils.py"></a>1.1-planar_utils.py</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_plot_decision_boundary(model, X, y):    """    plot decision boundary    :param model:     :param X:    :param y:    :return    """    # Set min and max values and give it some padding    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1    h = 0.01    # Generate a grid of points with distance h between them    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))    # Predict the function value for the whole grid    Z = model(np.c_[xx.ravel(), yy.ravel()])    Z = Z.reshape(xx.shape)    # Plot the contour and training examples    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)    plt.ylabel('x2')    plt.xlabel('x1')    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)def func_sigmoid(x):    """    Compute the sigmoid of x    Arguments:    x -- A scalar or numpy array of any size.    Return:    s -- sigmoid(x)    """    s = 1/(1+np.exp(-x))    return sdef func_load_planar_dataset():    """    load_planar_dataset    """    np.random.seed(2030)    m = 400 # number of examples    N = int(m/2) # number of points per class    D = 2 # dimensionality    X = np.zeros((m,D)) # data matrix where each row is a single example    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)    a = 4 # maximum ray of the flower    for j in range(2):        ix = range(N*j,N*(j+1))        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]        Y[ix] = j    X = X.T    Y = Y.T    return X, Ydef func_load_extra_datasets():    """    load_extra_datasets    """    N = 200    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-2-testCases-py"><a href="#1-2-testCases-py" class="headerlink" title="1.2-testCases.py"></a>1.2-testCases.py</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_layer_sizes_test_case():    """    layer sizes test case    """    np.random.seed(2030)    X_assess = np.random.randn(5, 3)    Y_assess = np.random.randn(2, 3)    return X_assess, Y_assessdef func_initialize_parameters_test_case():    """    initialize_parameters_test_case    """    n_x, n_h, n_y = 2, 4, 1    return n_x, n_h, n_ydef func_forward_propagation_test_case():    """    forward_propagation_test_case    """    np.random.seed(2030)    X_assess = np.random.randn(2, 3)    parameters = {        'W1': np.array([[-0.00416758, -0.00056267],                        [-0.02136196,  0.01640271],                        [-0.01793436, -0.00841747],                        [ 0.00502881, -0.01245288]]),        'W2': np.array([[-0.01057952, -0.00909008,                           0.00551454,  0.02292208]]),        'b1': np.array([[ 0.], [ 0.], [ 0.], [ 0.]]),        'b2': np.array([[ 0.]])}    return X_assess, parametersdef func_compute_cost_test_case():    """    compute_cost_test_case    """    np.random.seed(2030)    Y_assess = np.random.randn(1, 3)    parameters = {        'W1': np.array([[-0.00416758, -0.00056267],                        [-0.02136196,  0.01640271],                        [-0.01793436, -0.00841747],                        [ 0.00502881, -0.01245288]]),        'W2': np.array([[-0.01057952, -0.00909008,                           0.00551454,  0.02292208]]),        'b1': np.array([[ 0.], [ 0.], [ 0.], [ 0.]]),        'b2': np.array([[ 0.]])}    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))    return a2, Y_assess, parametersdef func_backward_propagation_test_case():    """    backward_propagation_test_case    """    np.random.seed(2030)    X_assess = np.random.randn(2, 3)    Y_assess = np.random.randn(1, 3)    parameters = {        'W1': np.array([[-0.00416758, -0.00056267],                        [-0.02136196,  0.01640271],                        [-0.01793436, -0.00841747],                        [ 0.00502881, -0.01245288]]),        'W2': np.array([[-0.01057952, -0.00909008,                       0.00551454,  0.02292208]]),        'b1': np.array([[ 0.], [ 0.], [ 0.], [ 0.]]),        'b2': np.array([[ 0.]])}    cache = {        'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],                        [-0.05225116,  0.02725659, -0.02646251],                        [-0.02009721,  0.0036869 ,  0.02883756],                        [ 0.02152675, -0.01385234,  0.02599885]]),        'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),        'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],                        [-0.05229879,  0.02726335, -0.02646869],                        [-0.02009991,  0.00368692,  0.02884556],                        [ 0.02153007, -0.01385322,  0.02600471]]),        'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}    return parameters, cache, X_assess, Y_assessdef func_update_parameters_test_case():    """    update_parameters_test_case    """    parameters = {        'W1': np.array([[-0.00615039,  0.0169021 ],                        [-0.02311792,  0.03137121],                        [-0.0169217 , -0.01752545],                        [ 0.00935436, -0.05018221]]),        'W2': np.array([[-0.0104319 , -0.04019007,                           0.01607211,  0.04440255]]),        'b1': np.array([[ -8.97523455e-07],                        [  8.15562092e-06],                        [  6.04810633e-07],                        [ -2.54560700e-06]]),        'b2': np.array([[  9.14954378e-05]])}    grads = {        'dW1': np.array([[ 0.00023322, -0.00205423],                         [ 0.00082222, -0.00700776],                         [-0.00031831,  0.0028636 ],                         [-0.00092857,  0.00809933]]),        'dW2': np.array([[ -1.75740039e-05, 3.70231337e-03,                            -1.25683095e-03, -2.55715317e-03]]),        'db1': np.array([[  1.05570087e-07],                         [ -3.81814487e-06],                         [ -1.90155145e-07],                         [  5.46467802e-07]]),        'db2': np.array([[ -1.08923140e-05]])    }    return parameters, gradsdef func_nn_model_test_case():    """    nn_model_test_case    """    np.random.seed(2030)    X_assess = np.random.randn(2, 3)    Y_assess = np.random.randn(1, 3)    return X_assess, Y_assessdef func_predict_test_case():    """    predict_test_case    """    np.random.seed(2030)    X_assess = np.random.randn(2, 3)    parameters = {            'W1': np.array([[-0.00615039,  0.0169021 ],                            [-0.02311792,  0.03137121],                            [-0.0169217 , -0.01752545],                            [ 0.00935436, -0.05018221]]),            'W2': np.array([[-0.0104319 , -0.04019007,                               0.01607211,  0.04440255]]),            'b1': np.array([[ -8.97523455e-07],                            [  8.15562092e-06],                            [  6.04810633e-07],                            [ -2.54560700e-06]]),            'b2': np.array([[  9.14954378e-05]])}    return parameters, X_assess<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-导入数据集"><a href="#2-导入数据集" class="headerlink" title="2-导入数据集"></a>2-导入数据集</h2><ol><li>将<code>flower</code>2分类数据加载到变量X,Y中</li><li>使用<code>matplotlib</code>可视化数据集</li><li>深入了解数据</li></ol><h3 id="2-1-加载"><a href="#2-1-加载" class="headerlink" title="2.1-加载"></a>2.1-加载</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python"># loadX, Y = func_load_planar_dataset()print(X.shape, Y.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(2, 400) (1, 400)</code></pre><h3 id="2-2-可视化"><a href="#2-2-可视化" class="headerlink" title="2.2-可视化"></a>2.2-可视化</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plot# X[0, :].shape  (400,)plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0, :].shape), s=40, cmap=plt.cm.Spectral)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>&lt;matplotlib.collections.PathCollection at 0x7f58a99ab670&gt;</code></pre><p><img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/output_11_1.png" alt="output_11_1"></p><h3 id="2-3-深入了解"><a href="#2-3-深入了解" class="headerlink" title="2.3-深入了解"></a>2.3-深入了解</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">shape_X = X.shapeshape_Y = Y.shapem = shape_X[1]print('the shape of X is: ', shape_X)print('the shape of Y is: ', shape_Y)print('m=%d training examples' % m)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>the shape of X is:  (2, 400)the shape of Y is:  (1, 400)m=400 training examples</code></pre><h2 id="3-使用简单logistic-regression对数据集进行二分类"><a href="#3-使用简单logistic-regression对数据集进行二分类" class="headerlink" title="3-使用简单logistic regression对数据集进行二分类"></a>3-使用简单logistic regression对数据集进行二分类</h2><ol><li>使用sklearn内置函数执行</li><li>画出决策边界</li></ol><h3 id="3-1-构建"><a href="#3-1-构建" class="headerlink" title="3.1-构建"></a>3.1-构建</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">clf = sklearn.linear_model.LogisticRegressionCV()clf.fit(X.T, Y.ravel())# Y.ravel()  # Return a flattened array.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>LogisticRegressionCV()</code></pre><h3 id="3-2-决策边界"><a href="#3-2-决策边界" class="headerlink" title="3.2-决策边界"></a>3.2-决策边界</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_plot_decision_boundary(lambda x: clf.predict(x), X, Y)plt.title('sklearn logistic regression')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>Text(0.5, 1.0, &#39;sklearn logistic regression&#39;)</code></pre><p><img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/output_18_1.png" alt="output_18_1"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">pred_lr = clf.predict(X.T)print('acc: ', float((np.dot(Y, pred_lr)) + np.dot(1-Y, 1-pred_lr)) / float(Y.size)*100)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>acc:  49.25</code></pre><ul><li>正确率只有49.25%， 由于数据集不是线性可分类的，因此逻辑回归效果不佳</li><li>接下来试试DNN</li></ul><h2 id="4-DNN结构实现"><a href="#4-DNN结构实现" class="headerlink" title="4-DNN结构实现"></a>4-DNN结构实现</h2><h3 id="4-1-理论介绍"><a href="#4-1-理论介绍" class="headerlink" title="4.1-理论介绍"></a>4.1-理论介绍</h3><ol><li><p>模型结构</p><p> <img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/q17ipqoyrg.png" alt="模型结构"></p></li><li><p>数学原理</p><p> 输入$x^{(i)}$</p><script type="math/tex; mode=display"> z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}  \\ a^{[1] (i)} = \tanh(z^{[1] (i)})  \\ z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}  \\ \hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})  \\ y^{(i)}_{prediction} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5 \\ 0 & \mbox{otherwise } \end{cases}  \\ loss = J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small</script></li><li><p>建立神经网络的一般方法</p><ol><li>定义神经网络结构，输入参数，隐藏单元参数等</li><li>初始化模型参数</li><li>循环<ol><li>实现前向传播</li><li>计算损失</li><li>后向传播以获得梯度</li><li>更新参数（梯度下降）</li></ol></li><li>预测</li></ol></li></ol><h3 id="4-2-定义神经网络结构"><a href="#4-2-定义神经网络结构" class="headerlink" title="4.2-定义神经网络结构"></a>4.2-定义神经网络结构</h3><ul><li>定义三个变量：<ol><li>n_x: 输入层的大小</li><li>n_h: 隐藏层的大小</li><li>n_y: 输出层的大小</li></ol></li><li>提示：<ol><li>使用shape来找到n_x和n_y</li><li>将隐藏层大小硬编码成4</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_layer_sizes(X, Y):    """    定义每层参数大小    :param X    :param Y    :return (n_x, n_h, n_y)    """    n_x = X.shape[0]    n_h = 4    n_y = Y.shape[0]    return (n_x, n_h, n_y)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">X_assess, Y_assess = func_layer_sizes_test_case()(n_x, n_h, n_y) = func_layer_sizes(X_assess, Y_assess)print(n_x, n_h, n_y)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>5 4 2</code></pre><h3 id="4-3-初始化模型参数"><a href="#4-3-初始化模型参数" class="headerlink" title="4.3-初始化模型参数"></a>4.3-初始化模型参数</h3><ul><li>练习<ol><li>实现函数<code>func_initialize_parameters()</code></li></ol></li><li>说明<ol><li>确保参数大小正确</li><li>使用随机初始化权重矩阵<ol><li>使用<code>np.random.randn(a, b)*0.01</code>随机初始化维度为(a,b)的矩阵</li></ol></li><li>将偏差向量初始化为0<ol><li>使用<code>np.zeros((a,b))</code>初始化维度为(a,b)的矩阵</li></ol></li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_initialize_parameters(n_x, n_h, n_y):    """    随机初始化模型参数    :param n_x: size of input layer    :param n_h: size of hidden layer    :param n_y: size of output layer    :return params    """    np.random.seed(2030)    W1 = np.random.randn(n_h, n_x) * 0.01    b1 = np.zeros((n_h, 1))    W2 = np.random.randn(n_y, n_h) * 0.01    b2 = np.zeros((n_y, 1))    assert (W1.shape == (n_h, n_x))    assert (b1.shape == (n_h, 1))    assert (W2.shape == (n_y, n_h))    assert (b2.shape == (n_y, 1))    params = {        'W1': W1,        'b1': b1,        'W2': W2,        'b2': b2    }    return params<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">params = func_initialize_parameters(n_x, n_h, n_y)for key, value in params.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>W1 [[ 0.0140356  -0.01123472  0.00181204  0.00371685  0.00331453] [ 0.00616523 -0.00719542  0.00392973 -0.01472587  0.01631223] [ 0.00700217 -0.00210701  0.00322235  0.0085826   0.01209621] [-0.01813631 -0.01153197  0.00429659 -0.00105094  0.00331243]](4, 5)b1 [[0.] [0.] [0.] [0.]](4, 1)W2 [[ 0.00530549  0.00229905 -0.00579193 -0.00166288] [-0.00867708  0.00637114 -0.00827742 -0.00441255]](2, 4)b2 [[0.] [0.]](2, 1)</code></pre><h3 id="4-4-循环"><a href="#4-4-循环" class="headerlink" title="4.4-循环"></a>4.4-循环</h3><ul><li>练习<ol><li>实现<code>func_forward_propagation()</code></li></ol></li><li>说明<ol><li>查看分类器的数学表达式</li><li>可以使用<code>func_sigmoid()</code>函数</li><li>可以使用<code>np.tanh()</code>函数</li><li>必须执行以下步骤<ol><li>使用<code>params[&#39;&#39;]</code>从中检索出每个参数</li><li>实现正向传播，计算$Z^{[1]}, A^{[1]}, Z^{[2]}$和$A^{[2]}$（所有训练数据的预测结果向量）</li></ol></li><li>向后传播所需的值存储在<code>cache</code>中，作为反向传播函数的输入</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_forward_propagation(X, parameters):    """    前向传播    :param X    :param parameters    return A2, cache    """    W1 = parameters['W1']    b1 = parameters['b1']    W2 = parameters['W2']    b2 = parameters['b2']    Z1 = np.dot(W1, X) + b1    A1 = np.tanh(Z1)    Z2 = np.dot(W2, A1) + b2    A2 = func_sigmoid(Z2)    assert (A2.shape == (1, X.shape[1]))    cache = {        'Z1': Z1,        'A1': A1,        'Z2': Z2,        'A2': A2    }    return A2, cache<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">X_assess, params = func_forward_propagation_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">X_assess.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(2, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">params<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>{&#39;W1&#39;: array([[-0.00416758, -0.00056267],        [-0.02136196,  0.01640271],        [-0.01793436, -0.00841747],        [ 0.00502881, -0.01245288]]), &#39;W2&#39;: array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]), &#39;b1&#39;: array([[0.],        [0.],        [0.],        [0.]]), &#39;b2&#39;: array([[0.]])}</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in params.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>W1 [[-0.00416758 -0.00056267] [-0.02136196  0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]](4, 2)W2 [[-0.01057952 -0.00909008  0.00551454  0.02292208]](1, 4)b1 [[0.] [0.] [0.] [0.]](4, 1)b2 [[0.]](1, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A2, cache = func_forward_propagation(X_assess, params)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A2.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(1, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in cache.items():    print(key, value)    print('shape', value.shape)    print('mean:', np.mean(value))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Z1 [[-0.00605859  0.00449566 -0.00110208] [-0.02388616  0.02943629  0.00624179] [-0.0283006   0.01735875 -0.00843933] [ 0.00242969 -0.00977727 -0.00676625]]shape (4, 3)mean: -0.002030674846044073A1 [[-0.00605851  0.00449563 -0.00110208] [-0.02388161  0.02942779  0.00624171] [-0.02829305  0.01735701 -0.00843913] [ 0.00242968 -0.00977696 -0.00676615]]shape (4, 3)mean: -0.0020304726884621785Z2 [[ 0.00018085 -0.00044345 -0.00024671]]shape (1, 3)mean: -0.00016977098916522824A2 [[0.50004521 0.49988914 0.49993832]]shape (1, 3)mean: 0.4999575572533775</code></pre><ul><li>练习：<ol><li>实现<code>func_compute_cost()</code>函数</li></ol></li><li>说明：<ol><li>计算公式<script type="math/tex; mode=display"> J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small</script></li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_compute_cost(A2, Y):    """    计算DNN损失函数    :param A2: the sigmoid output of the second activation, of shape (1, num_samples)    :param Y: 'true' labels vector of shape (1, num_samples)    return cost: cost value    """    m = Y.shape[1]  # num_samples    log_probs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)    cost = -1 / m * np.sum(log_probs)    cost = np.squeeze(cost)  # eg. turns [[17]] into 17    assert(isinstance(cost, float))    return cost<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A2, Y_assess, params = func_compute_cost_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">A2.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(1, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">Y_assess.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(1, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in params.items():    print(key, value)    print('shape', value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>W1 [[-0.00416758 -0.00056267] [-0.02136196  0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]shape (4, 2)W2 [[-0.01057952 -0.00909008  0.00551454  0.02292208]]shape (1, 4)b1 [[0.] [0.] [0.] [0.]]shape (4, 1)b2 [[0.]]shape (1, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print('cost=', func_compute_cost(A2, Y_assess))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>cost= 0.6926644838406011</code></pre><ul><li>练习<ol><li>实现函数<code>func_backward_propagation()</code></li></ol></li><li>说明<ol><li>反向传播通常是DNN中最难（最数学）的部分，详情见下图<br> <img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/q17hcd4yra.png" alt="反向传播过程"></li><li>由于激活函数是<code>tanh</code>，所以$g^{[1]’}(z) = 1-a^2$</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_backward_propagation(parameters, cache, X, Y):    """    DNN反向传播    :param parameters:     :param cache: Z1, A1, ...    :param X:    :param Y:    :return grads:    """    m = X.shape[1]    W1 = parameters['W1']    b1 = parameters['b1']    W2 = parameters['W2']    b2 = parameters['b2']    A1 = cache['A1']    A2 = cache['A2']    dZ2 = A2 - Y    dW2 = 1 / m * np.dot(dZ2, A1.T)    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))    dW1 = 1 / m * np.dot(dZ1, X.T)    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)    grads = {        'dW2': dW2,        'db2': db2,        'dW1': dW1,        'db1': db1    }    return grads<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">params, cache, X_assess, Y_assess = func_backward_propagation_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in params.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>W1 [[-0.00416758 -0.00056267] [-0.02136196  0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]](4, 2)W2 [[-0.01057952 -0.00909008  0.00551454  0.02292208]](1, 4)b1 [[0.] [0.] [0.] [0.]](4, 1)b2 [[0.]](1, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in cache.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>A1 [[-0.00616578  0.0020626   0.00349619] [-0.05225116  0.02725659 -0.02646251] [-0.02009721  0.0036869   0.02883756] [ 0.02152675 -0.01385234  0.02599885]](4, 3)A2 [[0.5002307  0.49985831 0.50023963]](1, 3)Z1 [[-0.00616586  0.0020626   0.0034962 ] [-0.05229879  0.02726335 -0.02646869] [-0.02009991  0.00368692  0.02884556] [ 0.02153007 -0.01385322  0.02600471]](4, 3)Z2 [[ 0.00092281 -0.00056678  0.00095853]](1, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">grads = func_backward_propagation(params, cache, X_assess, Y_assess)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in grads.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>dW2 [[-0.00013434 -0.03767573  0.01092386  0.02535612]](1, 4)db2 [[1.09982824]](1, 1)dW1 [[-0.00687442 -0.0060129 ] [-0.00589216 -0.00516006] [ 0.00358158  0.00313207] [ 0.01488723  0.01302015]](4, 2)db1 [[-0.0116354 ] [-0.00998301] [ 0.00606112] [ 0.02519569]](4, 1)</code></pre><ul><li>练习<ol><li>实现参数更新。使用梯度下降。规则为：$\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$</li></ol></li><li>提示<ol><li>具有良好的学习速率（收敛）图和较差的学习速率（发散）的图如下<br> <img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/q17hh4otzu.gif" alt="良好"><br> <img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/q17hharbth.gif" alt="较差"></li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_update_parameters(parameters, grads, lr=0.5):    """    用梯度下降法求解参数    :param parameters:    :param grads:     :return params:    """    W1 = parameters['W1']    b1 = parameters['b1']    W2 = parameters['W2']    b2 = parameters['b2']    dW1 = grads['dW1']    db1 = grads['db1']    dW2 = grads['dW2']    db2 = grads['db2']    W1 = W1 - lr * dW1    b1 = b1 - lr * db1    W2 = W2 - lr * dW2    b2 = b2 - lr * db2    params = {        'W1': W1,        'b1': b1,        'W2': W2,        'b2': b2    }    return params<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, grads = func_update_parameters_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_update_parameters(parameters, grads)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in parameters.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>W1 [[-0.006267    0.01792921] [-0.02352903  0.03487509] [-0.01676255 -0.01895725] [ 0.00981865 -0.05423187]](4, 2)b1 [[-9.50308498e-07] [ 1.00646934e-05] [ 6.99888206e-07] [-2.81884090e-06]](4, 1)W2 [[-0.01042311 -0.04204123  0.01670053  0.04568113]](1, 4)b2 [[9.69415948e-05]](1, 1)</code></pre><h3 id="4-5-集成4-2-4-4"><a href="#4-5-集成4-2-4-4" class="headerlink" title="4.5-集成4.2-4.4"></a>4.5-集成4.2-4.4</h3><ul><li>练习<ol><li>实现<code>func_nn_model()</code></li></ol></li><li>说明<ol><li>DNN必须以正确的顺序组合先前构建的函数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_nn_model(X, Y, n_h, lr, num_epochs=10000, print_cost=False):    """    DNN模型结构    :param X:    :param Y:    :param n_h: size of hidden layer    :param lr: learning rate    :param num_epochs:    :param print_cost:    :return params    """    np.random.seed(2030)    n_x, _, n_y = func_layer_sizes(X, Y)    parameters = func_initialize_parameters(n_x, n_h, n_y)    for i in range(num_epochs):        A2, cache = func_forward_propagation(X, parameters)        cost = func_compute_cost(A2, Y)        grads = func_backward_propagation(parameters, cache, X, Y)        parameters = func_update_parameters(parameters, grads, lr)        if print_cost and i % 1000 == 0:            print('cost after iter %i: %f' % (i, cost))    return parameters<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">X_assess, Y_assess = func_nn_model_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_nn_model(X_assess, Y_assess, n_h=4, print_cost=True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>cost after iter 0: 0.693205cost after iter 1000: -infcost after iter 2000: -infcost after iter 3000: -infcost after iter 4000: -inf/tmp/ipykernel_33337/3831275435.py:35: RuntimeWarning: overflow encountered in exp  s = 1/(1+np.exp(-x))/tmp/ipykernel_33337/2034926829.py:10: RuntimeWarning: divide by zero encountered in log  log_probs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)/home/meiyunhe/softwares/miniconda3/envs/python38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)cost after iter 5000: -infcost after iter 6000: -infcost after iter 7000: -infcost after iter 8000: -infcost after iter 9000: -inf</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for key, value in parameters.items():    print(key, value)    print(value.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>W1 [[  11.52632597    4.53170427] [ -11.52483312   -4.53805397] [   3.7056901    -5.59975822] [-135.67556482   43.1988336 ]](4, 2)b1 [[  3.76204789] [ -3.75833722] [-11.97631404] [127.6183328 ]](4, 1)W2 [[-4101.12484223  4100.86581429  3195.42022681 -1806.0368121 ]](1, 4)b2 [[-3201.6699135]](1, 1)</code></pre><h3 id="4-6-预测"><a href="#4-6-预测" class="headerlink" title="4.6-预测"></a>4.6-预测</h3><ul><li>练习<ol><li>实现<code>func_predict()</code></li></ol></li><li>提示<ol><li>预测公式为：<script type="math/tex; mode=display"> y_{prediction} = \mathbb 1 \textfalse = \begin{cases}   1 & \text{if}\ activation > 0.5 \\   0 & \text{otherwise}   \end{cases}</script></li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_predict(parameters, X):    """    预测    :param parameters:    :param X:    :return preds    """    A2, _ = func_forward_propagation(X, parameters)    preds = np.round(A2)    return preds<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters, X_assess = func_predict_test_case()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">preds = func_predict(parameters, X_assess)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">preds<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[1., 0., 0.]])</code></pre><h2 id="5-使用DNN对数据集进行二分类"><a href="#5-使用DNN对数据集进行二分类" class="headerlink" title="5-使用DNN对数据集进行二分类"></a>5-使用DNN对数据集进行二分类</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">X.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(2, 400)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">Y.shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>(1, 400)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">parameters = func_nn_model(X, Y, n_h=4, num_epochs=10000, print_cost=True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>cost after iter 0: 0.693208cost after iter 1000: 0.255412cost after iter 2000: 0.237878cost after iter 3000: 0.230251cost after iter 4000: 0.225817cost after iter 5000: 0.222741cost after iter 6000: 0.220396cost after iter 7000: 0.218504cost after iter 8000: 0.216923cost after iter 9000: 0.215573</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_plot_decision_boundary(lambda x: func_predict(parameters, x.T), X, Y)plt.title('decision boundary for hidden layer size %i' % 4)plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/output_70_0.png" alt="output_70_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">preds = func_predict(parameters, X)print('acc: ', (np.dot(Y, preds.T)+np.dot(1-Y, (1-preds).T))/float(Y.size)*100)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>acc:  [[91.25]]</code></pre><ul><li>与简单逻辑回归相比，DNN准确性更高，它学习了flower的叶子图案，与逻辑回归不同，DNN能够学习非线性的决策边界</li></ul><h3 id="4-7-调整隐藏层的大小"><a href="#4-7-调整隐藏层的大小" class="headerlink" title="4.7-调整隐藏层的大小"></a>4.7-调整隐藏层的大小</h3><ul><li>观察不同大小隐藏层的模型表现</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.figure(figsize=(16, 32))hidden_layer_sizes = [1,2,3,4,5,10,20]for i, n_h in enumerate(hidden_layer_sizes):    plt.subplot(5, 2, i+1)    parameters = func_nn_model(X, Y, n_h, num_epochs=5000)    func_plot_decision_boundary(lambda x: func_predict(parameters, x.T), X, Y)    preds = func_predict(parameters, X)    acc = float((np.dot(Y,preds.T) + np.dot(1-Y,1-preds.T))/float(Y.size)*100)#     print ("Accuracy for {} hidden units: {} %".format(n_h, acc))    plt.title("Accuracy for {} hidden units: {} %".format(n_h, acc))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/output_74_0.png" alt="output_74_0"></p><ul><li>较大的模型能够更好的拟合训练集</li><li>隐藏层units数量也不是越多越好</li><li>可以借助正则化，避免模型过拟合</li><li>正则化方法：<ol><li>l2正则</li><li>dropout</li><li>数据增强</li><li>early stopping</li><li></li></ol></li></ul><h2 id="6-模型在其他数据集上的性能"><a href="#6-模型在其他数据集上的性能" class="headerlink" title="6-模型在其他数据集上的性能"></a>6-模型在其他数据集上的性能</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_train_predict_extra_dataset():    """    在其他数据集上的表现    :return    """    nosiy_circles, nosiy_moons, blobs, gaussian_quantiles, no_structure = func_load_extra_datasets()    datasets = {        'nosiy_circles': nosiy_circles,        'nosiy_moons': nosiy_moons,        'blobs': blobs,        'gaussian_quantiles': gaussian_quantiles    }    plt.figure(figsize=(16, 16))    for i, dataset in enumerate(datasets.keys()):        plt.subplot(2, 2, i+1)        X, Y = datasets[dataset]        X, Y = X.T, np.expand_dims(Y, axis=0)        if dataset == 'blobs':            Y = Y % 2#         plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0, :].shape), s=40, cmap=plt.cm.Spectral)        parameters = func_nn_model(X, Y, n_h=4, num_epochs=10000, print_cost=False)        func_plot_decision_boundary(lambda x: func_predict(parameters, x.T), X, Y)        plt.title('dataset %s decision boundary for hidden layer size %i' % (dataset, 4))    plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_train_predict_extra_dataset()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2021/08/11/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-2/output_78_0.png" alt="output_78_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python"><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之吴恩达课程作业1</title>
      <link href="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/"/>
      <url>/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/</url>
      
        <content type="html"><![CDATA[<h1 id="吴恩达深度学习课程作业L1W2HW1-numpy入门"><a href="#吴恩达深度学习课程作业L1W2HW1-numpy入门" class="headerlink" title="吴恩达深度学习课程作业L1W2HW1=numpy入门"></a>吴恩达深度学习课程作业L1W2HW1=numpy入门</h1><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li><li><a href="https://www.heywhale.com/mw/project/5dd236a800b0b900365eca9b" target="_blank" rel="noopener">作业链接</a></li></ol><h2 id="Numpy基础"><a href="#Numpy基础" class="headerlink" title="Numpy基础"></a>Numpy基础</h2><h3 id="1-使用numpy构建基本函数"><a href="#1-使用numpy构建基本函数" class="headerlink" title="1-使用numpy构建基本函数"></a>1-使用numpy构建基本函数</h3><h4 id="1-1-sigmoid函数和np-exp"><a href="#1-1-sigmoid函数和np-exp" class="headerlink" title="1-1 sigmoid函数和np.exp()"></a>1-1 sigmoid函数和np.exp()</h4><ul><li>sigmoid函数公式</li></ul><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+exp(-x)}</script><ul><li>为什么np.exp()比math.exp()更可取？<ol><li>深度学习中主要使用的是矩阵和向量，因此numpy更为实用</li><li>math.exp()只适用于输入是实数</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">import mathdef func_basic_sigmoid(x):    """    使用math.exp构建sigmoid函数    :param x: a scaler    :return sigmoid(x)    """    return 1 / (1 + math.exp(-x))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_basic_sigmoid(3)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>0.9525741268224334</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># func_basic_sigmoid([1,2,3])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([1,2,3])np.exp(x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([ 2.71828183,  7.3890561 , 20.08553692])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">import numpy as npdef func_numpy_sigmoid(x):    """    使用np.exp构建sigmoid函数    :param x: a scaler or numpy array of any size    :return sigmoid(x)    """    return 1 / (1 + np.exp(-x))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">x = np.array([1,2,3])func_numpy_sigmoid(x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([0.73105858, 0.88079708, 0.95257413])</code></pre><h4 id="1-2-simoid函数的梯度"><a href="#1-2-simoid函数的梯度" class="headerlink" title="1-2 simoid函数的梯度"></a>1-2 simoid函数的梯度</h4><ul><li>梯度求解公式</li></ul><script type="math/tex; mode=display">\sigma^{'}(x) = \sigma(x)(1-\sigma(x))</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_numpy_sigmoid_derivative(x):    """    计算sigmoid函数在x处的梯度    :param x: a scaler or numpy array of any size    :return 梯度    """    s = func_numpy_sigmoid(x)    return s * (1 - s)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">x = np.array([0,1,2,3,100])func_numpy_sigmoid_derivative(x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([0.25      , 0.19661193, 0.10499359, 0.04517666, 0.        ])</code></pre><h4 id="1-3重塑数组"><a href="#1-3重塑数组" class="headerlink" title="1-3重塑数组"></a>1-3重塑数组</h4><ul><li>np.shape: 获取矩阵或者向量的维度</li><li>np.reshape(): 将矩阵或者向量重塑成其他shape</li><li>例如实现image2vector()，将输入(length, height, depth)的3D数组输入，转换成(length<em>height</em>depth, 1)的1D数组</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_image2vector(image):    """    图像输入转换成vector    :param image: a numpy array of shape (length, height, depth)    :return v: a vector of shape (length*height*depth, 1)    """    return image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">image = np.array([[[ 0.67826139,  0.29380381],                    [ 0.90714982,  0.52835647],                    [ 0.4215251 ,  0.45017551]],                   [[ 0.92814219,  0.96677647],                    [ 0.85304703,  0.52351845],                    [ 0.19981397,  0.27417313]],                   [[ 0.60659855,  0.00533165],                    [ 0.10820313,  0.49978937],                    [ 0.34144279,  0.94630077]]])image.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(3, 3, 2)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">v = func_image2vector(image)v.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>(18, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">v<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[0.67826139],       [0.29380381],       [0.90714982],       [0.52835647],       [0.4215251 ],       [0.45017551],       [0.92814219],       [0.96677647],       [0.85304703],       [0.52351845],       [0.19981397],       [0.27417313],       [0.60659855],       [0.00533165],       [0.10820313],       [0.49978937],       [0.34144279],       [0.94630077]])</code></pre><h4 id="1-4-行标准化"><a href="#1-4-行标准化" class="headerlink" title="1-4 行标准化"></a>1-4 行标准化</h4><ul><li>为什么标准化？<ol><li>数据归一化后，梯度下降的收敛速度更快，通常会表现出更好的效果</li></ol></li><li>练习：构建normalize_rows()函数来标准化矩阵的行</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_normalize_rows(x):    """    行标准化    :param: x: a numpy matrix of shape (n, m)    :return x: 行标准化后的矩阵    """    x_norm = np.linalg.norm(x, axis=1, ord=2, keepdims=True)  # Matrix or vector norm    print(x_norm, x_norm.shape)    return x / x_norm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">x = np.array([[0,3,4],               [1,6,4]])x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>array([[0, 3, 4],       [1, 6, 4]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_normalize_rows(x)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>[[5.        ] [7.28010989]] (2, 1)array([[0.        , 0.6       , 0.8       ],       [0.13736056, 0.82416338, 0.54944226]])</code></pre><h4 id="1-5-broadcast-and-softmax"><a href="#1-5-broadcast-and-softmax" class="headerlink" title="1-5 broadcast and softmax"></a>1-5 broadcast and softmax</h4><ul><li>broadcast: 广播，数组+标量，会将标量自动broadcast成和数组一样维度再相加</li><li>softmax:</li></ul><script type="math/tex; mode=display">softmax(x) = \frac{exp(x_i)}{\sum{exp(x_j)}}</script><ul><li>练习：实现softmax函数</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_softmax_rows(x):    """    softmax函数， 按行    :param x: a matrix of shape (n, m)    :return x: softmax过后的矩阵    """    x_exp = np.exp(x)    x_row_sum = np.sum(x_exp, axis=1, keepdims=True)    return x_exp/ x_row_sum<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">x = np.array([    [9, 2, 5, 0, 0],    [7, 5, 0, 0 ,0]])x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([[9, 2, 5, 0, 0],       [7, 5, 0, 0, 0]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_softmax_rows(x)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04,        1.21052389e-04],       [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04,        8.01252314e-04]])</code></pre><h3 id="2-向量化"><a href="#2-向量化" class="headerlink" title="2-向量化"></a>2-向量化</h3><ul><li>为了确保代码的高效计算，我们将使用向量化</li><li>例如：尝试区分点，外部，元素乘积之间的区别</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">import timex1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>经典点乘-dot product</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()dot = 0for i in range(len(x1)):    dot += x1[i]*x2[i]toc = time.process_time()print(dot, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>278 0.2324579999997134</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()dot = np.dot(x1, x2)print(dot, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>278 -10.069867000000343</code></pre><ul><li>经典外乘-outer product</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()outer = np.zeros((len(x1), len(x2)))for i in range(len(x1)):    for j in range(len(x2)):        outer[i, j] = x1[i] * x2[j]toc = time.process_time()print(outer, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.] [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.] [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.] [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.] [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.] [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]] 0.4756360000000015</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()outer = np.outer(x1, x2)print(outer, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>[[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0] [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0] [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0] [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0] [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0] [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]] -12.807576000000154</code></pre><ul><li>元素相乘-elementwise product</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()mul = np.zeros(len(x1))for i in range(len(x1)):    mul[i] = x1[i] * x2[i]toc = time.process_time()print(mul, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.] 0.2550199999999947</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()mul = np.multiply(x1, x2)print(mul, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>[81  4 10  0  0 63 10  0  0  0 81  4 25  0  0] -9.051946999999672</code></pre><ul><li>矩阵相乘-general dot product</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">np.random.seed(2030)W = np.random.rand(3, len(x1))tic = time.process_time()gdot = np.zeros(W.shape[0])for i in range(W.shape[0]):    for j in range(len(x1)):        gdot[i] += W[i, j]*x1[j]toc = time.process_time()print(gdot, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[23.51513471 26.17443232 12.03612207] 0.3430280000000785</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">tic = time.process_time()gdot = np.dot(W, x1)print(gdot, (toc-tic)*1000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>[23.51513471 26.17443232 12.03612207] -9.034191000000025</code></pre><h4 id="2-1-实现l1和l2损失函数"><a href="#2-1-实现l1和l2损失函数" class="headerlink" title="2-1 实现l1和l2损失函数"></a>2-1 实现l1和l2损失函数</h4><ul><li>向量化版本</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_l1_l2(y_hat, y):    """    l1损失函数    :param y_hat: 预测值    :param y: 实际值    return l1_loss, l2_loss:    """    return np.sum(np.abs(y-y_hat)), np.dot((y-y_hat), (y-y_hat).T)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_hat = np.array([.9, 0.2, 0.1, .4, .9])y = np.array([1, 0, 0, 1, 1])func_l1_l2(y_hat, y)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(1.1, 0.43)</code></pre><h1 id="吴恩达深度学习课程作业L1W2HW2-神经网络实现逻辑回归"><a href="#吴恩达深度学习课程作业L1W2HW2-神经网络实现逻辑回归" class="headerlink" title="吴恩达深度学习课程作业L1W2HW2=神经网络实现逻辑回归"></a>吴恩达深度学习课程作业L1W2HW2=神经网络实现逻辑回归</h1><h2 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">视频链接</a></li><li><a href="https://www.heywhale.com/mw/project/5dd23dbf00b0b900365ecef1" target="_blank" rel="noopener">作业链接</a></li></ol><h2 id="神经网络实现逻辑回归"><a href="#神经网络实现逻辑回归" class="headerlink" title="神经网络实现逻辑回归"></a>神经网络实现逻辑回归</h2><ul><li>建立学习算法的一般步骤<ol><li>初始化参数</li><li>计算损失函数及梯度</li><li>使用优化算法（梯度下降）</li></ol></li></ul><h3 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1-导入模块"></a>1-导入模块</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">import numpy as np  # python科学计算的基本包import matplotlib.pyplot as plt  # python图形库import h5py  # 处理存储为h5文件格式的数据集import scipy  # python算法库和数学工具包from PIL import Image  # 图像处理库from scipy import ndimage  # 图像处理# from lr_utils import load_dataset%matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_load_dataset():    """    from lr_utils import load_dataset    :return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes    """    train_dataset = h5py.File('./L1W2/train_catvnoncat.h5', "r")    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels    test_dataset = h5py.File('./L1W2/test_catvnoncat.h5', "r")    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels    classes = np.array(test_dataset["list_classes"][:]) # the list of classes    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-问题描述"><a href="#2-问题描述" class="headerlink" title="2-问题描述"></a>2-问题描述</h3><ul><li><p>问题说明：</p><ol><li>label为cat(y=1)或非cat(y=0)的图像集</li><li>图像维度为(length, height, 3)</li><li>构建一个简单的图像识别算法，对图片进行分类</li></ol></li><li><p>数据EDA</p><ol><li>深度学习中许多报错都来自于矩阵/向量尺寸不匹配，如果可以保持他们不变，则可以消除很多错误</li><li>机器学习常见的数据预处理是对数据集进行居中和标准化，即减去均值除以标准差。但是图片数据集只需要将每一行除以255，效果差不多</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = func_load_dataset()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(train_set_x_orig.shape)print(test_set_x_orig.shape)print(train_set_y_orig.shape)print(test_set_y_orig.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(209, 64, 64, 3)(50, 64, 64, 3)(1, 209)(1, 50)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">index = 5plt.imshow(train_set_x_orig[index])print('y=' + str(train_set_y_orig[:, index]) + ' it is a ' + classes[np.squeeze(train_set_y_orig[:, index])].decode('utf-8'))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>y=[0] it is a non-cat</code></pre><p><img src="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/output_49_1.png" alt="output_49_1"></p><ul><li>重塑数据集，将大小(n, length, height, 3)的重塑为(length*height*3, n)</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).Ttest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).Tprint(train_set_x_flatten.shape)print(test_set_x_flatten.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(12288, 209)(12288, 50)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x_flatten[:5, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([17, 31, 56, 22, 33], dtype=uint8)</code></pre><ul><li>数据预处理</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x = train_set_x_flatten / 255.test_set_x = test_set_x_flatten / 255.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set_x[:5, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([0.06666667, 0.12156863, 0.21960784, 0.08627451, 0.12941176])</code></pre><h3 id="3-学习算法的一般架构"><a href="#3-学习算法的一般架构" class="headerlink" title="3-学习算法的一般架构"></a>3-学习算法的一般架构</h3><ul><li>逻辑回归实际是一个简单的神经网络，如下图</li></ul><p><img src="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/LogReg_kiank.png" alt="LogReg_kiank"></p><ul><li>算法的数学表达式</li></ul><script type="math/tex; mode=display">z^{(i)} = w^Tx^{(i)}+b \\\hat{y}^{(i)} = a^{(i)} = \sigma(z^{(i)})  \\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})</script><ul><li>关键步骤<ol><li>初始化模型参数</li><li>最小化损失函数来学习参数</li><li>使用学习到的参数进行预测（在测试集上）</li><li>分析结果并得出结论</li></ol></li></ul><h3 id="4-构建算法的各个部分"><a href="#4-构建算法的各个部分" class="headerlink" title="4-构建算法的各个部分"></a>4-构建算法的各个部分</h3><ul><li>构建神经网络主要步骤：<ol><li>定义模型架构</li><li>初始化模型参数</li><li>循环：<ol><li>计算当前损失（正向传播）</li><li>计算当前梯度（反向传播）</li><li>更新参数（梯度下降）</li></ol></li></ol></li></ul><h4 id="4-1-辅助函数-sigmoid"><a href="#4-1-辅助函数-sigmoid" class="headerlink" title="4-1 辅助函数=sigmoid"></a>4-1 辅助函数=sigmoid</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_numpy_sigmoid(x):    """    sigmoid函数    :param x: a scalar or numpy array of any size    :return s: sigmoid(x)    """    return 1 / (1 + np.exp(-x))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">func_numpy_sigmoid(np.array([0, 2]))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([0.5       , 0.88079708])</code></pre><h4 id="4-2-初始化参数"><a href="#4-2-初始化参数" class="headerlink" title="4-2 初始化参数"></a>4-2 初始化参数</h4><ul><li>将w初始化为0的向量</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_initial_parameters(dim):    """    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.    :param dim: w参数的维度    :return w, b    """    w = np.zeros((dim, 1))    b = 0    assert(w.shape==(dim, 1))    assert(isinstance(b, float) or isinstance(b, int))    return w, b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">w, b = func_initial_parameters(2)print(w, b)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>[[0.] [0.]] 0</code></pre><h4 id="4-3-前向，后向传播"><a href="#4-3-前向，后向传播" class="headerlink" title="4-3 前向，后向传播"></a>4-3 前向，后向传播</h4><ul><li>前向传播：</li></ul><script type="math/tex; mode=display">A = \sigma(W^TX+b)=(a^{(0)},...,a^{(m)})  \\J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})</script><ul><li>后向传播</li></ul><script type="math/tex; mode=display">\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T  \\\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_propagate(w, b, X, Y):    """    前向后向传播    :param w: weights of size(length*height*3, 1)    :param b: bias    :param X: data of size(length*height*3, num_samples)    :param Y: target of size(1, num_samples)    """    m = X.shape[1]    # forward    A = func_numpy_sigmoid(np.dot(w.T, X) + b)#     print('A: \n', A, '\n', A.shape)#     J = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))    J = -1 / m * np.sum(np.multiply(Y, np.log(A)) + np.multiply((1 - Y), np.log(1 - A)))#     print('J: \n', J, '\n', J.shape)    # backward    dw = 1 / m * np.dot(X, (A-Y).T)    db = 1 / m * np.sum(A-Y)    assert(dw.shape == w.shape)    assert(db.dtype == float)    J = np.squeeze(J)    assert(J.shape == ())    grads = {        'dw': dw,        'db': db    }    return grads, J<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])print(w.shape, b, X.shape, Y.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>(2, 1) 2 (2, 2) (1, 2)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">grads, cost = func_propagate(w, b, X, Y)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print ("dw = " + str(grads["dw"]))print ("db = " + str(grads["db"]))print ("cost = " + str(cost))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>dw = [[0.99993216] [1.99980262]]db = 0.49993523062470574cost = 6.000064773192205</code></pre><h4 id="4-4-优化函数"><a href="#4-4-优化函数" class="headerlink" title="4-4 优化函数"></a>4-4 优化函数</h4><ul><li>最小化J来学习w,b</li><li>参数的更新规则：</li></ul><script type="math/tex; mode=display">\theta = \theta - \alpha * d\theta</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_optimizer(w, b, X, Y, epochs, lr, print_cost=False):    """    优化函数，求解w,b    :param w: (length*height*3, 1)    :param b:    :param X: (length*height*3, num_samples)    :param Y: (1, num_samples)    :param epochs: num_iterations    :param lr: learning_rate    :param print_cost: True to print the loss every 100 steps    :return params, grads, costs    """    costs = []    for i in range(epochs):        grads, cost = func_propagate(w, b, X, Y)        dw = grads['dw']        db = grads['db']        w = w - lr * dw        b = b - lr * db        if i % 100 == 0:            costs.append(cost)            if print_cost:                print('cost after epoch %i: %f' % (i, cost))    params = {        'w': w,        'b': b    }    grads = {        'dw': dw,        'db': db    }    return params, grads, costs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])print(w.shape, b, X.shape, Y.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>(2, 1) 2 (2, 2) (1, 2)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">params, grads, costs = func_optimizer(w, b, X, Y, epochs=100, lr=0.009, print_cost=False)print('w=', params['w'])print('b=', params['b'])print('dw=', grads['dw'])print('db=', grads['db'])print(costs)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>w= [[0.1124579 ] [0.23106775]]b= 1.5593049248448891dw= [[0.90158428] [1.76250842]]db= 0.4304620716786828[6.000064773192205]</code></pre><ul><li>predict</li><li>预测公式</li></ul><script type="math/tex; mode=display">\hat{Y} = A = \sigma(w^TX + b)</script><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_predict(w, b, X):    """    predict    :param w: (length*height*3, 1)    :param b:    :param X: (length*height*3, num_samples)    :return y_pred    """    m = X.shape[1]#     y_pred = np.zeros((1, m))    w = w.reshape(X.shape[0], 1)    A = func_numpy_sigmoid(np.dot(w.T, X) + b)  # predict    y_pred = [0 if A[0, i] <= 0.5 else 1 for i in range(A.shape[1])]    y_pred = np.expand_dims(y_pred, 0)    assert(y_pred.shape==(1, m))    return y_pred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_pred = func_predict(w, b, X)print('y_pred=', y_pred)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>y_pred= [[1 1]]</code></pre><h4 id="5-将所有功能合并到模型中"><a href="#5-将所有功能合并到模型中" class="headerlink" title="5-将所有功能合并到模型中"></a>5-将所有功能合并到模型中</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def func_model(x_train, y_train, x_test, y_test, epochs=2000, lr=0.5, print_cost=False):    """    构建logistic regression    :param x_train:    :param y_train:    :param x_test:    :param y_test:    :param epochs:    :param lr:    :param print_cost:    :return d: dictionary containing information about the model.    """    w, b = func_initial_parameters(x_train.shape[0])    params, grads, costs = func_optimizer(w, b, x_train, y_train, epochs, lr, print_cost=print_cost)    w = params['w']    b = params['b']    y_pred_train = func_predict(w, b, x_train)    y_pred_test = func_predict(w, b, x_test)    print('train acc: {} %'.format(100-np.mean(np.abs(y_pred_train - y_train))*100))    print('test acc: {} %'.format(100-np.mean(np.abs(y_pred_test - y_test))*100))    d = {        'costs': costs,        'y_pred_train': y_pred_train,        'y_pred_test': y_pred_test,        'w': w,        'b': b,        'lr': lr,        'epochs': epochs    }    return d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">d = func_model(train_set_x, train_set_y_orig, test_set_x, test_set_y_orig, epochs=2000, lr=0.005, print_cost=True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>cost after epoch 0: 0.693147cost after epoch 100: 0.584508cost after epoch 200: 0.466949cost after epoch 300: 0.376007cost after epoch 400: 0.331463cost after epoch 500: 0.303273cost after epoch 600: 0.279880cost after epoch 700: 0.260042cost after epoch 800: 0.242941cost after epoch 900: 0.228004cost after epoch 1000: 0.214820cost after epoch 1100: 0.203078cost after epoch 1200: 0.192544cost after epoch 1300: 0.183033cost after epoch 1400: 0.174399cost after epoch 1500: 0.166521cost after epoch 1600: 0.159305cost after epoch 1700: 0.152667cost after epoch 1800: 0.146542cost after epoch 1900: 0.140872train acc: 99.04306220095694 %test acc: 70.0 %</code></pre><ul><li>结果分析：<ol><li>训练acc接近100%，测试acc为70%</li><li>该模型明显适合训练数据，有些过拟合</li></ol></li><li>使用下面代码可以查看预测结果</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">index = 1plt.imshow(test_set_x[:, index].reshape(test_set_x_orig.shape[1], test_set_x_orig.shape[1], 3))print('y=', test_set_y_orig[0, index], 'pred=', d['y_pred_test'][0, index])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>y= 1 pred= 1</code></pre><p><img src="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/output_80_1.png" alt="output_80_1"></p><ul><li>绘制损失函数和梯度</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">costs = np.array(d['costs'])plt.plot(costs)plt.ylabel('cost')plt.xlabel('epochs')plt.title('lr='+str(d['lr']))plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/output_82_0.png" alt="output_82_0"></p><h4 id="6-学习率的选择"><a href="#6-学习率的选择" class="headerlink" title="6-学习率的选择"></a>6-学习率的选择</h4><ul><li>lr决定我们更新参数的速度</li><li>如果太大，可能会超出最佳值</li><li>如果太小，可能需要更多的迭代才能收敛到最佳值</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">lr_list = [0.01, 0.001, 0.0001]models = {}for lr in lr_list:    print('lr=', lr)    models[str(lr)] = func_model(train_set_x, train_set_y_orig, test_set_x, test_set_y_orig,                                  epochs=1500, lr=lr, print_cost=False)    print('-'*52)for lr in lr_list:    plt.plot(np.array(models[str(lr)]['costs']), label=str(models[str(lr)]['lr']))plt.ylabel('cost')plt.xlabel('epochs')legend = plt.legend(loc='upper center', shadow=True)frame = legend.get_frame()frame.set_facecolor('0.90')plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>lr= 0.01train acc: 99.52153110047847 %test acc: 68.0 %----------------------------------------------------lr= 0.001train acc: 88.99521531100478 %test acc: 64.0 %----------------------------------------------------lr= 0.0001train acc: 68.42105263157895 %test acc: 36.0 %----------------------------------------------------</code></pre><p><img src="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/output_84_1.png" alt="output_84_1"></p><ul><li>分析<ol><li>不同的学习率带来不同损失</li><li>学习率太大，损失上下波动，甚至可能会发散</li><li>较低损失并不意味着模型效果好，当训练精度比测试精度高很多时，会发生过拟合情况</li></ol></li><li>深度学习中，通常建议<ol><li>选择好学习率</li><li>如果过拟合，则使用其他方法减少过拟合</li></ol></li></ul><h4 id="7-预测自己的图像"><a href="#7-预测自己的图像" class="headerlink" title="7-预测自己的图像"></a>7-预测自己的图像</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">image_path = './L1W2/cat_in_iran.jpg'image = np.array(plt.imread(image_path))image.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(1115, 1114, 3)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">num_px = train_set_x_orig.shape[1]my_image = np.array(Image.fromarray(image).resize((num_px, num_px))).reshape((1, num_px*num_px*3)).Tmy_image.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>(12288, 1)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">my_pred_image = func_predict(d['w'], d['b'], my_image)my_pred_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([[1]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.imshow(image)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&lt;matplotlib.image.AxesImage at 0x7f12aa284e20&gt;</code></pre><p><img src="/2021/08/08/shen-du-xue-xi-zhi-wu-en-da-ke-cheng-zuo-ye-1/output_90_1.png" alt="output_90_1"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">print('y_pred=', np.squeeze(my_pred_image), ", your algorithm predicts a \"" + classes[int(np.squeeze(my_pred_image)),].decode("utf-8") +  "\" picture.")<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>y_pred= 1 , your algorithm predicts a &quot;cat&quot; picture.</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> numpy </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>求职必备之各大官网</title>
      <link href="/2021/08/06/qiu-zhi-bi-bei-zhi-ge-da-guan-wang/"/>
      <url>/2021/08/06/qiu-zhi-bi-bei-zhi-ge-da-guan-wang/</url>
      
        <content type="html"><![CDATA[<h1 id="官网"><a href="#官网" class="headerlink" title="官网"></a>官网</h1><h2 id="国企"><a href="#国企" class="headerlink" title="国企"></a>国企</h2><ol><li>==<a href="https://job.10086.cn/personal/society/" target="_blank" rel="noopener">中国移动</a>==</li><li><a href="http://hr.gdtel.com.cn/portal/recruit/more/42" target="_blank" rel="noopener">中国电信</a></li><li><a href="http://www.chinaunicom.com/hr/socialrecruit.html" target="_blank" rel="noopener">中国联通</a></li><li>==<a href="https://www.gac.com.cn/cn/talent#join" target="_blank" rel="noopener">广州汽车集团股份有限公司</a>==</li><li>==<a href="https://yuexiu.hotjob.cn/wt/YUEXIU/web/index/social" target="_blank" rel="noopener">广州越秀集团有限公司</a>==</li><li><a href="http://www.gzpgroup.com/zpxx/shzp/" target="_blank" rel="noopener">广州港集团有限公司</a></li><li><a href="https://gzmpc.com/index.php/Human/index/id/14" target="_blank" rel="noopener">广州医药集团有限公司</a></li><li><a href="https://www.gz-gofar.com/index.php?ac=article&amp;at=list&amp;tid=59" target="_blank" rel="noopener">广州国资发展控股有限公司</a></li><li>==<a href="http://www.gdg.com.cn/cn/zhaopinxinxi/index.html" target="_blank" rel="noopener">广州发展集团股份有限公司</a>==</li><li><a href="http://www.gzmcg.com/information/job.html" target="_blank" rel="noopener">广州市建筑集团有限公司</a></li><li>==<a href="https://gzmetro.zhiye.com/social" target="_blank" rel="noopener">广州地铁集团有限公司</a>==</li><li><a href="http://www.gzci.net/catalog.aspx?id=20&amp;?id=6" target="_blank" rel="noopener">广州市城市建设投资集团有限公司</a></li><li>广州市水务投资集团有限公司</li><li><a href="http://www.gzjrkg.com/%e5%8a%a0%e5%85%a5%e6%88%91%e4%bb%ac/%e6%8b%9b%e8%81%98%e8%81%8c%e4%bd%8d" target="_blank" rel="noopener">广州金融控股集团有限公司</a></li><li>==<a href="http://sc.hotjob.cn/wt/GZCB/web/index/social" target="_blank" rel="noopener">广州银行股份有限公司</a>==</li><li><a href="http://grcbank.zhiye.com/social?k=&amp;d=-1&amp;c=-1&amp;p=3^-1,1^-1&amp;PageIndex=1" target="_blank" rel="noopener">广州农村商业银行股份有限公司</a></li><li><a href="https://www.gjtjt.cn/join.aspx" target="_blank" rel="noopener">广州交通投资集团有限公司</a></li><li><a href="http://www.gzfzs.com/news/28" target="_blank" rel="noopener">广州轻工工贸集团有限公司</a></li><li>广州工业投资控股集团有限公司</li><li>广州商贸投资控股集团有限公司</li><li>广州岭南国际企业集团有限公司</li><li>广州无线电集团有限公司</li><li>广州珠江实业集团有限公司</li><li>广州智能装备集团有限公司</li><li>广州珠江啤酒股份有限公司</li><li>广州珠江钢琴集团股份有限公司</li><li>广州酒家集团股份有限公司</li><li>广州环保投资集团有限公司</li><li>广州市公共交通集团有限公司</li><li>广州赛马娱乐总公司</li></ol>]]></content>
      
      
      <categories>
          
          <category> 求职必备 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 官网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之FM_FFM_DeepFM</title>
      <link href="/2021/07/02/ji-qi-xue-xi-zhi-fm-ffm-deepfm/"/>
      <url>/2021/07/02/ji-qi-xue-xi-zhi-fm-ffm-deepfm/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><ul><li>CTR-CVR预估任务上，常用的模型介绍</li></ul><h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><ul><li>Factorization machines</li></ul><h1 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h1><ul><li>Fields Factorization machines</li></ul><h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><h1 id="DeepFFM"><a href="#DeepFFM" class="headerlink" title="DeepFFM"></a>DeepFFM</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.hrwhisper.me/machine-learning-fm-ffm-deepfm-deepffm/" target="_blank" rel="noopener">hrwhisper.me的博客</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CTR预估 </tag>
            
            <tag> 广告推荐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资源整合之谷歌书签</title>
      <link href="/2021/04/23/zi-yuan-zheng-he-zhi-gu-ge-shu-qian/"/>
      <url>/2021/04/23/zi-yuan-zheng-he-zhi-gu-ge-shu-qian/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://msdn.itellyou.cn/" target="_blank" rel="noopener">msdn下载</a></li></ul><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><ol><li><a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a></li></ol><h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><ol><li><a href="https://colorsupplyyy.com/app" target="_blank" rel="noopener">PPT配色工具</a></li><li><a href="https://jsoneditoronline.org/#right=local.wajogi&amp;left=local.horoku" target="_blank" rel="noopener">json在线解析</a></li><li><a href="https://npm.taobao.org/mirrors/" target="_blank" rel="noopener">npm淘宝镜像</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 资源整合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 谷歌书签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之语音识别</title>
      <link href="/2021/03/23/shen-du-xue-xi-zhi-yu-yin-shi-bie/"/>
      <url>/2021/03/23/shen-du-xue-xi-zhi-yu-yin-shi-bie/</url>
      
        <content type="html"><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://github.com/nobody132/masr" target="_blank" rel="noopener">nobody132/masr: 中文语音识别; Mandarin Automatic … - GitHubgithub.com › nobody132 › masr</a></li><li><a href="https://github.com/nl8590687/ASRT_SpeechRecognition" target="_blank" rel="noopener">nl8590687/ASRT_SpeechRecognition: A Deep … - GitHubgithub.com › ASRT_SpeechRecognition</a></li><li><a href="https://github.com/audier/DeepSpeechRecognition" target="_blank" rel="noopener">audier/DeepSpeechRecognition: A Chinese Deep … - GitHubgithub.com › audier › DeepSpeechRecognition</a></li><li><a href="https://github.com/xxbb1234021/speech_recognition" target="_blank" rel="noopener">xxbb1234021/speech_recognition: 中文语音识别 - GitHubgithub.com › xxbb1234021 › speech_recognition</a></li><li><a href="https://www.pythonf.cn/read/138089" target="_blank" rel="noopener">从视频中提取音频——Python三行程序,的,python,搞定www.pythonf.cn › read</a></li><li><a href="https://zhuanlan.zhihu.com/p/32085405" target="_blank" rel="noopener">人人都能看懂的LSTM - 知乎zhuanlan.zhihu.com › …</a></li><li><a href="https://deepspeech.readthedocs.io/en/v0.9.3/?badge=latest" target="_blank" rel="noopener">deepspeech项目</a>：<code>star:16.9k</code></li><li>使用<code>google</code>接口进行语音识别：<a href="https://github.com/Uberi/speech_recognition" target="_blank" rel="noopener">speech_recognition</a></li><li>相关paper: <a href="https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers#Automatic-Speech-Recognition" target="_blank" rel="noopener">GitHub</a></li><li><a href="https://github.com/topics/speech-recognition?o=desc&amp;s=stars" target="_blank" rel="noopener">github speech recognition topics排行榜</a></li><li><a href="https://github.com/athena-team/athena" target="_blank" rel="noopener">滴滴语音识别开源</a></li><li><a href="https://github.com/wenet-e2e/wenet" target="_blank" rel="noopener">2021最新语音识别开源WENET</a></li></ol><h1 id="不OK的项目"><a href="#不OK的项目" class="headerlink" title="不OK的项目"></a>不OK的项目</h1><ol><li><a href="https://github.com/xxbb1234021/speech_recognition" target="_blank" rel="noopener">speech_recognition</a></li></ol><h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="视频提取音频"><a href="#视频提取音频" class="headerlink" title="视频提取音频"></a>视频提取音频</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python"># -*- coding:utf-8 _*-"""Author: Email: Date: 2021/03/23File: audio_test.pySoftware: PyCharmDescription: 音频识别"""# load modulesimport osimport sysfrom moviepy.editor import AudioFileClipcurrent_path = sys.path[0]video_path = '%s/video/' % current_path  # 视频源文件目录video_name = os.listdir(video_path)[0]audio_path = '%s/audio_extracts/' % current_path  # 音频提取文件目录if not os.path.exists(audio_path):    os.mkdir(audio_path)audio_name = video_name.split('.')[0] + '.wav'# 音频提取my_audio_clip = AudioFileClip(video_path+video_name)my_audio_clip.write_audiofile(audio_path+audio_name)# 音频分析import numpy as npimport librosaaudio, freq = librosa.load(audio_path+audio_name)time = np.arange(0, len(audio)) / freqprint(len(audio), type(audio), freq, sep="\t")# 画信号增强图import librosa.displayimport matplotlib.pyplot as pltaudio, _ = librosa.effects.trim(audio)  # Trim leading and trailing #silence from an audio signal.librosa.display.waveplot(audio, sr=freq)plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="音频识别"><a href="#音频识别" class="headerlink" title="音频识别"></a>音频识别</h2><h3 id="使用接口"><a href="#使用接口" class="headerlink" title="使用接口"></a>使用接口</h3><ul><li><a href="https://app.xunjiepdf.com/voice2text/" target="_blank" rel="noopener">迅捷PDF转换器</a>：免费，效果还行</li><li><a href="https://jianwai.youdao.com/index/0" target="_blank" rel="noopener">网易见外</a>：免费，效果一般</li></ul><p><a href="https://github.com/topics/speech-recognition" target="_blank" rel="noopener">speech-recognition</a>：音频识别相关topic</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之CNN</title>
      <link href="/2021/03/22/shen-du-xue-xi-zhi-cnn/"/>
      <url>/2021/03/22/shen-du-xue-xi-zhi-cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks" target="_blank" rel="noopener">神经网络与深度学习</a></li><li><a href="https://www.zhihu.com/question/52668301" target="_blank" rel="noopener">知乎</a></li></ol><h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之TensorFlow安装</title>
      <link href="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/"/>
      <url>/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210621153613138.png" alt="image-20210621153613138"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/35717544" target="_blank" rel="noopener">windows tensorflow-gpu的安装</a></li><li><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows" target="_blank" rel="noopener">官网</a></li></ul><h1 id="windows安装"><a href="#windows安装" class="headerlink" title="windows安装"></a>windows安装</h1><h2 id="先更新显卡驱动"><a href="#先更新显卡驱动" class="headerlink" title="先更新显卡驱动"></a>先更新显卡驱动</h2><ul><li><p>右键更新显卡驱动</p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320115932959.png" alt="image-20210320115932959"></p></li></ul><h2 id="查看显卡驱动对应cuda版本"><a href="#查看显卡驱动对应cuda版本" class="headerlink" title="查看显卡驱动对应cuda版本"></a>查看显卡驱动对应<code>cuda</code>版本</h2><ul><li><p>桌面右键显卡控制面板</p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320120246190.png" alt="image-20210320120246190"></p></li></ul><h2 id="下载安装cuda"><a href="#下载安装cuda" class="headerlink" title="下载安装cuda"></a>下载安装<code>cuda</code></h2><ul><li><p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">下载地址</a>：非常之慢</p></li><li><p><a href="https://developer.nvidia.com/zh-cn/cuda-downloads" target="_blank" rel="noopener">下载地址2</a></p></li><li><p><a href="https://blog.csdn.net/XunCiy/article/details/89070315" target="_blank" rel="noopener">安装参考地址</a></p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320153609006.png" alt="image-20210320153609006"></p></li><li><p>以下两个地方注意比选就行</p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320122925046.png" alt="image-20210320122925046"></p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320122936325.png" alt="image-20210320122936325"></p></li></ul><h2 id="查看cuda对应的cudnn版本"><a href="#查看cuda对应的cudnn版本" class="headerlink" title="查看cuda对应的cudnn版本"></a>查看<code>cuda</code>对应的<code>cudnn</code>版本</h2><ul><li><p><a href="https://www.tensorflow.org/install/source_windows#gpu" target="_blank" rel="noopener">地址</a></p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320121040837.png" alt="image-20210320121040837"></p></li></ul><h2 id="下载安装cudnn"><a href="#下载安装cudnn" class="headerlink" title="下载安装cudnn"></a>下载安装<code>cudnn</code></h2><ul><li><p><a href="https://developer.nvidia.com/zh-cn/cudnn" target="_blank" rel="noopener">下载地址</a>：需要登录</p></li><li><p><a href="https://developer.nvidia.cn/rdp/cudnn-download" target="_blank" rel="noopener">下载地址2</a></p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320162205564.png" alt="image-20210320162205564"></p></li><li><p>安装</p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320163021436.png" alt="image-20210320163021436"></p></li></ul><h2 id="安装tensorflow-gpu"><a href="#安装tensorflow-gpu" class="headerlink" title="安装tensorflow-gpu"></a>安装<code>tensorflow-gpu</code></h2><ul><li><p>根据<code>cuda</code>对应版本安装<code>tf</code>的对应版本，使用<code>pycharm</code>安装</p><p><img src="/2021/03/20/shen-du-xue-xi-zhi-tensorflow-an-zhuang/image-20210320163946401.png" alt="image-20210320163946401"></p></li></ul><h2 id="测试是否安装成功"><a href="#测试是否安装成功" class="headerlink" title="测试是否安装成功"></a>测试是否安装成功</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">import tensorflow as tfprint(tf.__version__)print('GPU', tf.test.is_gpu_available())# output# GPU [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>出现以下报错时：</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">2021-03-20 16:51:38.885475: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>将<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\vx.x\bin</code>目录下的<code>cusolver64_11.dll</code>复制，并将副本改名为<code>cusolver64_10.dll</code></p></li></ul><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><ol><li>显卡有对应的显卡驱动</li><li>显卡驱动对应<code>cuda</code>的版本</li><li>根据<code>cuda</code>的版本对应<code>cudnn</code>、<code>tf</code>版本</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之分类</title>
      <link href="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/"/>
      <url>/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/</url>
      
        <content type="html"><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="chrome-extension://cdonnmffkdaoajfknoeeecmchibpmkmg/assets/pdf/web/viewer.html?file=http%3A%2F%2Findex-of.es%2FVarios-2%2FHands%2520on%2520Machine%2520Learning%2520with%2520Scikit%2520Learn%2520and%2520Tensorflow.pdf" target="_blank" rel="noopener">hands on machine learning with scikit-learn and tensorflow</a></li><li><a href="https://github.com/ageron/handson-ml" target="_blank" rel="noopener">参考书代码</a></li><li><a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">参考书代码2</a></li></ul><h1 id="三、分类"><a href="#三、分类" class="headerlink" title="三、分类"></a>三、分类</h1><pre class="line-numbers language-lang-python"><code class="language-lang-python"># !jupyter labextension install @jupyterlab/toc# Python ≥3.5 is requiredimport sysassert sys.version_info >= (3, 5)# Scikit-Learn ≥0.20 is requiredimport sklearnassert sklearn.__version__ >= "0.20"# Common importsimport numpy as npimport os# to make this notebook's output stable across runsnp.random.seed(42)# To plot pretty figures%matplotlib inlineimport matplotlib as mplimport matplotlib.pyplot as pltmpl.rc('axes', labelsize=14)mpl.rc('xtick', labelsize=12)mpl.rc('ytick', labelsize=12)# Where to save the figuresPROJECT_ROOT_DIR = "."CHAPTER_ID = "classification"IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)os.makedirs(IMAGES_PATH, exist_ok=True)def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)    print("Saving figure", fig_id)    if tight_layout:        plt.tight_layout()    plt.savefig(path, format=fig_extension, dpi=resolution)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><h3 id="使用代码下载该数据集"><a href="#使用代码下载该数据集" class="headerlink" title="使用代码下载该数据集"></a>使用代码下载该数据集</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.datasets import fetch_openml# Load data from https://www.openml.org/d/554X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(X.shape)print(y.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>(70000, 784)(70000,)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">import matplotlib as mplimport matplotlib.pyplot as pltdef digit_show(some_digit):    some_digit_image = some_digit.reshape(28,28)    plt.imshow(some_digit_image, cmap=mpl.cm.binary, interpolation="nearest")    plt.axis("off")    plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">digit_show(X[0])print(y[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_6_0.png" alt="output_6_0"></p><pre><code>5</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y[0]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&#39;5&#39;</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y=y.astype(np.uint8)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y[0]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>5</code></pre><h3 id="划分训练集、测试集、将测试集放一边"><a href="#划分训练集、测试集、将测试集放一边" class="headerlink" title="划分训练集、测试集、将测试集放一边"></a>划分训练集、测试集、将测试集放一边</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python"># # 先shuffle# import random# combined = list(zip([1,2,3], [4,5,6]))# random.shuffle(combined)# list(zip(*combined))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[(3, 2, 1), (6, 5, 4)]</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 划分index = 60000X_train, X_test, y_train, y_test = X[:index], X[index:], y[:index], y[index:]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="训练一个二分类器"><a href="#训练一个二分类器" class="headerlink" title="训练一个二分类器"></a>训练一个二分类器</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_train_5 = (y_train == 5)y_test_5 = (y_test == 5)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.linear_model import SGDClassifiersgd_clf = SGDClassifier(random_state=42)sgd_clf.fit(X_train, y_train_5)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>SGDClassifier(random_state=42)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(sgd_clf.predict([X_train[0]]))print(y_train[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>[ True]5</code></pre><h3 id="perfomance-measures-评估表现"><a href="#perfomance-measures-评估表现" class="headerlink" title="perfomance measures:评估表现"></a>perfomance measures:评估表现</h3><ol><li>用CV来评估：cross_val_score()</li><li>confusion matrix:混淆矩阵<ul><li>precision and recall</li></ul></li></ol><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.model_selection import cross_val_scorecross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([0.95035, 0.96035, 0.9604 ])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 得看看是不是严重不平衡from sklearn.base import BaseEstimatorclass Never5Classifier(BaseEstimator):    def fit(self, X, y=None):        pass    def predict(self, X):        return np.zeros((len(X), 1), dtype=bool)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">never_5_clf = Never5Classifier()cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([0.91125, 0.90855, 0.90915])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 使用混淆矩阵from sklearn.model_selection import cross_val_predictfrom sklearn.metrics import confusion_matrixy_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)confusion_matrix(y_train_5, y_train_pred)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([[53892,   687],       [ 1891,  3530]], dtype=int64)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_train_pred<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([ True, False, False, ...,  True, False, False])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># precision 、recall and f1_scorefrom sklearn.metrics import precision_score, recall_score, f1_scoreprint(precision_score(y_train_5, y_train_pred))print(recall_score(y_train_5, y_train_pred))print(f1_score(y_train_5, y_train_pred))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0.83708797723500120.65117137059583110.7325171197343846</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># precision_recall curvey_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def plot_precision_recall_vs_threhold(precisions, recalls, thresholds):    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')    plt.legend(loc='best')    plt.tight_layout()    plt.show()plot_precision_recall_vs_threhold(precisions, recalls, thresholds)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>​<br><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_25_0.png" alt="output_25_0"><br>​    </p><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 使精确率达到90%的阈值threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]y_train_pred_90 = (y_scores >= threshold_90_precision)print(precision_score(y_train_5, y_train_pred_90))print(recall_score(y_train_5, y_train_pred_90))print(f1_score(y_train_5, y_train_pred_90))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0.90003459010722930.47998524257517060.626082771896054</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># roc_curvefrom sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def plot_roc_curve(fpr, tpr, label=None):    plt.plot(fpr, tpr, linewidth=2, label=label)    plt.plot([0,1], [0,1], 'k--')    plt.xlabel('False Positive Rate')    plt.ylabel('True Positive Rate')    plt.grid()plot_roc_curve(fpr, tpr)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_28_0.png" alt="output_28_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.metrics import roc_auc_scoreroc_auc_score(y_train_5, y_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>0.9604938554008616</code></pre><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.ensemble import RandomForestClassifier<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">forest_clf = RandomForestClassifier(random_state=42)y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_scores_forest = y_probas_forest[:, 1]fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">plt.plot(fpr, tpr,linewidth=2, label='SGD')plot_roc_curve(fpr_forest, tpr_forest, label='RF')plt.legend()plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>​<br><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_34_0.png" alt="output_34_0"><br>​    </p><pre class="line-numbers language-lang-python"><code class="language-lang-python">print(roc_auc_score(y_train_5, y_scores_forest))print(precision_score(y_train_5, y_scores_forest>=0.5))print(recall_score(y_train_5, y_scores_forest>=0.5))print(f1_score(y_train_5, y_scores_forest>=0.5))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0.99834367313281450.98974680895584850.87253274303634020.9274509803921569</code></pre><h3 id="二分类总结"><a href="#二分类总结" class="headerlink" title="二分类总结"></a>二分类总结</h3><ol><li>选择一个合适的评估指标</li><li>用CV交叉验证来执行分类模型</li><li>选择precision\recall来避免陷阱</li><li>用roc和auc比较不同模型</li></ol><h3 id="多类别分类"><a href="#多类别分类" class="headerlink" title="多类别分类"></a>多类别分类</h3><ul><li>一个行记录只属于多类别中的一个类别</li></ul><ol><li>ovo：拆成N*(N-1)/2个分类，例如类别1vs类别2，类别1vs类别3，…</li><li>ovr: 拆成N个分类，例如类别1vs剩下，类别2vs剩下，…</li></ol><pre class="line-numbers language-lang-python"><code class="language-lang-python">sgd_clf = SGDClassifier(random_state=42)sgd_clf.fit(X_train, y_train)some_digit = X_train[0]sgd_clf.predict([some_digit])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([3], dtype=uint8)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">some_digit_scores = sgd_clf.decision_function([some_digit])some_digit_scores<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([[-31893.03095419, -34419.69069632,  -9530.63950739,          1823.73154031, -22320.14822878,  -1385.80478895,        -26188.91070951, -16147.51323997,  -4604.35491274,        -12050.767298  ]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">np.argmax(some_digit_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>3</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">sgd_clf.classes_<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 标准化处理from sklearn.preprocessing import StandardScaler<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># scaler = StandardScaler()# X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))# cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="错误率分析"><a href="#错误率分析" class="headerlink" title="错误率分析"></a>错误率分析</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python"># scaler = StandardScaler()# X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))# y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv = 3)# conf_mx = confusion_matrix(y_train, y_train_pred)# conf_mx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># plt.matshow(conf_mx, cmap=plt.cm.gray)# plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># row_sums = conf_mx.sum(axis=1, keepdims=True)# norm_conf_mx = conf_mx / row_sums# np.fill_diagonal(norm_conf_mx, 0)# plt.matshow(norm_conf_mx, cmap=plt.cm.gray)# plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># # 分析3、5的差别# def plot_digits(instances, images_per_row=10, **options): #     size = 28 #     images_per_row = min(len(instances), images_per_row) #     images = [instance.reshape(size,size) for instance in instances] #     n_rows = (len(instances) - 1) // images_per_row + 1 #     row_images = [] #     n_empty = n_rows * images_per_row - len(instances) #     images.append(np.zeros((size, size * n_empty)))  #     for row in range(n_rows): #         rimages = images[row * images_per_row : (row + 1) * images_per_row] #         row_images.append(np.concatenate(rimages, axis=1)) #     image = np.concatenate(row_images, axis=0) #     plt.imshow(image, cmap = plt.cm.binary, **options) #     plt.axis("off")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># cl_a, cl_b = 3, 5# X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]# X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]# X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]# X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]# plt.figure(figsize=(8,8))# plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)# plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)# plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)# plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)# plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h3><ul><li>一个行记录可能属于多个类别，例如一个人是男人、律师</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.neighbors import KNeighborsClassifier<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_train_large = (y_train >= 7)y_train_odd = (y_train % 2 == 1)y_multilabel = np.c_[y_train_large, y_train_odd]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">knn_clf = KNeighborsClassifier()knn_clf.fit(X_train, y_multilabel)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>KNeighborsClassifier()</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">knn_clf.predict([some_digit])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>array([[False,  True]])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">digit_show(some_digit)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_55_0.png" alt="output_55_0"></p><h4 id="评估多标签分类器"><a href="#评估多标签分类器" class="headerlink" title="评估多标签分类器"></a>评估多标签分类器</h4><ul><li>一个方法是对每个个体标签度量F1_score，然后计算平均值</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">f1_score(y_multilabel, y_train_knn_pred, average='macro')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>0.976410265560605</code></pre><h3 id="多输出-多类分类"><a href="#多输出-多类分类" class="headerlink" title="多输出-多类分类"></a>多输出-多类分类</h3><ul><li>一个标签可以是多类别的</li><li>例如：分类器的输出是多标签（一个像素一个标签），而每个标签可以有多个值，类标签和值标签</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">noise = np.random.randint(0, 100, (len(X_train), 784))X_train_mod = X_train + noisey_train_mod = X_trainnoise = np.random.randint(0, 100, (len(X_test), 784))X_test_mod = X_test + noisey_test_mod = X_test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">knn_clf = KNeighborsClassifier()knn_clf.fit(X_train_mod, y_train_mod)clean_digit = knn_clf.predict([X_test_mod[0]])digit_show(clean_digit)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_61_0.png" alt="output_61_0"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">digit_show(X_test_mod[0])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-fen-lei/output_62_0.png" alt="output_62_0"></p><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><ol><li>尝试在MNIST数据集上建立一个分类器，使它在测试集上的精度超过97%<ul><li>提示：使用KNeighborsClassifier，找出一个好的超参数，（对权重和超参数n_neighbors进行网格搜索）</li></ul></li></ol><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 导入数据# from sklearn.datasets import fetch_openml# # Load data from https://www.openml.org/d/554# X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)from sklearn.datasets import fetch_openmlX, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)print(X.shape, y.shape)# 划分训练集、测试集import numpy as npy = y.astype(np.uint8)index = 55000X_train, y_train, X_test, y_test = X[:index], y[:index], X[index:], y[index:]print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)# 画图查看其中一条记录import matplotlib as mplimport matplotlib.pyplot as pltindex = 0tmp = X[index].reshape((28, 28))plt.imshow(tmp, cmap=mpl.cm.binary, interpolation='nearest')plt.axis('off')plt.show()y[0]# 建立分类器y_train_5 = (y_train == 5)y_test_5 = (y_test == 5)from sklearn.neighbors import KNeighborsClassifierknn_clf = KNeighborsClassifier()knn_clf.fit(X_train, y_train_5)pred = knn_clf.predict(X_test)from sklearn.metrics import confusion_matrixconfusion_matrix(y_test_5, pred)from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_scoreprint(accuracy_score(y_test_5, pred))print(precision_score(y_test_5, pred))print(recall_score(y_test_5, pred))print(f1_score(y_test_5, pred))# 使用gridcv来进行超参数筛选from sklearn.model_selection import GridSearchCVpara_grid = [{'n_neighbors': [3,4,5], 'weights': ['uniform', 'distance']}]knn_fit = KNeighborsClassifier()grid_search = GridSearchCV(knn_fit, para_grid, cv=5)grid_search.fit(X_train, y_train_5)print(grid_search.best_params_)print(grid_search.best_estimator_)# 评估评分cvres = grid_search.cv_results_for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):    print(np.sqrt(mean_score), params)# 预测pred = grid_search.predict(X_test)from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_scoreprint(accuracy_score(y_test_5, pred))print(precision_score(y_test_5, pred))print(recall_score(y_test_5, pred))print(f1_score(y_test_5, pred))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><ol><li>拿Titanic 数据集去捣鼓一番。开始这个项目有一个很棒的平台：Kaggle！</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之一个完整项目</title>
      <link href="/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/"/>
      <url>/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/</url>
      
        <content type="html"><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="chrome-extension://cdonnmffkdaoajfknoeeecmchibpmkmg/assets/pdf/web/viewer.html?file=http%3A%2F%2Findex-of.es%2FVarios-2%2FHands%2520on%2520Machine%2520Learning%2520with%2520Scikit%2520Learn%2520and%2520Tensorflow.pdf" target="_blank" rel="noopener">hands on machine learning with scikit-learn and tensorflow</a></li><li><a href="https://github.com/ageron/handson-ml" target="_blank" rel="noopener">参考书代码</a></li><li><a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">参考书代码2</a></li></ul><h1 id="一个完整的机器学习项目"><a href="#一个完整的机器学习项目" class="headerlink" title="一个完整的机器学习项目"></a>一个完整的机器学习项目</h1><h2 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h2><ol><li>项目概述</li><li>获取数据</li><li>发现并可视化数据，发现规律</li><li>为机器学习算法准备数据</li><li>选择模型，进行训练</li><li>微调模型</li><li>给出解决方案</li><li>部署、监控、维护系统</li></ol><h2 id="使用真实数据"><a href="#使用真实数据" class="headerlink" title="使用真实数据"></a>使用真实数据</h2><ul><li>流行的开源数据仓库<ul><li><a href="http://archive.ics.uci.edu/ml/index.php" target="_blank" rel="noopener">UCI</a></li><li><a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">Kaggle</a></li><li><a href="https://registry.opendata.aws/" target="_blank" rel="noopener">Amazon AWS</a></li></ul></li><li>准入口（提供开源数据列表）<ul><li><a href="http://dataportals.org/search" target="_blank" rel="noopener">dataportals</a></li><li><a href="https://opendatamonitor.eu/frontend/web/index.php?r=dashboard%2Findex" target="_blank" rel="noopener">opendatamonitor</a></li><li><a href="https://www.quandl.com/" target="_blank" rel="noopener">quandl</a></li></ul></li><li>其他<ul><li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research" target="_blank" rel="noopener">wikipedia</a></li><li><a href="https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public" target="_blank" rel="noopener">quora</a></li><li><a href="https://www.reddit.com/r/datasets/" target="_blank" rel="noopener">reddit</a></li></ul></li></ul><h2 id="项目概览"><a href="#项目概览" class="headerlink" title="项目概览"></a>项目概览</h2><ul><li>利用加州普查数据，建立一个加州房价模型。这个数据包含每个街区组的人口、收入中位数、房价中位数等指标。</li></ul><h3 id="划定问题"><a href="#划定问题" class="headerlink" title="划定问题"></a>划定问题</h3><h4 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h4><ul><li>第一个问题：商业目标是什么？如何使用、并从模型受益？</li><li>第二个问题：现在的解决方案效果如何？</li></ul><h4 id="划定问题-1"><a href="#划定问题-1" class="headerlink" title="划定问题"></a>划定问题</h4><ul><li>监督或非监督或强化学习？</li><li>分类或回归或其他？</li><li>批量还是线上？</li></ul><h3 id="选择性能指标"><a href="#选择性能指标" class="headerlink" title="选择性能指标"></a>选择性能指标</h3><ul><li>回归问题的典型指标：均方根误差（RMSE）、平方绝对误差（MAE）</li></ul><h3 id="核实假设"><a href="#核实假设" class="headerlink" title="核实假设"></a>核实假设</h3><ul><li>首先列出并核对迄今（你或其他人）作出的假设</li></ul><h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><h3 id="下载数据"><a href="#下载数据" class="headerlink" title="下载数据"></a>下载数据</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">import osimport tarfilefrom six.moves import urllibDOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"HOUSING_PATH = "datasets/housing"HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):    if not os.path.isdir(housing_path):        os.makedirs(housing_path)    tgz_path = os.path.join(housing_path, "housing.tgz")    urllib.request.urlretrieve(housing_url, tgz_path)    housing_tgz = tarfile.open(tgz_path)    housing_tgz.extractall(path=housing_path)    housing_tgz.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># fetch_housing_data()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">import pandas as pddef load_housing_data(housing_path=HOUSING_PATH):    csv_path = os.path.join(housing_path, "housing.csv")    return pd.read_csv(csv_path)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing = load_housing_data()housing.head()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>      <th>median_house_value</th>      <th>ocean_proximity</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-122.23</td>      <td>37.88</td>      <td>41.0</td>      <td>880.0</td>      <td>129.0</td>      <td>322.0</td>      <td>126.0</td>      <td>8.3252</td>      <td>452600.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>1</th>      <td>-122.22</td>      <td>37.86</td>      <td>21.0</td>      <td>7099.0</td>      <td>1106.0</td>      <td>2401.0</td>      <td>1138.0</td>      <td>8.3014</td>      <td>358500.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>2</th>      <td>-122.24</td>      <td>37.85</td>      <td>52.0</td>      <td>1467.0</td>      <td>190.0</td>      <td>496.0</td>      <td>177.0</td>      <td>7.2574</td>      <td>352100.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>3</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>1274.0</td>      <td>235.0</td>      <td>558.0</td>      <td>219.0</td>      <td>5.6431</td>      <td>341300.0</td>      <td>NEAR BAY</td>    </tr>    <tr>      <th>4</th>      <td>-122.25</td>      <td>37.85</td>      <td>52.0</td>      <td>1627.0</td>      <td>280.0</td>      <td>565.0</td>      <td>259.0</td>      <td>3.8462</td>      <td>342200.0</td>      <td>NEAR BAY</td>    </tr>  </tbody></table></div><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing.info()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 20640 entries, 0 to 20639Data columns (total 10 columns): #   Column              Non-Null Count  Dtype  ---  ------              --------------  -----   0   longitude           20640 non-null  float64 1   latitude            20640 non-null  float64 2   housing_median_age  20640 non-null  float64 3   total_rooms         20640 non-null  float64 4   total_bedrooms      20433 non-null  float64 5   population          20640 non-null  float64 6   households          20640 non-null  float64 7   median_income       20640 non-null  float64 8   median_house_value  20640 non-null  float64 9   ocean_proximity     20640 non-null  object dtypes: float64(9), object(1)memory usage: 1.6+ MB</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing['ocean_proximity'].value_counts()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&lt;1H OCEAN     9136INLAND        6551NEAR OCEAN    2658NEAR BAY      2290ISLAND           5Name: ocean_proximity, dtype: int64</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing.describe()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>      <th>median_house_value</th>    </tr>  </thead>  <tbody>    <tr>      <th>count</th>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20433.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>      <td>20640.000000</td>    </tr>    <tr>      <th>mean</th>      <td>-119.569704</td>      <td>35.631861</td>      <td>28.639486</td>      <td>2635.763081</td>      <td>537.870553</td>      <td>1425.476744</td>      <td>499.539680</td>      <td>3.870671</td>      <td>206855.816909</td>    </tr>    <tr>      <th>std</th>      <td>2.003532</td>      <td>2.135952</td>      <td>12.585558</td>      <td>2181.615252</td>      <td>421.385070</td>      <td>1132.462122</td>      <td>382.329753</td>      <td>1.899822</td>      <td>115395.615874</td>    </tr>    <tr>      <th>min</th>      <td>-124.350000</td>      <td>32.540000</td>      <td>1.000000</td>      <td>2.000000</td>      <td>1.000000</td>      <td>3.000000</td>      <td>1.000000</td>      <td>0.499900</td>      <td>14999.000000</td>    </tr>    <tr>      <th>25%</th>      <td>-121.800000</td>      <td>33.930000</td>      <td>18.000000</td>      <td>1447.750000</td>      <td>296.000000</td>      <td>787.000000</td>      <td>280.000000</td>      <td>2.563400</td>      <td>119600.000000</td>    </tr>    <tr>      <th>50%</th>      <td>-118.490000</td>      <td>34.260000</td>      <td>29.000000</td>      <td>2127.000000</td>      <td>435.000000</td>      <td>1166.000000</td>      <td>409.000000</td>      <td>3.534800</td>      <td>179700.000000</td>    </tr>    <tr>      <th>75%</th>      <td>-118.010000</td>      <td>37.710000</td>      <td>37.000000</td>      <td>3148.000000</td>      <td>647.000000</td>      <td>1725.000000</td>      <td>605.000000</td>      <td>4.743250</td>      <td>264725.000000</td>    </tr>    <tr>      <th>max</th>      <td>-114.310000</td>      <td>41.950000</td>      <td>52.000000</td>      <td>39320.000000</td>      <td>6445.000000</td>      <td>35682.000000</td>      <td>6082.000000</td>      <td>15.000100</td>      <td>500001.000000</td>    </tr>  </tbody></table></div><pre class="line-numbers language-lang-python"><code class="language-lang-python">%matplotlib inlineimport matplotlib.pyplot as plthousing.hist(bins=50, figsize=(20, 15))plt.tight_layout()plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/output_8_0.png" alt="output_8_0"></p><h2 id="创建测试集"><a href="#创建测试集" class="headerlink" title="创建测试集"></a>创建测试集</h2><h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">import numpy as npdef train_test_split(data, test_ratio):    np.random.seed(0)  # 设置随机数种子以每次运行产生相同测试集，但是新数据加入就可能把之前训练集的加入到测试集    shuffled_indices = np.random.permutation(len(data))    test_set_size = int(len(data)*test_ratio)    test_indices = shuffled_indices[:test_set_size]    train_indices = shuffled_indices[test_set_size:]    return data.iloc[train_indices], data.iloc[test_indices]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">train_set, test_set = train_test_split(housing, test_ratio=0.2)print(len(train_set), "train + ", len(test_set), "test")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>16512 train +  4128 test</code></pre><ul><li>上述随机抽样如果新数据来，重新运行程序会将之前的训练样本有可能划分成测试样本</li><li>解决办法：一个通常的解决办法是使用每个实例的ID来判定这个实例是否应该放入测试集（假设每个实例都有唯一并且不变的ID）【例如，你可以计算出每个实例ID的哈希值，只保留其最后一个字节，如果该值小于等于51（约为256 的20%），就将其放入测试集。这样可以保证在多次运行中，测试集保持不变，即使更新了数据集。新的测试集会包含新实例中的20%，但不会有之前位于训练集的实例。】</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">import hashlibdef test_set_check(identifier, test_ratio, hash):    return hash(np.int64(identifier)).digest()[-1]<256*test_ratiodef split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):    ids = data[id_column]    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))    return data.loc[~in_test_set], data.loc[in_test_set]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 如果使用行索引作为唯一识别码，你需要保证新数据都放到现有数据的尾部，且没有行被删除housing_with_id = housing.reset_index()train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")print(len(train_set), "train + ", len(test_set), "test")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>16362 train +  4278 test</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 一个区的维度和经度在几百万年之内是不变的，所以可以将两者结合成一个IDhousing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")print(len(train_set), "train + ", len(test_set), "test")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>16267 train +  4373 test</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 最简单的函数是train_test_split ，它的作用和之前的函数split_train_test 很像，并带有其它一些功能。首先，它有一个random_state 参数，可以设定前面讲过的随机生成器种子；第二，你可以将种子传递给多个行数相同的数据集，可以在相同的索引上分割数据集（这个功能非常有用，比如你的标签值是放在另一个DataFrame 里的）from sklearn.model_selection import train_test_splittrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)print(len(train_set), "train + ", len(test_set), "test")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>16512 train +  4128 test</code></pre><ul><li>目前为止，我们采用的都是纯随机的取样方法。当你的数据集很大时（尤其是和属性数相比），这通常可行；但如果数据集不大，就会有采样偏差的风险。 </li></ul><h3 id="分层采样"><a href="#分层采样" class="headerlink" title="分层采样"></a>分层采样</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing["income_cat"] = np.ceil(housing["median_income"] / 1.5)housing["income_cat"].where(housing["income_cat"]<5, 5.0, inplace=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.model_selection import StratifiedShuffleSplitsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)for train_index, test_index in split.split(housing, housing["income_cat"]):    strat_train_set = housing.loc[train_index]    strat_test_set = housing.loc[test_index]print(len(strat_train_set), "train + ", len(strat_test_set), "test")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>16512 train +  4128 test</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing["income_cat"].value_counts() / len(housing)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>3.0    0.3505812.0    0.3188474.0    0.1763085.0    0.1144381.0    0.039826Name: income_cat, dtype: float64</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">strat_train_set["income_cat"].value_counts() / len(strat_train_set)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>3.0    0.3505942.0    0.3188594.0    0.1762965.0    0.1144021.0    0.039850Name: income_cat, dtype: float64</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">for set in (strat_train_set, strat_test_set):    set.drop(["income_cat"], axis=1, inplace=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="数据EDA"><a href="#数据EDA" class="headerlink" title="数据EDA"></a>数据EDA</h2><ul><li>首先将测试集放一旁，只研究训练集</li><li>如果训练集非常大，需要再采样一个探索集</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing = strat_train_set.copy()housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)  # alpha控制透明度<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&lt;AxesSubplot:xlabel=&#39;longitude&#39;, ylabel=&#39;latitude&#39;&gt;</code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/output_24_1.png" alt="output_24_1"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing.plot(kind='scatter', x="longitude", y="latitude", alpha=0.4,            s=housing['population']/100, label='population',            c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)plt.legend()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>&lt;matplotlib.legend.Legend at 0x1cb9a343b50&gt;</code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/output_25_1.png" alt="output_25_1"></p><ul><li>这张图说明房价和位置、人口密度联系密切</li></ul><h3 id="查找关联"><a href="#查找关联" class="headerlink" title="查找关联"></a>查找关联</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 相关系数corr_matrix = housing.corr()corr_matrix['median_house_value'].sort_values(ascending=False)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>median_house_value    1.000000median_income         0.687160total_rooms           0.135097housing_median_age    0.114110households            0.064506total_bedrooms        0.047689population           -0.026920longitude            -0.047432latitude             -0.142724Name: median_house_value, dtype: float64</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 相关系数图from pandas.plotting import scatter_matrixattributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']scatter_matrix(housing[attributes], figsize=(12,8))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([[&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_house_value&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_house_value&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_house_value&#39;&gt;],       [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;median_income&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_income&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;median_income&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;median_income&#39;&gt;],       [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;total_rooms&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;total_rooms&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;total_rooms&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;total_rooms&#39;&gt;],       [&lt;AxesSubplot:xlabel=&#39;median_house_value&#39;, ylabel=&#39;housing_median_age&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;housing_median_age&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;total_rooms&#39;, ylabel=&#39;housing_median_age&#39;&gt;,        &lt;AxesSubplot:xlabel=&#39;housing_median_age&#39;, ylabel=&#39;housing_median_age&#39;&gt;]],      dtype=object)</code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/output_28_1.png" alt="output_28_1"></p><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 最有希望用来预测房价中位数的属性是收入中位数，因此将这张图放大housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>&lt;AxesSubplot:xlabel=&#39;median_income&#39;, ylabel=&#39;median_house_value&#39;&gt;</code></pre><p><img src="/2021/03/19/ji-qi-xue-xi-zhi-yi-ge-wan-zheng-xiang-mu/output_29_1.png" alt="output_29_1"></p><ul><li>会发现一些直线，希望去除这些直线对应的记录，以防止算法重复这些巧合</li></ul><h3 id="属性组合试验"><a href="#属性组合试验" class="headerlink" title="属性组合试验"></a>属性组合试验</h3><ol><li>探索数据可能发现一些数据的巧合，需要将其去除</li><li>发现一些属性间有趣的关联</li><li>一些属性有长尾分布，将其转换</li><li>……</li></ol><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing['rooms_per_household'] = housing['total_rooms'] / housing['households']housing['bedrooms_per_room'] = housing['total_bedrooms'] / housing['total_rooms']housing['population_per_household'] = housing['population'] / housing['households']<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">corr_matrix = housing.corr()corr_matrix['median_house_value'].sort_values(ascending=False)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>median_house_value          1.000000median_income               0.687160rooms_per_household         0.146285total_rooms                 0.135097housing_median_age          0.114110households                  0.064506total_bedrooms              0.047689population_per_household   -0.021985population                 -0.026920longitude                  -0.047432latitude                   -0.142724bedrooms_per_room          -0.259984Name: median_house_value, dtype: float64</code></pre><h2 id="为机器学习算法准备数据"><a href="#为机器学习算法准备数据" class="headerlink" title="为机器学习算法准备数据"></a>为机器学习算法准备数据</h2><ul><li>写函数的好处：<ol><li>函数可以让你在任何数据集上方便的进行重复数据转换</li><li>你能慢慢建立一个转换函数库，可以在未来的项目中复用</li><li>在将数据传给算法之前，你可以在实时系统中使用这些函数</li><li>这可以让你方便的尝试多种数据转换，查看哪些转换方法结合起来效果最好</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing = strat_train_set.drop('median_house_value', axis=1)housing_labels = strat_train_set['median_house_value'].copy()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><ul><li>前面探索发现<code>total_bedrooms</code>有一些缺失值，有三个解决选项：<ol><li>去掉对应行记录</li><li>去掉这个属性</li><li>进行赋值（0、平均值、中位数等等）</li></ol></li><li>也可以使用sklearn里面的Imputer类来处理缺失值</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing.dropna(subset=['total_bedrooms'])  # 选项1housing.drop('total_bedrooms', axis=1)  # 选项2median = housing['total_bedrooms'].median()  # 选项3housing['total_bedrooms'].fillna(median)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>17606     351.018632     108.014650     471.03230      371.03555     1525.0          ...  6563      236.012053     294.013908     872.011159     380.015775     682.0Name: total_bedrooms, Length: 16512, dtype: float64</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.impute import SimpleImputerimputer = SimpleImputer(missing_values=np.nan, strategy='median')# 只有数值属性才能算出中位数housing_num = housing.drop('ocean_proximity', axis=1)imputer.fit(housing_num)print(imputer.statistics_)print(housing.median().values)# 对训练集进行转换X = imputer.transform(housing_num)housing_tr = pd.DataFrame(X, columns=housing_num.columns)housing_tr.head()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[-118.51     34.26     29.     2119.5     433.     1164.      408.    3.5409][-118.51     34.26     29.     2119.5     433.     1164.      408.    3.5409]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>longitude</th>      <th>latitude</th>      <th>housing_median_age</th>      <th>total_rooms</th>      <th>total_bedrooms</th>      <th>population</th>      <th>households</th>      <th>median_income</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>-121.89</td>      <td>37.29</td>      <td>38.0</td>      <td>1568.0</td>      <td>351.0</td>      <td>710.0</td>      <td>339.0</td>      <td>2.7042</td>    </tr>    <tr>      <th>1</th>      <td>-121.93</td>      <td>37.05</td>      <td>14.0</td>      <td>679.0</td>      <td>108.0</td>      <td>306.0</td>      <td>113.0</td>      <td>6.4214</td>    </tr>    <tr>      <th>2</th>      <td>-117.20</td>      <td>32.77</td>      <td>31.0</td>      <td>1952.0</td>      <td>471.0</td>      <td>936.0</td>      <td>462.0</td>      <td>2.8621</td>    </tr>    <tr>      <th>3</th>      <td>-119.61</td>      <td>36.31</td>      <td>25.0</td>      <td>1847.0</td>      <td>371.0</td>      <td>1460.0</td>      <td>353.0</td>      <td>1.8839</td>    </tr>    <tr>      <th>4</th>      <td>-118.59</td>      <td>34.23</td>      <td>17.0</td>      <td>6592.0</td>      <td>1525.0</td>      <td>4459.0</td>      <td>1463.0</td>      <td>3.0347</td>    </tr>  </tbody></table></div><h4 id="处理文本和类别属性"><a href="#处理文本和类别属性" class="headerlink" title="处理文本和类别属性"></a>处理文本和类别属性</h4><ol><li>将文本标签转换为数字：这样做的问题是：ML算法会认为两个临近值比两个疏远值更相似，但是对标签数据这样是不准确的<ul><li>sklearn提供一个转换器：LabelEncoder</li></ul></li><li>one-hot编码：每个属性弄成0-1<ul><li>sklearn提供一个转换器：OneHotEncoder</li></ul></li></ol><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.preprocessing import LabelEncoderencoder = LabelEncoder()housing_cat = housing['ocean_proximity']housing_cat_encoded = encoder.fit_transform(housing_cat)print(housing_cat_encoded)# 有多个文本特征列时，应使用factorize()方法housing_cat_encoded, housing_categories = housing_cat.factorize()print(housing_cat_encoded, housing_categories)print(encoder.classes_)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[0 0 4 ... 1 0 3][0 0 1 ... 2 0 3] Index([&#39;&lt;1H OCEAN&#39;, &#39;NEAR OCEAN&#39;, &#39;INLAND&#39;, &#39;NEAR BAY&#39;, &#39;ISLAND&#39;], dtype=&#39;object&#39;)[&#39;&lt;1H OCEAN&#39; &#39;INLAND&#39; &#39;ISLAND&#39; &#39;NEAR BAY&#39; &#39;NEAR OCEAN&#39;]</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder()housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))print(housing_cat_1hot.toarray())housing_cat_1hot  # 稀疏矩阵<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[[1. 0. 0. 0. 0.] [1. 0. 0. 0. 0.] [0. 1. 0. 0. 0.] ... [0. 0. 1. 0. 0.] [1. 0. 0. 0. 0.] [0. 0. 0. 1. 0.]]&lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;    with 16512 stored elements in Compressed Sparse Row format&gt;</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 使用LabelBinarize一步到位：获得稀疏矩阵+to arrayfrom sklearn.preprocessing import LabelBinarizerencoder = LabelBinarizer(sparse_output=False)housing_cat_1hot = encoder.fit_transform(housing_cat)print(housing_cat_1hot)# # 有多个文本特征列时，应使用sklearn中的CategoricalEncoder类# from sklearn.preprocessing import CategoricalEncoder<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[[1 0 0 0 0] [1 0 0 0 0] [0 0 0 0 1] ... [0 1 0 0 0] [1 0 0 0 0] [0 0 0 1 0]]</code></pre><h4 id="自定义转换器"><a href="#自定义转换器" class="headerlink" title="自定义转换器"></a>自定义转换器</h4><ul><li>尽管Scikit-Learn 提供了许多有用的转换器，你还是需要自己动手写转换器执行任务，比如自定义的清理操作，或属性组合。你需要让自制的转换器与Scikit-Learn 组件（比如流水线）无缝衔接工作，因为Scikit-Learn 是依赖鸭子类型的（而不是继承），你所需要做的是创建一个类并执行三个方法：fit() （返回self ），transform() ，和fit_transform() 。</li><li>鸭子类型（英语：duck typing）在程序设计中是动态类型的一种风格。在这种风格中，一个对象有效的语义，不是由继承自特定的类或实现特定的接口，而是由”当前方法和属性的集合”决定。</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.base import BaseEstimator, TransformerMixinrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6class CombinedAttributesAdder(BaseEstimator, TransformerMixin):    def __init__(self, add_bedrooms_pre_room=True):        self.add_bedrooms_pre_room = add_bedrooms_pre_room    def fit(self, X, y=None):        return self    def transform(self, X, y=None):        rooms_pre_household = X[:, rooms_ix] / X[:, household_ix]        population_pre_household = X[:, population_ix] / X[:, household_ix]        if self.add_bedrooms_pre_room:            bedrooms_pre_room = X[:, bedrooms_ix] / X[:, rooms_ix]            return np.c_[X, rooms_pre_household, population_pre_household, bedrooms_pre_room]        else:            return np.c_[X, rooms_pre_household, population_pre_household]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">attr_adder = CombinedAttributesAdder(add_bedrooms_pre_room=False)housing_extra_attribs = attr_adder.transform(housing.values)housing_extra_attribs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>array([[-121.89, 37.29, 38.0, ..., &#39;&lt;1H OCEAN&#39;, 4.625368731563422,        2.094395280235988],       [-121.93, 37.05, 14.0, ..., &#39;&lt;1H OCEAN&#39;, 6.008849557522124,        2.7079646017699117],       [-117.2, 32.77, 31.0, ..., &#39;NEAR OCEAN&#39;, 4.225108225108225,        2.0259740259740258],       ...,       [-116.4, 34.09, 9.0, ..., &#39;INLAND&#39;, 6.34640522875817,        2.742483660130719],       [-118.01, 33.82, 31.0, ..., &#39;&lt;1H OCEAN&#39;, 5.50561797752809,        3.808988764044944],       [-122.45, 37.77, 52.0, ..., &#39;NEAR BAY&#39;, 4.843505477308295,        1.9859154929577465]], dtype=object)</code></pre><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><ul><li>让所有属性具有相同量度：<ol><li>线性归一化： MinMaxScaler()</li><li>标准化: StandarScaler()</li></ol></li><li>与所有的转换一样，缩放器只能向训练集拟合，而不是向完整的数据集（包括测试集）。只有这样，你才能用缩放器转换训练集和测试集（和新数据）</li></ul><h4 id="转换流水线"><a href="#转换流水线" class="headerlink" title="转换流水线"></a>转换流水线</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipline = Pipeline([('imputer', SimpleImputer(strategy='median')),                        ('attribs_addr', CombinedAttributesAdder()),                        ('std_scaler', StandardScaler()),                       ])housing_num_tr = num_pipline.fit_transform(housing_num)housing_num_tr<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([[-1.15604281,  0.77194962,  0.74333089, ..., -0.31205452,        -0.08649871,  0.15531753],       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.21768338,        -0.03353391, -0.83628902],       [ 1.18684903, -1.34218285,  0.18664186, ..., -0.46531516,        -0.09240499,  0.4222004 ],       ...,       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.3469342 ,        -0.03055414, -0.52177644],       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.02499488,         0.06150916, -0.30340741],       [-1.43579109,  0.99645926,  1.85670895, ..., -0.22852947,        -0.09586294,  0.10180567]])</code></pre><h4 id="将数值流水线和属性型流水线一起处理"><a href="#将数值流水线和属性型流水线一起处理" class="headerlink" title="将数值流水线和属性型流水线一起处理"></a>将数值流水线和属性型流水线一起处理</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.pipeline import FeatureUnionfrom sklearn.base import BaseEstimator, TransformerMixinclass DataFrameSelector(BaseEstimator, TransformerMixin):    def __init__(self, attribute_names):        self.attribute_names=attribute_names    def fit(self, X, y=None):        return self    def transform(self, X):        return X[self.attribute_names].valuesclass MyLabelBinarizer(BaseEstimator, TransformerMixin):    def __init__(self, *args, **kwargs):        self.encoder = LabelBinarizer(*args, **kwargs)    def fit(self, x, y=None):        self.encoder.fit(x)        return self    def transform(self, x, y=None):        return self.encoder.transform(x)num_attribs = list(housing_num)cat_attribs = ['ocean_proximity']num_pipline = Pipeline([('selector', DataFrameSelector(num_attribs)),                        ('imputer', SimpleImputer(strategy='median')),                        ('attribs_addr', CombinedAttributesAdder()),                        ('std_scaler', StandardScaler()),                       ])cat_pipline = Pipeline([('selector', DataFrameSelector(cat_attribs)),                        ('label_binarizer', MyLabelBinarizer()),                       ])full_pipeline = FeatureUnion(transformer_list=[('num_pipeline', num_pipline),                                               ('cat_pipeline', cat_pipline),                                              ])housing_prepared = full_pipeline.fit_transform(housing)housing_prepared.shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>(16512, 16)</code></pre><h3 id="选择并训练模型"><a href="#选择并训练模型" class="headerlink" title="选择并训练模型"></a>选择并训练模型</h3><ul><li>前面已做<ol><li>限定了问题</li><li>获取数据</li><li>探索数据</li><li>采样测试集</li><li>自动化的转换流水线：清理数据</li></ol></li></ul><h4 id="在训练集上训练和评估"><a href="#在训练集上训练和评估" class="headerlink" title="在训练集上训练和评估"></a>在训练集上训练和评估</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 回归模型from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression();lin_reg.fit(housing_prepared, housing_labels);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 用一些训练数据看看some_data = housing.iloc[:5]some_labels = housing_labels.iloc[:5]some_data_prepared = full_pipeline.transform(some_data)print(lin_reg.predict(some_data_prepared))print(list(some_labels))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[210644.60459286 317768.80697211 210956.43331178  59218.98886849 189747.55849879][286600.0, 340600.0, 196900.0, 46300.0, 254500.0]</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 计算RSMEfrom sklearn.metrics import mean_squared_errorhousing_pred = lin_reg.predict(housing_prepared)lin_mse = mean_squared_error(housing_labels, housing_pred)lin_rmse = np.sqrt(lin_mse)lin_rmse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>68628.19819848923</code></pre><ul><li>显然回归模型欠拟合，特征没有提供足够多的信息来做一个好的预测，或者模型不够强大</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 决策树from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor();tree_reg.fit(housing_prepared, housing_labels);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">housing_pred = tree_reg.predict(housing_prepared)tree_mse = mean_squared_error(housing_labels, housing_pred)tree_rmse = np.sqrt(tree_mse)tree_rmse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>0.0</code></pre><ul><li>显然模型可能过拟合</li></ul><h4 id="使用交叉验证来做更佳的评估"><a href="#使用交叉验证来做更佳的评估" class="headerlink" title="使用交叉验证来做更佳的评估"></a>使用交叉验证来做更佳的评估</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.model_selection import cross_val_scorescores = cross_val_score(tree_reg, housing_prepared, housing_labels,                        scoring='neg_mean_squared_error', cv=10)tree_rmse_scores = np.sqrt(-scores)tree_rmse_scores<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([70402.31297471, 67320.10682085, 70223.47037104, 69614.92234167,       71787.54609682, 75496.45097963, 71048.37280966, 71597.08425246,       77043.79328191, 70578.37265691])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">def displayScores(scores):    print("Scores:", scores)    print("Mean:", scores.mean())    print("Std:", scores.std())displayScores(tree_rmse_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Scores: [70402.31297471 67320.10682085 70223.47037104 69614.92234167 71787.54609682 75496.45097963 71048.37280966 71597.08425246 77043.79328191 70578.37265691]Mean: 71511.24325856709Std: 2677.852533862523</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">scores = cross_val_score(lin_reg, housing_prepared, housing_labels,                        scoring='neg_mean_squared_error', cv=10)lin_rmse_scores = np.sqrt(-scores)lin_rmse_scores<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([66782.73843989, 66960.118071  , 70347.95244419, 74739.57052552,       68031.13388938, 71193.84183426, 64969.63056405, 68281.61137997,       71552.91566558, 67665.10082067])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">displayScores(lin_rmse_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067]Mean: 69052.46136345083Std: 2731.6740017983425</code></pre><ul><li>判断没错：决策树模型过拟合很严重，它的性能比线性回归模型还差</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 随机森林from sklearn.ensemble import RandomForestRegressorforest_reg = RandomForestRegressor()forest_reg.fit(housing_prepared, housing_labels)housing_pred = forest_reg.predict(housing_prepared)forest_mse = mean_squared_error(housing_labels, housing_pred)forest_rmse = np.sqrt(forest_mse)forest_rmse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>18691.633224101523</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">scores = cross_val_score(forest_reg, housing_prepared, housing_labels,                        scoring='neg_mean_squared_error', cv=10)forest_rmse_scores = np.sqrt(-scores)forest_rmse_scores<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>array([49503.09765569, 47556.24185705, 49876.34879458, 52163.99071947,       49535.73021436, 53483.37725634, 48882.69703437, 47583.62724759,       53339.88235391, 49722.75212307])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">displayScores(forest_rmse_scores)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>Scores: [49503.09765569 47556.24185705 49876.34879458 52163.99071947 49535.73021436 53483.37725634 48882.69703437 47583.62724759 53339.88235391 49722.75212307]Mean: 50164.77452564256Std: 2032.5814147343965</code></pre><ul><li>在继续深入前，应该尝试下不同模型，不要在调节超参数上花费太多时间，目标是列出一个可能的模型列表（2-5个）</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 保存模型import jobliboutput_path = 'model/'if not os.path.isdir(output_path):    os.makedirs(output_path)joblib.dump(forest_reg, output_path+'forest_reg.pkl');<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">import gcdel forest_reggc.collect();<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">forest_reg = joblib.load(output_path + 'forest_reg.pkl')forest_reg.predict(housing_prepared)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([267951.  , 323307.  , 221516.  , ..., 102155.  , 214044.  ,       464662.73])</code></pre><h3 id="模型微调"><a href="#模型微调" class="headerlink" title="模型微调"></a>模型微调</h3><ul><li>假设有一个列表了，列表里面有几个有希望的模型了，现在对它们进行微调</li></ul><ol><li>网格搜索</li><li>随即搜索</li><li>集成方法</li></ol><h4 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.model_selection import GridSearchCVparam_grid = [    {'n_estimators': [3,10,30], 'max_features': [2,4,6,8]},    {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]},]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg, param_grid, cv=5,                            scoring='neg_mean_squared_error')grid_search.fit(housing_prepared, housing_labels)(grid_search.best_params_)(grid_search.best_estimator_)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>RandomForestRegressor(max_features=6, n_estimators=30)</code></pre><ul><li>param_grid 告诉Scikit-Learn 首先评估所有的列在第一个dict 中的n_estimators 和max_features 的3 × 4 = 12 种组合（不用担心这些超参数的含义，会在第7 章中解释）。然后尝试第二个dict 中超参数的2 × 3 = 6 种组合，这次会将超参数bootstrap 设为False 而不是True （后者是该超参数的默认值）。</li><li>总之，网格搜索会探索12 + 6 = 18 种RandomForestRegressor 的超参数组合，会训练每个模型五次（因为用的是五折交叉验证）。换句话说，训练总共有18 × 5 = 90 轮！K 折将要花费大量时间，完成后，你就能获得参数的最佳组合</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 评估评分cvres = grid_search.cv_results_for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):    print(np.sqrt(-mean_score), params)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>64484.368945438095 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3}55801.68474277632 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10}53103.397117972374 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30}60739.34011042108 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3}52606.55731138118 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10}50461.78456229793 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30}58694.1806275357 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3}52167.01603965681 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10}49789.76142728186 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30}59244.45170151536 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3}52471.88272318626 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10}50024.71009745971 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30}62288.45521151971 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3}54541.851530599495 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10}60763.31894922859 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3}52256.3308785951 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10}58006.11393494447 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3}51525.95227740632 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10}</code></pre><h4 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h4><ul><li>当超参数的搜索空间很大时，最好使用<code>RandomizedSearchCV</code></li><li>随机搜索通过选择每个超参数的一个随机值的特定数量的随机组合，两个优点：<ol><li>如果你让随机搜索运行，比如1000 次，它会探索每个超参数的1000 个不同的值（而不是像网格搜索那样，只搜索每个超参数的几个值</li><li>你可以方便地通过设定搜索次数，控制超参数搜索的计算量</li></ol></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">from sklearn.model_selection import RandomizedSearchCVdistributions = dict(n_estimators=[3,10,30], max_features=[2,4,6,8])forest_reg = RandomForestRegressor()grid_search = RandomizedSearchCV(forest_reg, distributions,                                  random_state=0, cv=5,                                 scoring='neg_mean_squared_error')grid_search.fit(housing_prepared, housing_labels)(grid_search.best_params_)(grid_search.best_estimator_)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>RandomForestRegressor(max_features=8, n_estimators=30)</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 评估评分cvres = grid_search.cv_results_for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):    print(np.sqrt(-mean_score), params)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>60313.361125661046 {&#39;n_estimators&#39;: 3, &#39;max_features&#39;: 6}49834.72365719627 {&#39;n_estimators&#39;: 30, &#39;max_features&#39;: 8}52515.52121503662 {&#39;n_estimators&#39;: 10, &#39;max_features&#39;: 4}51547.81101848066 {&#39;n_estimators&#39;: 10, &#39;max_features&#39;: 8}52824.54605534015 {&#39;n_estimators&#39;: 30, &#39;max_features&#39;: 2}50011.013090069086 {&#39;n_estimators&#39;: 30, &#39;max_features&#39;: 6}55772.624674081875 {&#39;n_estimators&#39;: 10, &#39;max_features&#39;: 2}51994.5987814074 {&#39;n_estimators&#39;: 10, &#39;max_features&#39;: 6}58695.313520315474 {&#39;n_estimators&#39;: 3, &#39;max_features&#39;: 8}60144.83595308729 {&#39;n_estimators&#39;: 3, &#39;max_features&#39;: 4}</code></pre><h4 id="集成方法"><a href="#集成方法" class="headerlink" title="集成方法"></a>集成方法</h4><ul><li>另一种微调系统的方法是将表现最好的模型组合起来。组合（集成）之后的性能通常要比单独的模型要好（就像随机森林要比单独的决策树要好），特别是当单独模型的误差类型不同时</li></ul><h3 id="分析最佳模型和它们的误差"><a href="#分析最佳模型和它们的误差" class="headerlink" title="分析最佳模型和它们的误差"></a>分析最佳模型和它们的误差</h3><ul><li>根据特征重要性，丢弃一些无用特征</li><li>观察系统误差，搞清为什么有这些误差，如何改正问题（添加、去掉、清洗异常值等措施）</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">feature_importances = grid_search.best_estimator_.feature_importances_feature_importances<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre><code>array([6.54145890e-02, 6.11239658e-02, 4.68569762e-02, 1.57799553e-02,       1.47877860e-02, 1.52606278e-02, 1.45150045e-02, 3.56582379e-01,       5.46405542e-02, 1.14200534e-01, 7.60282167e-02, 7.53161308e-03,       1.52163827e-01, 3.13174862e-05, 2.25971757e-03, 2.82293596e-03])</code></pre><pre class="line-numbers language-lang-python"><code class="language-lang-python">extra_attribs = ['rooms_per_hold', 'pop_per_hold', 'bedrooms_per_room']cat_ont_hot_attribs = list(encoder.classes_)attributes = num_attribs + extra_attribs + cat_ont_hot_attribssorted(zip(feature_importances, attributes), reverse=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>[(0.35658237869153536, &#39;median_income&#39;), (0.1521638272583547, &#39;INLAND&#39;), (0.11420053447156554, &#39;pop_per_hold&#39;), (0.07602821669031448, &#39;bedrooms_per_room&#39;), (0.06541458900157228, &#39;longitude&#39;), (0.06112396584108679, &#39;latitude&#39;), (0.05464055415192507, &#39;rooms_per_hold&#39;), (0.04685697616305872, &#39;housing_median_age&#39;), (0.015779955349765996, &#39;total_rooms&#39;), (0.015260627821204058, &#39;population&#39;), (0.01478778596813993, &#39;total_bedrooms&#39;), (0.014515004497103572, &#39;households&#39;), (0.007531613076317472, &#39;&lt;1H OCEAN&#39;), (0.0028229359597008235, &#39;NEAR OCEAN&#39;), (0.0022597175721814156, &#39;NEAR BAY&#39;), (3.131748617381553e-05, &#39;ISLAND&#39;)]</code></pre><h2 id="用测试集评估系统"><a href="#用测试集评估系统" class="headerlink" title="用测试集评估系统"></a>用测试集评估系统</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">final_model = grid_search.best_estimator_X_test = strat_test_set.drop('median_house_value', axis=1)y_test = strat_test_set['median_house_value'].copy()X_test_prepared = full_pipeline.transform(X_test)final_predictions = final_model.predict(X_test_prepared)final_mse = mean_squared_error(y_test, final_predictions)final_rmse = np.sqrt(final_mse)final_rmse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>48237.045259350096</code></pre><h2 id="项目预上线阶段"><a href="#项目预上线阶段" class="headerlink" title="项目预上线阶段"></a>项目预上线阶段</h2><ol><li>展示方案<ol><li>学到了什么</li><li>做了什么</li><li>没做什么</li><li>做过什么假设</li><li>系统的限制是什么</li><li>……</li></ol></li></ol><ul><li>用漂亮的图表和容易记住的表达</li></ul><h2 id="启动、监控、维护系统"><a href="#启动、监控、维护系统" class="headerlink" title="启动、监控、维护系统"></a>启动、监控、维护系统</h2><ol><li>接入输入数据，编写测试</li><li>编写监控代码</li><li>编写评估系统</li></ol><h2 id="实践！"><a href="#实践！" class="headerlink" title="实践！"></a>实践！</h2>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之聚类</title>
      <link href="/2021/03/17/ji-qi-xue-xi-zhi-ju-lei/"/>
      <url>/2021/03/17/ji-qi-xue-xi-zhi-ju-lei/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a><code>KMeans</code></h2><h3 id="KMeans-1"><a href="#KMeans-1" class="headerlink" title="KMeans"></a><code>KMeans</code></h3><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ul><li>输入：<ul><li>样本集$D=\{x_1,x_2,…x_m\}$</li><li>需要聚成多少类$k$</li><li>最大迭代次数$N$</li></ul></li><li>输出：<ul><li>$k$个类簇$C=\{C_1,C_2,…C_k\}$</li></ul></li><li>流程<ol><li>从数据集$D$中<code>随机选择</code>$k$个样本作为初始的质心$\mu=\{\mu_1,\mu_2,…,\mu_k\}$</li><li>将$k$个类簇初始化为$C_t = \varnothing \;\; t =1,2…k$</li><li>from n=1 to N：<ol><li>对$m$个样本，计算样本$x_i,i=1,…,m$与$k$个质心的距离$d_{ij} = ||x_i - \mu_j||_2^2$。将该样本纳入最小$d_{ij}$对应的质心对应类别$\lambda_i$中，更新$C_{\lambda_i} = C_{\lambda_i} \cup \{x_i\}$</li><li>对$k$个类簇各自的样本重新计算新的质心$\mu_j = \frac{1}{|C_j|}\sum\limits_{x \in C_j}x$</li><li>如果这$k$个质心都没有发生变化，则<code>break</code></li></ol></li><li>输出类簇$C$</li></ol></li></ul><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><script type="math/tex; mode=display">E = \sum\limits_{i=1}^k\sum\limits_{x \in C_i} ||x-\mu_i||_2^2</script><h4 id="k-和距离如何确定"><a href="#k-和距离如何确定" class="headerlink" title="$k$和距离如何确定"></a>$k$和距离如何确定</h4><ul><li>$k$：根据对数据的先验经验选择一个合适的值，如果没有什么先验知识，则通过交叉验证选择一个合适值</li><li>距离：<code>欧氏距离</code></li></ul><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><ul><li>初始<code>k</code>个质心随机选取，太耗时</li><li>没必要所有距离都计算</li></ul><h3 id="KMeans-2"><a href="#KMeans-2" class="headerlink" title="KMeans++"></a><code>KMeans++</code></h3><h4 id="改进的地方"><a href="#改进的地方" class="headerlink" title="改进的地方"></a>改进的地方</h4><ul><li>初始<code>k</code>个质心<code>不再</code>随机选取，改进如下：<ol><li>对数据集$D$随机选择第一个点作为第一个质心$\mu_1$</li><li>对$D$中其他点$x_i$，计算它与已选择的质心中最近质心的距离$D(x_i) = arg\;min||x_i- \mu_r||_2^2\;\;r=1,2,…k_{selected}$</li><li>选择一个新的数据点作为新的质心，选择的原则是：$D(x)$较大的点，被选取作为质心的概率较大</li><li>重复2-3步，直到选出$k$个质心</li><li>将这$k$个质心作为初始化质心去进行<code>KMeans</code>计算</li></ol></li></ul><h3 id="elkan-KMeans"><a href="#elkan-KMeans" class="headerlink" title="elkan KMeans"></a><code>elkan KMeans</code></h3><h4 id="改进的地方-1"><a href="#改进的地方-1" class="headerlink" title="改进的地方"></a>改进的地方</h4><ul><li>有些样本点到质心的距离不计算，改进如下：<ol><li>利用三角形两边之和大于第三边，两边之差小于第三边的性质，减少距离的计算</li></ol></li></ul><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>如果样本的特征稀疏、有缺失值的时候，不能使用该方法，因为某些距离无法计算</li></ul><h3 id="大样本优化MiNi-Batch-KMeans"><a href="#大样本优化MiNi-Batch-KMeans" class="headerlink" title="大样本优化MiNi Batch KMeans"></a>大样本优化<code>MiNi Batch KMeans</code></h3><h4 id="改进的地方-2"><a href="#改进的地方-2" class="headerlink" title="改进的地方"></a>改进的地方</h4><ul><li>当样本量或者特征维度特别大时，<code>KMeans</code>算法非常耗时，使用<code>elkan KMeans</code>都还是耗时，此时就是<code>MiNi Batch KMeans</code>，即使用样本集$D$中的一部分<code>Batch size</code>样本进行<code>KMeans</code>，这部分样本通过<code>无放回随机采样</code></li><li>一般要对样本集$D$多运行几次<code>MiNi Batch KMeans</code>，选取相对最优类簇</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>会失去一些精度，需要效率和精度作权衡</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>优点：<ol><li>原理简单，收敛快</li><li>可解释强</li><li>仅需对$k$调参</li></ol></li><li>缺点：<ol><li>$k$值不好调参</li><li>对不是凸的数据集难收敛</li><li>如果隐含类别的数据不平衡，则聚类效果不佳</li><li>迭代只是局部最优</li><li>对噪音和异常点敏感</li></ol></li></ul><h2 id="BIRCH"><a href="#BIRCH" class="headerlink" title="BIRCH"></a><code>BIRCH</code></h2><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a><code>DBSCAN</code></h2><h2 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h2><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.cnblogs.com/pinard/p/6164214.html" target="_blank" rel="noopener">     K-Means聚类算法原理         </a></li><li><a href="https://www.cnblogs.com/pinard/p/6179132.html" target="_blank" rel="noopener">     BIRCH聚类算法原理         </a></li><li><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">     DBSCAN密度聚类算法         </a></li><li><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">     谱聚类（spectral clustering）原理总结         </a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据挖掘 </tag>
            
            <tag> 十大算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之决策树</title>
      <link href="/2021/03/15/ji-qi-xue-xi-zhi-jue-ce-shu/"/>
      <url>/2021/03/15/ji-qi-xue-xi-zhi-jue-ce-shu/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/03/15/ji-qi-xue-xi-zhi-jue-ce-shu/image-20210621160625825.png" alt="image-20210621160625825"></p><h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><ul><li>决策树既可以分类、也可以回归</li><li>又适合集成学习如随机森林、GBDT</li></ul><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li><p>输入训练集$T = \lbrace (x_1,y_1),(x_2,y_2), \cdots ,(x_n,y_n) \rbrace$，构建如下图的一棵树</p><p><img src="/2021/03/15/ji-qi-xue-xi-zhi-jue-ce-shu/image-20210315195647811.png" alt="image-20210315195647811"></p></li></ul><h2 id="面临的2个问题"><a href="#面临的2个问题" class="headerlink" title="面临的2个问题"></a>面临的2个问题</h2><h3 id="如何选择特征进行分裂"><a href="#如何选择特征进行分裂" class="headerlink" title="如何选择特征进行分裂"></a>如何选择特征进行分裂</h3><ul><li>显而易见，划分后每个节点内的样本类别混乱程度越低（纯度越高），则应该优先划分这个特征</li></ul><h3 id="啥时候停止分裂"><a href="#啥时候停止分裂" class="headerlink" title="啥时候停止分裂"></a>啥时候停止分裂</h3><ul><li>有两种自然情况应该停止分裂<ul><li>一是该节点对应的所有样本记录均属于同一类别，</li><li>二是该节点对应的所有样本的特征属性值均相等。</li></ul></li><li>但除此之外，是不是还应该其他情况停止分裂呢？<ul><li>使用剪枝方法提高模型的泛化能力</li><li>预剪枝</li><li>后剪枝</li></ul></li></ul><h2 id="混乱程度的度量"><a href="#混乱程度的度量" class="headerlink" title="混乱程度的度量"></a>混乱程度的度量</h2><h3 id="熵：越混乱越大"><a href="#熵：越混乱越大" class="headerlink" title="熵：越混乱越大"></a>熵：越混乱越大</h3><script type="math/tex; mode=display">Entropy(t)=-\sum\limits_{k}p(c_k|t)\log p(c_k|t)</script><ul><li>$t$：某个特征节点</li><li>$c_k$：样本所属类别</li><li>$p(c_k|t)$：$t$节点的$c_k$类别对应样本比例，<ul><li>例如D节点有15个样本，输出为0或者1。其中有9个输出为1， 6个输出为0，则类别1的样本比例为：$9/15$</li></ul></li></ul><h3 id="GINI系数：越混乱越大"><a href="#GINI系数：越混乱越大" class="headerlink" title="GINI系数：越混乱越大"></a><code>GINI</code>系数：越混乱越大</h3><script type="math/tex; mode=display">Gini(t)=1-\sum\limits_{k}[p(c_k|t)]^2</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="table-container"><table><thead><tr><th>算法</th><th>模型</th><th>树结构</th><th>特征选择</th><th>连续值</th><th>缺失值</th><th>剪枝</th></tr></thead><tbody><tr><td><code>ID3</code></td><td>分类</td><td>多叉</td><td>信息增益</td><td>no</td><td>no</td><td>no</td></tr><tr><td><code>C4.5</code></td><td>分类</td><td>多叉</td><td>信息增益率</td><td>yes</td><td>yes</td><td>yes</td></tr><tr><td><code>CART</code></td><td>分裂、回归</td><td>二叉</td><td><code>gini</code>、均方差</td><td>yes</td><td>yes</td><td>yes</td></tr></tbody></table></div><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h2 id="ID3：至少二叉"><a href="#ID3：至少二叉" class="headerlink" title="ID3：至少二叉"></a><code>ID3</code>：至少二叉</h2><h3 id="分裂目标函数：信息增益，越大，越适合"><a href="#分裂目标函数：信息增益，越大，越适合" class="headerlink" title="分裂目标函数：信息增益，越大，越适合"></a>分裂目标函数：信息增益，越大，越适合</h3><script type="math/tex; mode=display">\Delta = Entropy(parent) - \sum\limits_{i=1}^{n}{N(a_i)\over N}Entropy(a_i)</script><ul><li>$a_i$：$parent$节点根据特征$X$分裂成的子节点</li><li>$N$：$parent$节点对应的样本量</li><li>$N(a_i)$：$a_i$节点对应的样本量</li></ul><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul><li>输入：<ul><li>$m$个样本，样本类别集合$D$，</li><li>每个样本有$n$个<code>离散</code>特征，特征集合$A$</li></ul></li><li>输出：<ul><li>决策树$T$</li></ul></li><li>过程<ol><li>初始化信息增益的阈值$\epsilon$</li><li>判断是否满足自然分裂条件：<ol><li>判断样本是否为同一类别输出$L_i$，如果是则返回单节点树$T$。标记类别为$L_i$</li><li>判断特征是否为空，如果是则返回单节点树$T$。标记类别为样本类别集合$D$中实例数最多的类别</li></ol></li><li>计算特征集合$A$中的各个特征对$D$的<code>信息增益</code>，选择信息增益最大的特征$A_g$进行分裂</li><li>如果$A_g$的信息增益小于$\epsilon$，则返回单节点数$T$，标记类别为样本类别集合$D$中实例数最多的类别</li><li>否则，按特征$A_g$的不同取值将对应样本类别$D$分成不同的类别集合$D_i$。每个类别产生一个子节点，对应特征值为$A_{gi}$。返回增加了节点的树$T$</li><li>对于所有的子节点，令$D=D_i,A=A-\{A_g\}$，重复2-6步</li><li>得到树$T$</li></ol></li></ul><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>缺点：<ol><li>没有考虑<code>连续特征</code></li><li>没有考虑<code>缺失值</code></li><li>分裂目标函数使用信息增益的方式，在相同条件下，取值比较多的特征比取值少的特征信息增益大，这样会有失偏颇</li><li>没有考虑过拟合问题</li></ol></li></ul><h2 id="C4-5：至少二叉，对ID3的改进版"><a href="#C4-5：至少二叉，对ID3的改进版" class="headerlink" title="C4.5：至少二叉，对ID3的改进版"></a><code>C4.5</code>：至少二叉，对<code>ID3</code>的改进版</h2><h3 id="改进的地方"><a href="#改进的地方" class="headerlink" title="改进的地方"></a>改进的地方</h3><ol><li>对连续特征离散化，要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程</li><li>针对缺失值处理，主要需要解决的是以下两个问题<ol><li>一是在某个特征有缺失值时，怎么分裂这个特征<ul><li>将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据<code>D1</code>，另一部分是没有特征A的数据<code>D2</code></li><li>然后对于没有缺失特征A的数据集<code>D1</code>来和对应的A特征的各个特征值一起计算加权重后的信息增益比，</li><li>最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例</li></ul></li><li>二是选定了划分属性，对于在该属性上缺失样本应该如何处理<ul><li>可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。</li><li>比如缺失特征A的样本a之前权重为1，特征A有3个特征值<code>A1,A2,A3</code>。 3个特征值对应的无缺失A特征的样本个数为<code>2,3,4</code>.则a同时划分入<code>A1，A2，A3</code>。对应权重调节为<code>2/9,3/9, 4/9</code>。</li></ul></li></ol></li><li>信息增益作为标准容易偏向于取值较多的特征的问题。<code>C4.5</code>引入一个<code>信息增益比</code>的变量</li><li><code>C4.5</code>引入了正则化系数进行初步的剪枝，处理过拟合问题</li></ol><h3 id="分裂目标函数，越大，越适合"><a href="#分裂目标函数，越大，越适合" class="headerlink" title="分裂目标函数，越大，越适合"></a>分裂目标函数，越大，越适合</h3><script type="math/tex; mode=display">Gain \ ratio = {\Delta \over Entropy(parent)}</script><h3 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul><li>同<code>ID3</code></li></ul><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>缺点<ol><li>由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，<code>C4.5</code>的剪枝方法有优化的空间。思路主要是两种，<ol><li>一种是预剪枝，即在生成决策树的时候就决定是否剪枝。</li><li>另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。</li></ol></li><li>生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率</li><li>只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围</li><li>由于使用了熵模型，里面有大量的耗时的对数运算，这对有连续值且还有大量的排序运算时，耗时糟糕</li></ol></li></ul><h2 id="CART：只能二叉，三个算法中最优"><a href="#CART：只能二叉，三个算法中最优" class="headerlink" title="CART：只能二叉，三个算法中最优"></a><code>CART</code>：只能二叉，三个算法中最优</h2><h3 id="改进的地方-1"><a href="#改进的地方-1" class="headerlink" title="改进的地方"></a>改进的地方</h3><ol><li>使用<code>GINI系数</code>，减少计算耗时</li><li>只能二叉，即每个特征只能分裂两个子节点<ol><li>对于离散特征，如有多个类别，则不停二分，然后找出<code>GINI</code>系数最小的那个分裂</li><li>对于连续特征，则按值从小到大排列，不断二分，找出<code>GINI</code>系数最小的那个分裂</li></ol></li><li></li></ol><h3 id="分裂目标函数，越小，越适合"><a href="#分裂目标函数，越小，越适合" class="headerlink" title="分裂目标函数，越小，越适合"></a>分裂目标函数，越小，越适合</h3><script type="math/tex; mode=display">Gini(D, X) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)</script><h3 id="算法步骤-2"><a href="#算法步骤-2" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul><li>输入：<ul><li>$m$个样本，样本类别集合$D$，</li><li>特征集合$A$</li><li><code>gini</code>系数的阈值，</li><li>样本个数阈值</li></ul></li><li>输出：<ul><li>决策树$T$</li></ul></li><li>过程<ol><li>判断是否满足自然分裂条件：<ol><li>判断样本是否为同一类别输出$L_i$，如果是则返回单节点树$T$。标记类别为$L_i$</li><li>判断特征是否为空，如果是则返回单节点树$T$。标记类别为样本类别集合$D$中实例数最多的类别</li></ol></li><li>判断是否满足条件：<ol><li>判断样本个数是否小于阈值，是则返回</li></ol></li><li>计算特征集合$A$中的各个特征对$D$的<code>gini系数</code>，选择值最小的特征$A_g$进行分裂<ol><li>对于离散、连续值分裂见改进的地方</li><li>对于缺失值，处理同<code>C4.5</code></li></ol></li><li>如果$A_g$的<code>gini系数</code>小于$\epsilon$，则返回单节点数$T$，标记类别为样本类别集合$D$中实例数最多的类别</li><li>否则，按分裂特征对应的样本类别$D$分成不同的类别集合$D_i$。每个类别产生一个子节点，对应特征值为$A_{gi}$。返回增加了节点的树$T$</li><li>对左右节点重复1-5步</li><li>得到树$T$</li></ol></li></ul><h3 id="CART回归树"><a href="#CART回归树" class="headerlink" title="CART回归树"></a><code>CART</code>回归树</h3><ul><li><p>如果样本输出是连续值，则是回归树</p></li><li><p>除了概念的不同，<code>CART</code>回归树和分类树的建立和预测区别主要是：</p><ol><li><p>连续值的处理方法不同</p><ul><li>分类树使用的是<code>gini</code>系数，而回归树使用均方误差最小来进行特征分裂，公式如下<script type="math/tex; mode=display">\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]</script></li></ul></li></ol></li></ul><ol><li><p>树建立后预测的方式不同</p><ul><li>回归树使用叶子节点的均值来作为样本预测输出</li></ul></li></ol><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><ul><li>由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，</li><li>即类似于线性回归的正则化，来增加决策树的泛化能力</li><li><code>CART</code>使用后剪枝</li></ul><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><ul><li>先生成决策树，然后产生所有可能的剪枝后的树</li><li>然后使用交叉验证来检验各种剪枝后的树的效果，选择泛化能力最好的树</li><li>剪枝步骤：<ol><li>确定损失函数：$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|$<ol><li>$C(T_t)$：预测误差，回归树是均方误差，分类树是<code>GINI</code>系数</li><li>$\alpha$：正则化参数，越大剪枝越厉害</li><li>$|T_t|$：叶子节点数量</li></ol></li><li>初始化$i=0,T=T_0$，最优子树集合$w=\{T\}$</li><li>从叶子节点自下而上的把所有节点是否剪枝的值$\alpha$计算出来</li><li>得到最小的$\alpha$以及对应的最优子树$T_i$</li><li>最优子树集合$w=w \cup T_i$</li><li>令$i=i+1,T=T_i$，重复2-5步</li><li>最后得到最优子树集合$w=\{T,T_0,T_1,…\}$</li><li>对最优子树集合的每一颗树去验证集预测，得到泛化能力最好的子树</li></ol></li></ul><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="chrome-extension://cdonnmffkdaoajfknoeeecmchibpmkmg/assets/pdf/web/viewer.html?file=https%3A%2F%2Flink.springer.com%2Fcontent%2Fpdf%2F10.1007%2FBF00116251.pdf" target="_blank" rel="noopener">Induction of Decision Trees </a></li><li><a href>Classification and regression trees：Breiman.etl</a></li><li><a href="https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener">wiki：决策树学习</a></li><li><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></li><li><a href="https://www.cnblogs.com/en-heng/p/5013995.html" target="_blank" rel="noopener">【十大经典数据挖掘算法】C4.5</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据挖掘 </tag>
            
            <tag> 十大算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之KNN</title>
      <link href="/2021/03/15/ji-qi-xue-xi-zhi-knn/"/>
      <url>/2021/03/15/ji-qi-xue-xi-zhi-knn/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li><p>输入：</p><ul><li>$x$：待分类的预测样本，</li><li>$k$：最近的$k$个作为标准</li><li>$T = \lbrace (x_1,y_1),(x_2,y_2), \cdots ,(x_n,y_n) \rbrace$：训练集，是多维特征空间向量，其中每个训练样本带有一个类别标签</li><li>距离度量：一般情况下，将<a href="https://zh.wikipedia.org/wiki/欧氏距离" target="_blank" rel="noopener">欧氏距离</a>作为距离度量，但是这是只适用于连续变量。在文本分类这种离散变量情况下，另一个度量——<strong>重叠度量</strong>（或<a href="https://zh.wikipedia.org/wiki/海明距离" target="_blank" rel="noopener">海明距离</a>）可以用来作为度量</li></ul></li><li><p>输出：</p><ul><li>$y$：待分类预测样本$x$对应的类别</li></ul></li><li><p>决策规则：</p><script type="math/tex; mode=display">\begin{equation} y = \arg \mathop {\max }\limits_{c_j} \sum\limits_{x_i \in n_k(x)} I(y_i = c_j),\ i = 1,2, \cdots ,n; \ j = 1,2, \cdots ,k \label{eq:obj} \end{equation}</script><ul><li>找出训练样本集中与预测样本$x$距离相近的$k$个样本，根据这$k$个样本对应的类别，采取<strong>多数表决</strong>的规则来确定预测样本$x$的类别</li></ul></li></ul><h2 id="参数选择"><a href="#参数选择" class="headerlink" title="参数选择"></a>参数选择</h2><h3 id="k"><a href="#k" class="headerlink" title="$k$"></a>$k$</h3><ul><li>在二元（两类）分类问题中，选取$k$为奇数有助于避免两个分类平票的情形</li><li>在此问题下，选取最佳经验$k$值的方法是<a href="https://zh.wikipedia.org/wiki/自助法" target="_blank" rel="noopener">自助法</a></li><li>常用的是交叉验证方法来选取$k$</li></ul><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><ul><li>一般情况下，将<a href="https://zh.wikipedia.org/wiki/欧氏距离" target="_blank" rel="noopener">欧氏距离</a>作为距离度量，但是这是只适用于连续变量。在文本分类这种离散变量情况下，另一个度量——<strong>重叠度量</strong>（或<a href="https://zh.wikipedia.org/wiki/海明距离" target="_blank" rel="noopener">海明距离</a>）可以用来作为度量</li></ul><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h2 id="修正之加权KNN"><a href="#修正之加权KNN" class="headerlink" title="修正之加权KNN"></a>修正之加权<code>KNN</code></h2><ul><li><p>上面提到的计算规则中，$k$邻域的样本点对预测结果的贡献度是相等的</p></li><li><p>但我们直观理解，距离更近的样本点应有更大的相似度，其贡献度应比距离更远的样本点大</p></li><li><p>所以可以加上权值$w_i = \frac{1}{\left| {x_i - x} \right|}$进行修正，则式(1)变成：</p><script type="math/tex; mode=display">\begin{equation} y = \arg \mathop {\max }\limits_{c_j} \sum\limits_{x_i \in n_k(x)} {w_i \times I(y_i = c_j)},\ i = 1,2, \cdots ,n; \ j = 1,2, \cdots ,k \label{eq:obj1} \end{equation}</script></li></ul><h2 id="加快计算之KD树"><a href="#加快计算之KD树" class="headerlink" title="加快计算之KD树"></a>加快计算之<code>KD</code>树</h2><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href>Michael Steinbach and Pang-Ning Tan, The Top Ten Algorithms in Data Mining.</a></li><li><a href="https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">wiki-K近邻算法</a></li><li><a href="https://www.cnblogs.com/en-heng/p/5000628.html" target="_blank" rel="noopener">【十大经典数据挖掘算法】kNN</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据挖掘 </tag>
            
            <tag> 十大算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习之图像识别</title>
      <link href="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/"/>
      <url>/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h2><ul><li>OCR，光学字符识别，大家可能都用过，就是将图片中的文字提取出来形成文本</li><li>那在算法上如何做呢？如下图<ul><li>首先是进行文字所在区域检测</li><li>然后识别检测到的区域</li></ul></li></ul><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/000025.jpg" alt="000025" style="zoom:25%;"></p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/000025_seg1_cnocr.png" alt="000025_seg1_cnocr"></p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/000025_segr1_cnocr.jpg" alt="000025_segr1_cnocr"></p><h1 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h1><h2 id="text-detection"><a href="#text-detection" class="headerlink" title="text detection"></a>text detection</h2><ol><li><p><a href="https://arxiv.org/abs/1911.08947" target="_blank" rel="noopener">DBNet</a>(AAAI’2020): <a href="https://zhuanlan.zhihu.com/p/94677957" target="_blank" rel="noopener">detail</a>，速度准确性兼顾，支持弯曲文本检测。可微二值化的实时场景文本检测，使用FPN(特征金字塔)结构，在精度、效率上都表现优秀，支持弯曲文本检测，但是无法处理文本包含文本</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@article{Liao_Wan_Yao_Chen_Bai_2020,    title={Real-Time Scene Text Detection with Differentiable Binarization},    journal={Proceedings of the AAAI Conference on Artificial Intelligence},    author={Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},    year={2020},    pages={11474-11481}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>(ICCV’2017): <a href="https://zhuanlan.zhihu.com/p/37998710" target="_blank" rel="noopener">detail</a>，ResNet-FPN+Fast RCNN+mask。ResNet-FPN+Fast RCNN+mask(目标掩码)，主要用于目标检测</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@INPROCEEDINGS{8237584,    author={K. {He} and G. {Gkioxari} and P. {Dollár} and R. {Girshick}},    booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},    title={Mask R-CNN},    year={2017},    pages={2980-2988},    doi={10.1109/ICCV.2017.322}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1908.05900" target="_blank" rel="noopener">PANet</a>(ICCV’2019): <a href="https://zhuanlan.zhihu.com/p/81415166" target="_blank" rel="noopener">detail</a>， 像素聚合网络，速度快，任意形状文字。像素聚合网络，速度快，包含了两个步骤（1）用分割网络预测文字区域、核参数以及相似向量；（2）从预测的核中重建完整的文字实例</p><ol><li>通过预测文字所处区域来描述文字的完整形状；</li><li>通过预测出核参数来区分不同的文字实例。</li><li>另外网络也会预测每个文字像素的相似向量，以保证像素的相似向量与来自同样文本的核之间的距离足够小</li></ol></li><li><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@inproceedings{WangXSZWLYS19,    author={Wenhai Wang and Enze Xie and Xiaoge Song and Yuhang Zang and Wenjia Wang and Tong Lu and Gang Yu and Chunhua Shen},    title={Efficient and Accurate Arbitrary-Shaped Text Detection With Pixel Aggregation Network},    booktitle={ICCV},    pages={8439--8448},    year={2019}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1903.12473" target="_blank" rel="noopener">PSENet</a>(CVPR’2019): <a href="https://zhuanlan.zhihu.com/p/63074253" target="_blank" rel="noopener">detail</a>，预测多个分割结果，逐步扩张。这个文章主要做的创新点大概就是<strong>预测多个分割结果，分别是S1,S2,S3…Sn</strong>代表不同的等级面积的结果，S1最小，基本就是文本骨架，Sn最大。然后在后处理的过程中，先用<strong>最小的预测结果去区分文本，再逐步扩张成正常文本大小</strong></p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@inproceedings{wang2019shape,    title={Shape robust text detection with progressive scale expansion network},    author={Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai},    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},    pages={9336--9345},    year={2019}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1807.01544" target="_blank" rel="noopener">TextSnake</a>(ECCV’2018): <a href="https://zhuanlan.zhihu.com/p/55492034" target="_blank" rel="noopener">detail</a>，不规则文本预测，类似文本蛇。 该论文的创新点主要在于提出一个类似于文本蛇的检测方式对<strong>不规则文本进行预测</strong>，该算法主要做的是五个任务：<strong>1、预测文本 2、预测文本中心线 3、预测一个文本中15个圆的半径 4、预测中心线与圆心的sin 5、预测cos</strong></p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@article{long2018textsnake,    title={TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes},    author={Long, Shangbang and Ruan, Jiaqiang and Zhang, Wenjie and He, Xin and Wu, Wenhao and Yao, Cong},    booktitle={ECCV},    pages={20-36},    year={2018}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1904.01941" target="_blank" rel="noopener">CRAFT</a>(CVPR’2019): <a href="https://zhuanlan.zhihu.com/p/141865260" target="_blank" rel="noopener">detail</a>，速度快，弱监督做字符尺度的分割。文章最大贡献是它创新的提出了一种弱监督模型，可以在真实样本只有文本行标注的情况下，做字符尺度的图像分割</p></li><li><p><a href="https://arxiv.org/abs/1609.03605" target="_blank" rel="noopener">CTPN</a>(ECCV’2016): <a href="https://zhuanlan.zhihu.com/p/34757009" target="_blank" rel="noopener">detail</a>，检测横向分布，字符级别。CTPN结合CNN与LSTM深度网络，能有效的检测出复杂场景的横向分布的文字</p></li></ol><p>下面两张图来自DBNET论文，主要说明这些检测网络在公开数据集的表现，可以看出DBNET在这些检测网络中是速度与精度兼顾的。除了DBNET，PSENET和CRAFT的表现也还不错</p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/dbnet_figure1.jpg" alt="dbnet_figure1" style="zoom: 50%;"></p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/dbnet_table5.jpg" alt="dbnet_table5" style="zoom:50%;"></p><h2 id="text-recognition"><a href="#text-recognition" class="headerlink" title="text recognition"></a>text recognition</h2><ol><li><p><a href="https://arxiv.org/abs/1507.05717" target="_blank" rel="noopener">CRNN</a>(TPAMI’2016): <a href="https://zhuanlan.zhihu.com/p/43534801" target="_blank" rel="noopener">detail</a>, CNN+RNN+CTC。卷积+循环神经网络+softmax即输出字符</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@article{shi2016end,    title={An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition},    author={Shi, Baoguang and Bai, Xiang and Yao, Cong},    journal={IEEE transactions on pattern analysis and machine intelligence},    year={2016}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1806.00926" target="_blank" rel="noopener">NRTR</a>(ICDAR’2019), A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition, 不使用CNN+RNN。注意力机制，本文首次提出了一种无重复序列到序列文本识别器，称为NRTR，它完全消除了重复和卷积。 NRTR遵循编码器-解码器范例，其中编码器使用堆叠式自注意提取图像特征，而解码器则应用堆叠式自注意来基于编码器输出识别文本。 NRTR完全依靠自我关注机制，因此可以以更高的并行度和更少的复杂性进行训练</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@inproceedings{sheng2019nrtr,    title={NRTR: A no-recurrence sequence-to-sequence model for scene text recognition},    author={Sheng, Fenfen and Chen, Zhineng and Xu, Bo},    booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},    pages={781--786},    year={2019},    organization={IEEE}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/2007.07542" target="_blank" rel="noopener">RobustScanner</a>(ECCV’2020): <a href="https://zhuanlan.zhihu.com/p/196419633" target="_blank" rel="noopener">detail</a>, 注意力机制，增加文本位置信息。 注意力机制，我们凭经验发现代表性的字符级序列解码器不仅利用上下文信息，而且利用位置信息。 现有方法高度依赖的文本信息会引起注意力漂移的问题。 为了抑制这种副作用，我们提出了一种新颖的位置增强分支，并将其输出与解码器关注模块的输出动态融合以进行场景文本识别</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@inproceedings{yue2020robustscanner,    title={RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition},    author={Yue, Xiaoyu and Kuang, Zhanghui and Lin, Chenhao and Sun, Hongbin and Zhang, Wayne},    booktitle={European Conference on Computer Vision},    year={2020}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><a href="https://arxiv.org/abs/1811.00751" target="_blank" rel="noopener">SAR</a>(AAAI’2019): <a href="https://zhuanlan.zhihu.com/p/53358238" target="_blank" rel="noopener">detail</a>，不规则文本识别，ResNet+LSTM+2-dimensional attention module。 由于文本外观的大变化（例如曲率，方向和变形），在自然场景图像中识别不规则文本非常困难。 现有的大多数方法都严重依赖复杂的模型设计和/或额外的细粒度注释，这在一定程度上增加了算法实现和数据收集的难度。 在这项工作中，我们使用现成的神经网络组件和仅单词级注释，为不规则场景文本识别提出了易于实施的强基准。 它由一个31层ResNet，一个基于LSTM的编码器-解码器框架和一个二维注意模块组成。 尽管它很简单，但是所提出的方法是健壮的。 它在不规则文本识别基准上达到了最先进的性能，并在常规文本数据集上实现了可比的结果</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@inproceedings{li2019show,    title={Show, attend and read: A simple and strong baseline for irregular text recognition},    author={Li, Hui and Wang, Peng and Shen, Chunhua and Zhang, Guyu},    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},    volume={33},    number={01},    pages={8610--8617},    year={2019}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><code>SegOCR Simple Baseline</code></p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">@unpublished{key,    title={SegOCR Simple Baseline.},    author={},    note={Unpublished Manuscript},    year={2021}    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="easyocr"><a href="#easyocr" class="headerlink" title="easyocr"></a><a href="https://github.com/JaidedAI/EasyOCR" target="_blank" rel="noopener">easyocr</a></h2><p>结构如下图</p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/easyocr_framework.jpeg" alt="easyocr_framework"></p><h3 id="CRAFT网络结构"><a href="#CRAFT网络结构" class="headerlink" title="CRAFT网络结构"></a><a href="https://arxiv.org/abs/1904.01941" target="_blank" rel="noopener">CRAFT</a>网络结构</h3><p>下图是它的网络结构，CRAFT的骨干卷积网络是<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG16</a>，在此基础上作者使用了类似于<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-net</a>的结构，浅层和深层的卷积特征相结合作为输出，有效的保留了浅层的结构特征和深层的语义特征。在U-net之后，网络增加一系列层卷积操层，最终的1x1卷积层使用两个卷积核输出两个分支结果，第一支为各像素点处于字符中心的概率（位置分），第二支为各像素点处于字符间隙的概率（邻域分）。通过这两层输出，我们可以分别得到字符位置和字符间连接情况，进而将结果整合为文本框</p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/craft.jpg" alt="craft" style="zoom:50%;"></p><h3 id="CRAFT位置分邻域分标注"><a href="#CRAFT位置分邻域分标注" class="headerlink" title="CRAFT位置分邻域分标注"></a><a href="https://arxiv.org/abs/1904.01941" target="_blank" rel="noopener">CRAFT</a>位置分邻域分标注</h3><p>在CRAFT中，图像分割的标注是一个连续的二维高斯分布，位于字符框中心的像素点有较高的位置分，而位于字符框边缘的像素点位置分较低，从而模型充分利用了像素点的位置信息。由于字符框通常为不规则的四边形，具体操作中，CRAFT需将二维标准高斯分布变换到字符框四边形中，如下图</p><p>获取邻域分标注时，我们首先将字符框四边形的对角线相连，如上图左侧Affinity Box Generation中蓝色实线所示。接着，我们分别找到上下两个三角形的重心（蓝色十字），两个相邻的字符共有四个三角形重心，我们将它们组成的四边形定为邻域框。最后，我们用之前位置分相同的方法，生成邻域框内的高斯分布，从而得到了邻域分。最终的结果可见上图最右侧的heat map</p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/craft1.jpg" alt="craft1"></p><h3 id="CRAFT弱监督学习"><a href="#CRAFT弱监督学习" class="headerlink" title="CRAFT弱监督学习"></a><a href="https://arxiv.org/abs/1904.01941" target="_blank" rel="noopener">CRAFT</a>弱监督学习</h3><p>如何从文本框标注获得可靠的字符框标注是本文的最大亮点。CRAFT采用了弱监督学习的方法，有效的解决了这个问题。在训练初期，我们使用的训练集为合成的非真实图片，合成图片中具有字符框准确的标注信息，因而可以直接使用。合成图片与真实图片的数据特征有相似之处但又不完全相同，其可以为模型训练提供有限的帮助。当模型具有一定预测能力后，我们再开始使用真实图片。</p><p>由于真实图片缺乏字符框标注，文章中采取了以下的训练方案：首先我们将文本行截取出来，利用当前训练好的模型预测每个像素点的位置分；接着从位置分的分布中，我们可以分离出来当前模型判断的字符框数量和位置，并利用这些字符框作为标注回头来训练模型。由于此时模型预测的字符框准确性并没有保证，在计入损失函数时，我们需要为对应的损失乘以一个置信概率。需要注意的是，实际的字符数量（文本标注长度）是已知的，未知的仅仅是字符框的位置。因此，我们可以利用预测和实际的字符数量的差来衡量预测的准确性，即置信概率=1-字符数差/实际字符数量。例如下图中，三个文本行的置信概率分别为6/6，5/7和5/6。需要注意的是，为了保证这种训练模式的有效性，作者在这一步训练中也掺入了较低比例（1:5）的具有准确字符框标注的合成图片</p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/craft2.jpg" alt="craft2"></p><h3 id="CRAFT文本框生成"><a href="#CRAFT文本框生成" class="headerlink" title="CRAFT文本框生成"></a><a href="https://arxiv.org/abs/1904.01941" target="_blank" rel="noopener">CRAFT</a>文本框生成</h3><p>在得到了所有像素点的位置分和邻域分以后，我们还需要将结果整合为最终的文本框作为输出。文章中作者对位置分和邻域分分别设定了一个阈值，将两者中至少有一方高于阈值的像素点标为1，其他标为0，然后将所有相连的值为1的像素定为一个文本目标。在需要矩形文本框的任务中，我们找到一个面积最小的包围整个目标的矩形作为输出；在需要多边形文本框的任务中，我们可以根据每个字符的中心点和宽度构造一个多边形，如下图所示</p><p>第一步是沿扫描方向寻找特征区域的局部极大值线，蓝线，连接所有局部极大值中心点的线称为中心线，黄线，然后，旋转局部max-ima线，使其垂直于中心线，以反映字符的倾斜角度，红色箭头，局部极大值线的端点是文本多边形控制点的候选点</p><p>如何从文本框标注获得可靠的字符框标注是本文的最大亮点，感兴趣的童鞋可以点击<a href="https://zhuanlan.zhihu.com/p/141865260" target="_blank" rel="noopener">detail</a></p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/craft3.jpg" alt="craft3"></p><h3 id="CRNN网络结构"><a href="#CRNN网络结构" class="headerlink" title="CRNN网络结构"></a><a href="https://arxiv.org/abs/1507.05717" target="_blank" rel="noopener">CRNN</a>网络结构</h3><ul><li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>: 残差网络</li><li><a href="https://zhuanlan.zhihu.com/p/32085405" target="_blank" rel="noopener">LSTM</a>: 循环神经网络</li><li><a href="https://dl.acm.org/doi/abs/10.1145/1143844.1143891" target="_blank" rel="noopener">CTC</a>: 解决输入输出对齐问题，详情请见<a href="https://zhuanlan.zhihu.com/p/42719047" target="_blank" rel="noopener">detail</a></li></ul><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/crnn.jpg" alt="crnn" style="zoom:50%;"></p><h2 id="cnstd-cnocr"><a href="#cnstd-cnocr" class="headerlink" title="cnstd+cnocr"></a><a href="https://github.com/breezedeus/cnstd" target="_blank" rel="noopener">cnstd</a>+<a href="https://github.com/breezedeus/cnocr" target="_blank" rel="noopener">cnocr</a></h2><ol><li>使用<a href="https://arxiv.org/abs/1903.12473" target="_blank" rel="noopener">PSENet</a>进行text detection</li><li>再使用<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Densenet</a>+<a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">GRU</a>进行text recognition</li></ol><h3 id="PSENet网络结构"><a href="#PSENet网络结构" class="headerlink" title="PSENet网络结构"></a><a href="https://arxiv.org/abs/1903.12473" target="_blank" rel="noopener">PSENet</a>网络结构</h3><p>骨干网络：<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>，预测多个分割结果，分别是$S1,…,Sn$，代表不同等级面积的结果，$S1$最小，基本就是文本骨架，$Sn$最大</p><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/PSENet1.jpg" alt="PSENet1"></p><h3 id="PSENet算法主体"><a href="#PSENet算法主体" class="headerlink" title="PSENet算法主体"></a><a href="https://arxiv.org/abs/1903.12473" target="_blank" rel="noopener">PSENet</a>算法主体</h3><p>如下图所示：</p><ol><li>网络有$S1,S2,S3$三个分割结果</li><li>先用最小的kernel生成的$S1$来区分四个文本实例</li><li>然后再逐步扩张成$S2,S3$</li></ol><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/PSENet2.jpg" alt="PSENet2"></p><h3 id="PSENet标注生成"><a href="#PSENet标注生成" class="headerlink" title="PSENet标注生成"></a><a href="https://arxiv.org/abs/1903.12473" target="_blank" rel="noopener">PSENet</a>标注生成</h3><ul><li>根据已有多边形框标注计算</li><li>$d_i=\frac{Area(p_n)\times (1-r_i^2)}{Perimeter(p_n)}$</li><li>$r_i = 1-\frac{(1-m)\times (n-i)}{n-1}$</li><li>$Area$: 面积</li><li>$Perimeter$: 周长</li><li>$r_i$: 缩放比例</li></ul><p><img src="/2021/03/11/shen-du-xue-xi-zhi-tu-xiang-shi-bie/PSENet3.jpg" alt="PSENet3"></p><h3 id="CRNN网络结构-1"><a href="#CRNN网络结构-1" class="headerlink" title="CRNN网络结构"></a><a href="https://arxiv.org/abs/1507.05717" target="_blank" rel="noopener">CRNN</a>网络结构</h3><ul><li><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Densenet</a>: 互相连接所有的层</li><li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">GRU</a>: Gate Recurrent Unit</li><li><a href="https://dl.acm.org/doi/abs/10.1145/1143844.1143891" target="_blank" rel="noopener">CTC</a></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li>[MMOCR Documents]: <a href="https://mmocr.readthedocs.io/en/latest/modelzoo.html" target="_blank" rel="noopener">https://mmocr.readthedocs.io/en/latest/modelzoo.html</a></li><li><a href="https://github.com/drsanwujiang/video-subtitle-recognize" target="_blank" rel="noopener">参考opencv实现视频切割</a></li></ul><h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="视频截帧成一张张图片"><a href="#视频截帧成一张张图片" class="headerlink" title="视频截帧成一张张图片"></a>视频截帧成一张张图片</h2><h3 id="使用opencv"><a href="#使用opencv" class="headerlink" title="使用opencv"></a>使用<code>opencv</code></h3><ul><li><a href="https://github.com/drsanwujiang/video-subtitle-recognize" target="_blank" rel="noopener">参考opencv实现视频切割</a></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python"># -*- coding:utf-8 _*-"""Author:Email: Date: 2021/03/12File: opencv_test.pySoftware: PyCharmDescription: 使用opencv将视频切成若干张静态图片（默认每秒1张）"""# load modulesimport shutilimport timeimport cv2import osimport sys# config类class Config:    Maps = {        # 以下请根据需要调整数值        "split_duration": 1.5,  # 切片间隔,每 split_duration 秒输出一帧        "jpg_quality": 40,  # 图片输出质量, 0~100        "probability": 0.66,  # OCR可信度下限, 0~1        "subtitle_top_rate": 0.66,  # 字幕范围倍率        "remove_duplicate": False,  # 强制去重        # 目录信息,在下方定义        "video_dir": "",        "video_path": "",        "video_frames": "",        "image_dir": "",        "output_dir": "",        # 视频信息,自动生成        "video_name": "",        "video_suffix": "",        "video_width": 0,        "video_height": 0,        "subtitle_top": 0,  # 字幕范围 = 字幕范围倍率 * 视频高度,此高度以下的文字被认为是字幕    }    @staticmethod    def set_path(video_name="", video_suffix=""):        current_path = sys.path[0]        Config.Maps["video_dir"] = '%s/video/' % current_path  # 视频源文件目录        Config.Maps["video_path"] = '%s/video/%s%s' % (current_path, video_name, video_suffix)  # 指定视频文件路径        Config.Maps["video_frames"] = '%s/video_frames/' % current_path  # 视频切片文件目录        Config.Maps["image_dir"] = '%s/video_frames/%s/' % (current_path, video_name)  # 指定视频切片文件目录        Config.Maps["output_dir"] = '%s/output/' % current_path  # 字幕输出目录        Config.Maps["video_name"] = video_name        Config.Maps["video_suffix"] = video_suffix    @staticmethod    def set_video_props(video_width, video_height):        Config.Maps["video_width"] = video_width        Config.Maps["video_height"] = video_height        Config.Maps["subtitle_top"] = Config.Maps["subtitle_top_rate"] * video_height    @staticmethod    def get_value(key):        return Config.Maps[key]# getFrame类class GetFrames:    @staticmethod    def main():        # 读取路径信息        video_path = Config.get_value("video_path")        image_dir = Config.get_value('image_dir')        jpg_quality = Config.get_value('jpg_quality')        split_duration = Config.get_value('split_duration')        if (os.path.exists(image_dir)):            shutil.rmtree(image_dir)  # 递归删除目录，os.rmdir只能删除空目录        os.mkdir(image_dir)        cv = cv2.VideoCapture(video_path)  # 读入视频文件        current_frame = 1        saved_frames = 1        if cv.isOpened():  # 判断是否正常打开            retval, frame = cv.read()        else:            cv.release()            print("Video open error")            return False        duration = int(cv.get(cv2.CAP_PROP_FPS) * split_duration)  # 间隔频率 = 帧率 * 切片时间间隔(四舍五入)        frame_count = int(cv.get(cv2.CAP_PROP_FRAME_COUNT))  # 视频总帧数        video_width = int(cv.get(cv2.CAP_PROP_FRAME_WIDTH))  # 视频宽度        video_height = int(cv.get(cv2.CAP_PROP_FRAME_HEIGHT))  # 视频高度        Config.set_video_props(video_width, video_height)        while retval:  # 循环读取视频帧            retval, frame = cv.read()            if current_frame % duration == 0:  # 每 duration 帧进行存储操作                cv2.imencode('.jpg', frame, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])[1]. \                    tofile(image_dir + str(current_frame).zfill(6) + '.jpg')                print(("Now: frame %d, saved: %d frame(s), process: %d%%" %                       (current_frame, saved_frames, (current_frame * 100) // frame_count)).ljust(60, ' '))                saved_frames += 1            current_frame += 1            cv2.waitKey(1)        print(("Now: frame %d, saved: %d frame(s), process: %d%%" %               (current_frame, saved_frames, (current_frame * 100) // frame_count)).ljust(60, ' '))        cv.release()        print("\nSaved: %d frame(s)" % saved_frames)        return True# 主类class Main:    @staticmethod    def clear():        if sys.platform.find("win") > -1:            os.system("cls")        else:            print()    @staticmethod    def main():        # 配置路径        Config.set_path()        if not (os.path.exists(Config.get_value('video_dir'))):            os.mkdir(Config.get_value('video_dir'))        if not (os.path.exists(Config.get_value('video_frames'))):            os.mkdir(Config.get_value('video_frames'))        if not (os.path.exists(Config.get_value('output_dir'))):            os.mkdir(Config.get_value('output_dir'))        # 列出所有video        Main.clear()        print("\n")        print("-"*40)        print("List Video")        print("-" * 40)        video_list = os.listdir(Config.get_value('video_dir'))        print(video_list)        if len(video_list) < 1:            print("Nothing found\n\n")            print("Process finished")            input()            return        # 对所有video进行处理        start_all = time.time()        print("\n")        print("-" * 40)        print("All Video Division")        print("-" * 40)        print("Start All video division")        for video in video_list:            print("%d.%s" % (video_list.index(video) + 1, video))            video_name = video[: video.rfind(".")]            video_suffix = video[video.rfind("."):]            Config.set_path(video_name, video_suffix)            start = time.time()            print("\n")            print("-" * 40)            print("Video: %s Division" % video)            print("-" * 40)            print("Start video division")            if not GetFrames.main():                print("Video division FAILED!")                print("Process finished")                input()                return            print("Video: %s division finished" % video)            print("Time: %.2fs\n" % (time.time() - start))        print("All Video division finished")        print("Time: %.2fs\n" % (time.time() - start_all))        returnif __name__ == "__main__":    Main.main()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://github.com/drsanwujiang/video-subtitle-recognize" target="_blank" rel="noopener">opencv实现视频切割</a><ol><li><a href="https://www.kancloud.cn/aollo/aolloopencv" target="_blank" rel="noopener">OpenCV-Python中文教程</a></li></ol></li><li><a href="https://www.ruanyifeng.com/blog/2020/01/ffmpeg.html" target="_blank" rel="noopener">FFmpeg视频处理</a></li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/chineseocr/darknet-ocr">darknet-ocr</a>：<code>star:851</code></li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/chineseocr/chineseocr">chineseocr</a>：<code>star:3.9k</code></li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/ouyanghuiyu/chineseocr_lite">https://github.com/ouyanghuiyu/chineseocr_litegithub.com</a>：<code>star:6.4k</code>，支持windows<ol><li><a href="https://zhuanlan.zhihu.com/p/111533615" target="_blank" rel="noopener">机器之心：实测超轻量中文OCR开源项目，总模型仅17Mzhuanlan.zhihu.com)</a></li></ol></li><li>包括AlexNet、RCNN、ResNet、YOLO、SSD等。</li><li><a href="https://github.com/hwalsuklee/awesome-deep-text-detection-recognition" target="_blank" rel="noopener">文本检测的资源汇总</a><ol><li><a href="https://zhuanlan.zhihu.com/p/71028209" target="_blank" rel="noopener">Github：深度学习文本检测识别（OCR）精选资源汇总</a></li></ol></li><li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">xiaofengshi：chinese-ocr</a>：<code>star:2.4k</code></li><li><a href="https://zhuanlan.zhihu.com/p/34757009" target="_blank" rel="noopener">场景文字检测—CTPN原理与实现</a><ol><li><a href="https://github.com/yizt/cv-papers/blob/master/CTPN.md" target="_blank" rel="noopener">CTPN论文</a></li></ol></li><li><a href="https://zhuanlan.zhihu.com/p/43534801" target="_blank" rel="noopener">一文读懂CRNN+CTC文字识别</a></li><li><a href="https://zhuanlan.zhihu.com/p/55118990" target="_blank" rel="noopener">视频标签算法解析</a></li><li><a href="https://github.com/topics/text-detection" target="_blank" rel="noopener">text-detection</a>：文字识别相关topic</li><li><a href="https://zhuanlan.zhihu.com/p/142455144" target="_blank" rel="noopener">多模态研究综述</a></li><li><a href="https://github.com/JaidedAI/EasyOCR" target="_blank" rel="noopener">easyocr</a><ol><li>解码器：<a href="https://zhuanlan.zhihu.com/p/157966981" target="_blank" rel="noopener">链接</a></li></ol></li><li><a href="https://github.com/topics/ocr?o=desc&amp;s=stars" target="_blank" rel="noopener">github ocr topics 排行榜</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP之TextRank</title>
      <link href="/2021/03/10/nlp-zhi-textrank/"/>
      <url>/2021/03/10/nlp-zhi-textrank/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>如何对一篇文章的关键词进行提取</li></ul><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><ul><li>使用类似网页排名算法<code>PageRank</code>的思路</li><li>构建词与词之间的图，然后迭代计算词的排名</li></ul><h2 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a><code>TextRank</code></h2><ul><li>回顾<code>PageRank</code>的计算公式：$P=(1-d)\frac{I}{n}+dA^TP$</li><li>直接说<code>TextRank</code>的计算公式：$P=(1-d)\frac{I}{n}+dW^TP$<ul><li>其中$W=(w_{ij})_{m\times n}$为词与词之间的权重，一般为词$i$与词$j$在滑动窗口$k$内的共现次数</li></ul></li></ul><h2 id="如何根据词构建图"><a href="#如何根据词构建图" class="headerlink" title="如何根据词构建图"></a>如何根据词构建图</h2><ol><li><p>对文章$S$进行分词，得到词列表</p></li><li><p>设定滑动窗口$k$的大小，统计滑动窗口内各词对的贡献次数</p><ol><li><p>例如：<code>淡黄的长裙，蓬松的头发</code>，分词后为[<code>淡黄</code>, <code>长裙</code>, <code>蓬松</code>, <code>头发</code>]</p></li><li><p>设定滑动窗口$k=2$，则得到词对：</p><ol><li><code>淡黄</code>,<code>长裙</code></li><li><code>长裙</code>,<code>蓬松</code></li><li><code>蓬松</code>,<code>头发</code></li></ol></li><li><p>根据这些词对构建<code>无向图</code>，注意<code>PageRank</code>是<code>有向图</code></p><p><img src="/2021/03/10/nlp-zhi-textrank/image-20210310171957155.png" alt="图片来源于参考链接1"></p></li><li><p>然后使用公式计算</p></li></ol></li></ol><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ul><li><p><a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">Jieba</a></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" target="_blank" rel="noopener">TextRank: Bringing Order into Texts</a></li><li><a href="https://zhuanlan.zhihu.com/p/41091116" target="_blank" rel="noopener">（九）通俗易懂理解——TF-IDF与TextRank</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关键词提取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之PageRank</title>
      <link href="/2021/03/10/ji-qi-xue-xi-zhi-pagerank/"/>
      <url>/2021/03/10/ji-qi-xue-xi-zhi-pagerank/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/03/10/ji-qi-xue-xi-zhi-pagerank/image-20210621160912259.png" alt="image-20210621160912259"></p><h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>如何给众多网页排名？</li></ul><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><ul><li>一个解决办法：</li><li>当很多网页都链接到同一个网页时，那这个网页应该排名靠前</li><li>链接这个网页的众多网页中，排名越靠前的网页，其重要性应该更高</li></ul><h2 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a><code>PageRank</code></h2><script type="math/tex; mode=display">PR_i = \sum_{(j,i) \in V}{\frac{PR_j}{O_j}}</script><ul><li>$PR_i$表示第$i$个网页的<code>PageRank</code>值，代表其网页排名依据</li><li>$V$是所有网页集合</li><li>$O_j$是第$j$个网页所存在的外链数，越多外链，它给$i$网页投票对应重要性越低</li></ul><h2 id="使用图构建PageRank"><a href="#使用图构建PageRank" class="headerlink" title="使用图构建PageRank"></a>使用图构建<code>PageRank</code></h2><ul><li><p>网页对应图的节点<code>node</code></p></li><li><p>网页$i$的外链有网页$j$代表$(i,j)$，即图的有向边<code>edge</code></p></li><li><p>如下图所示：</p><p><img src="/2021/03/10/ji-qi-xue-xi-zhi-pagerank/image-20210310155933176.png" alt="图片来源参考链接"></p></li><li><p>表示为$G(V,E)$，上图的邻接矩阵表示为：</p><script type="math/tex; mode=display">A = \left( \begin{array} {ccc}     0 & 1/2 & 1/2 & 0 & 0 & 0 \\     1/2 & 0 & 1/2 & 0 & 0 & 0 \\     0 & 1 & 0 & 0 & 0 & 0 \\     0 & 0 & 1/3 & 0 & 1/3 & 1/3 \\     0 & 0 & 0 & 0 & 0 & 0 \\     0 & 0 & 0 & 1/2 & 1/2 & 0 \\ \end{array} \right)</script></li><li><p>用矩阵表示<code>式(1)</code>：$P=A^TP$</p></li><li><p>其中$P=[PR_1, PR_2, …, PR_n]^T$</p></li></ul><h2 id="如何求解"><a href="#如何求解" class="headerlink" title="如何求解"></a>如何求解</h2><ul><li><p>先给每个网页给定一个初始$PR$值，一般都是$1/n$</p></li><li><p>然后使用$P=A^TP$进行迭代，直到$||P^k - P^{k-1}||&lt; \epsilon$，这是一个马尔可夫收敛过程</p></li><li><p>但是需要邻接矩阵$A$满足下面3个条件上式才有解：</p><ol><li>转移概率矩阵<code>(stochastic matrix)</code>：即每行至少有一个非零值</li><li>不可约性：矩阵A对应的有向图必须是强连通的，即任意两个节点，存在一条路径可达</li><li>非周期性：即存在一个最小正整数$k$，使得从某节点$i$出发又回到状态$i$的所有路径长度是$k$的整数倍，这就需要阻尼系数<code>d</code></li></ol></li><li><p>一般情况下矩阵$A$不满足上述三个条件，故需要对求解式作平滑处理，最终等式如下：</p><script type="math/tex; mode=display">P = \big ((1-d)\frac{I}{n} + dA^TP \big )</script></li><li><p>其中：$d \in [0,1],I\text{是单位阵}$</p></li></ul><h2 id="最终求解式"><a href="#最终求解式" class="headerlink" title="最终求解式"></a>最终求解式</h2><script type="math/tex; mode=display">P = \big ((1-d)\frac{I}{n} + dA^TP \big )</script><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul><li>缺点：旧的页面的排名往往会比新页面高<ul><li>因为即使是质量很高的新页面也往往不会有很多外链，除非它是某个已经存在站点的子站点。这也是<code>PageRank</code>需要多项算法结合以保证其结果的准确性的原因</li></ul></li></ul><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.cnblogs.com/en-heng/p/6124526.html" target="_blank" rel="noopener">【十大经典数据挖掘算法】PageRank</a></li><li><a href="https://zh.wikipedia.org/wiki/PageRank" target="_blank" rel="noopener">维基百科</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据挖掘 </tag>
            
            <tag> 十大算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP之TF-IDF</title>
      <link href="/2021/03/10/nlp-zhi-tf-idf/"/>
      <url>/2021/03/10/nlp-zhi-tf-idf/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>一篇文章中如何提取出能代表这篇文章的关键词</li></ul><h2 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h2><ul><li>一个解决方案是提取出该篇文章中<code>高频词</code></li><li>但是<code>高频词</code>可能是那些<code>的，是</code>等词，所以要剔除这些词，这些词被称为<code>停用词(stop words)</code></li><li>还有就是类似<code>中国</code>等词高频，但是可能不是文章中的关键词，因为这种词在很多文章中都会出现，所以需要针对高频词作一个重要性调整，称为重要性调整系数</li><li>那如果一个词高频，且比较少见，那这种词最可能反映该文章特性</li><li>这就对应了<code>TF-IDF</code>算法</li></ul><h2 id="TF-IDF算法"><a href="#TF-IDF算法" class="headerlink" title="TF-IDF算法"></a><code>TF-IDF</code>算法</h2><h3 id="TF"><a href="#TF" class="headerlink" title="TF"></a><code>TF</code></h3><ul><li>$\text{词频}(TF)=\frac{\text{某个词在文章中的出现次数}}{\text{文章的总词数}}$</li><li>或者</li><li>$\text{词频}(TF)=\frac{\text{某个词在文章中的出现次数}}{\text{该文出现次数最多的词的出现次数}}$</li></ul><h3 id="IDF"><a href="#IDF" class="headerlink" title="IDF"></a><code>IDF</code></h3><ul><li>$\text{逆文档频率}(IDF)=\log{\frac{\text{语料库的文档总数}}{\text{包含该词的文档数}+1}}$</li></ul><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a><code>TF-IDF</code></h3><ul><li>$TF-IDF=TF \times IDF$</li></ul><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ol><li>优点：简单快速，结果比较符合实际情况</li><li>缺点：<ol><li>单纯以<code>词频</code>衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多</li><li>而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）</li></ol></li></ol><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h2 id="计算工具"><a href="#计算工具" class="headerlink" title="计算工具"></a>计算工具</h2><ol><li><p><a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">Jieba</a></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">import jieba.analysejieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关键词提取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP之分词原理</title>
      <link href="/2021/03/09/nlp-zhi-fen-ci-yuan-li/"/>
      <url>/2021/03/09/nlp-zhi-fen-ci-yuan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="分词的基本原理"><a href="#分词的基本原理" class="headerlink" title="分词的基本原理"></a>分词的基本原理</h2><ul><li><p>基于统计的分词，统计的样本内容来自一些标准的语料库</p></li><li><p>举个栗子，我们分词的时候希望~小爱/来到/天河/区~这个分词后句子出现的概率要比~小爱/来到/天/河/区~的大</p></li><li><p>上述例子用数学语言表示如下</p></li><li><p>如果有一个句子<code>S</code>，它有<code>m</code>中分词选项如下：</p><script type="math/tex; mode=display">A_{11}A_{12}...A_{1n_1}  \\A_{21}A_{22}...A_{2n_2}  \\...  \\A_{m1}A_{m2}...A_{mn_m}</script></li><li><p>其中$n_i$代表第$i$种分词的词个数，我们希望从中选出的是：$r = \underbrace{arg\;max}_iP(A_{i1},A_{i2},…,A_{in_i})$</p></li><li><p>但是$P(A_{i1},A_{i2},…,A_{in_i})$不好求，故使用马尔可夫假设，假设当前词只与其前一个词有关，则：$P(A_{i1},A_{i2},…,A_{in_i}) = P(A_{i1})P(A_{i2}|A_{i1})P(A_{i3}|A_{i2})…P(A_{in_i}|A_{i(n_i-1)})$</p></li><li><p>通过标准语料库，可以近似计算所有分词的二元条件概率，即：</p><script type="math/tex; mode=display">P(w_2|w_1) = \frac{P(w_1,w_2)}{P(w_1)} \approx \frac{freq(w_1,w_2)}{freq(w_1)}  \\P(w_1|w_2) = \frac{P(w_2,w_1)}{P(w_2)} \approx \frac{freq(w_1,w_2)}{freq(w_2)}</script></li><li><p><code>N</code>元模型实际应用中会出现一些问题：</p><ul><li>某些生僻词，或者相邻分词联合分布在语料库中没有，概率为0。这种情况我们一般会使用拉普拉斯平滑，即给它一个较小的概率值</li><li>第二个问题是如果句子长，分词有很多情况，计算量也非常大，这时我们可以用下一节==维特比算法==来优化算法时间复杂度。</li></ul></li><li><p><code>N</code>元模型即假设当前词与其前<code>N</code>个词相关</p></li></ul><h2 id="使用维特比算法分词"><a href="#使用维特比算法分词" class="headerlink" title="使用维特比算法分词"></a>使用维特比算法分词</h2><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><ul><li><p>输入：<code>S=</code>~人生如梦境~，它在语料库中可能的概率图如下：</p><p><img src="/2021/03/09/nlp-zhi-fen-ci-yuan-li/image-20210309171556507.png" alt="图片来源：文本挖掘的分词原理"></p></li><li><p>使用维特比算法如下：</p></li><li><p>首先初始化有：$\delta(人) = 0.26\;\;\Psi(人)=Start\;\;\delta(人生) = 0.44\;\;\Psi(人生)=Start$</p></li><li><p>对节点<code>生</code>：$\delta(生) = \delta(人)P(生|人) = 0.0442 \;\; \Psi(生)=人$</p></li><li><p>对节点<code>如</code>：$\delta(如) = max\{\delta(生)P(如|生)，\delta(人生)P(如|人生)\} = max\{0.01680, 0.3168\} = 0.3168 \;\; \Psi(如) = 人生$</p></li><li><p>其他节点类似如下：</p><script type="math/tex; mode=display">\delta(如梦) = \delta(人生)P(如梦|人生) = 0.242 \;\; \Psi(如梦)=人生  \\\delta(梦) = \delta(如)P(梦|如) = 0.1996 \;\; \Psi(梦)=如  \\\delta(境) = max\{\delta(梦)P(境|梦) ,\delta(如梦)P(境|如梦)\}= max\{0.0359, 0.0315\} = 0.0359 \;\; \Psi(境)=梦  \\\delta(梦境) = \delta(如)P(梦境|如) = 0.1616 \;\; \Psi(梦境)=如  \\\delta(End) = max\{\delta(梦境)P(End|梦境), \delta(境)P(End|境)\} = max\{0.0396, 0.0047\} = 0.0396\;\;\Psi(End)=梦境 \\\Psi(End)=梦境 \to \Psi(梦境)=如 \to \Psi(如)=人生 \to \Psi(人生)=start</script></li><li><p>从而最终分词结果为~人生/如/梦境~</p></li></ul><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="疑难"><a href="#疑难" class="headerlink" title="疑难"></a>疑难</h1><h2 id="常用中文分词工具"><a href="#常用中文分词工具" class="headerlink" title="常用中文分词工具"></a>常用中文分词工具</h2><ol><li><a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">Jieba</a>：<code>star:25.7k</code>做最好的 Python 中文分词组件</li><li><a href="https://github.com/isnowfy/snownlp" target="_blank" rel="noopener">SnowNLP</a>：<code>star:5.3k</code>Simplified Chinese Text Processing</li><li><a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">pkuseg</a>：<code>star:5.3k</code>一个多领域中文分词工具包</li><li><a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">THULAC</a>：<code>star:1.5k</code>一个高效的中文词法分析工具包</li><li><a href="https://www.52nlp.cn/python%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d%e5%b7%a5%e5%85%b7-%e5%90%88%e9%9b%86-%e5%88%86%e8%af%8d%e5%ae%89%e8%a3%85-%e5%88%86%e8%af%8d%e4%bd%bf%e7%94%a8-%e5%88%86%e8%af%8d%e6%b5%8b%e8%af%95" target="_blank" rel="noopener">其他</a>：详情请见</li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://www.cnblogs.com/pinard/p/6677078.html" target="_blank" rel="noopener">文本挖掘的分词原理</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分词 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP之HMM</title>
      <link href="/2021/03/03/nlp-zhi-hmm/"/>
      <url>/2021/03/03/nlp-zhi-hmm/</url>
      
        <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="什么样的问题可以用HMM模型"><a href="#什么样的问题可以用HMM模型" class="headerlink" title="什么样的问题可以用HMM模型"></a>什么样的问题可以用HMM模型</h2><ul><li>可以用HMM的问题具备两个特征<ul><li>问题是基于序列的，例如：时间序列、状态序列等</li><li>问题中有两类数据，可以观测的观测序列和无法观测的隐藏序列</li></ul></li><li>举个栗子<ol><li>打字的时候，键盘打出来的一系列字符是可以观测的观测序列，而我脑子里想通过这些字符写的一段话是没法观测的隐藏序列</li><li>说话的时候，声音是观测序列，而实际想表达的是隐藏序列，得通过声音来判断出想表达的隐藏序列</li></ol></li></ul><h2 id="HMM模型定义"><a href="#HMM模型定义" class="headerlink" title="HMM模型定义"></a>HMM模型定义</h2><h3 id="举个栗子说明"><a href="#举个栗子说明" class="headerlink" title="举个栗子说明"></a>举个栗子说明</h3><ul><li>栗子来源于《统计学习方法=李航》</li><li>栗子如下：</li></ul><p>假设有3个盒子，每个盒子有多个红白两种球，数量如下表：</p><div class="table-container"><table><thead><tr><th>盒子</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>红</td><td>5</td><td>4</td><td>7</td></tr><tr><td>白</td><td>5</td><td>6</td><td>3</td></tr></tbody></table></div><p>有放回抽球3次，规则如下：</p><ol><li>第一次从盒1抽的概率0.2，盒2、盒3均是0.4</li><li>接下来每一次的抽取规则：<ol><li>如果当前抽的盒1，则按0.5、0.2、0.3进行盒1、2、3抽取</li><li>如果当前抽的盒2，则按0.3、0.5、0.2进行盒1、2、3抽取</li><li>如果当前抽的盒3，则按0.2、0.3、0.5进行盒1、2、3抽取</li></ol></li><li>抽3次，得到观测序列：$O=\{\text{红、白、红}\}$，而隐藏序列是$Q=\{\text{盒子1、2、3}\}$，因为不知道每个球是从哪个盒子抽出的</li></ol><p>从上述规则可以提取出：</p><ol><li>观测序列集合$V=\{\text{红、白}\},M=2$</li><li>隐藏序列集合$Q=\{\text{盒子1、2、3}\},N=3$</li><li>隐藏序列初始状态分布$\Pi =(0.2,0.4,0.4)^T$</li><li>隐藏序列转移分布矩阵$A = \left( \begin{array} {ccc} 0.5 &amp; 0.2 &amp; 0.3 \\ 0.3 &amp; 0.5 &amp; 0.2 \\ 0.2 &amp; 0.3 &amp;0.5 \end{array} \right)$</li><li>观测序列概率矩阵$B = \left( \begin{array} {ccc} 0.5 &amp; 0.5 \\ 0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 \end{array} \right)$，行是盒1、2、3，列是红、白</li></ol><h3 id="通俗定义"><a href="#通俗定义" class="headerlink" title="通俗定义"></a>通俗定义</h3><ul><li>设$Q = \{q_1,q_2,…,q_N\}, \; V =\{v_1,v_2,…v_M\}$分别为隐藏序列集合、可观测序列集合</li><li>设$I = \{i_1,i_2,…,i_T\}, \; O =\{o_1,o_2,…o_T\}$分别为隐藏序列列表、可观测序列列表</li><li>其中任意一个$i_t \in Q$，任意一个$o_t \in V$</li></ul><h4 id="两个重要假设-一个隐藏序列集合初始分布"><a href="#两个重要假设-一个隐藏序列集合初始分布" class="headerlink" title="两个重要假设+一个隐藏序列集合初始分布"></a>两个重要假设+一个隐藏序列集合初始分布</h4><ol><li>齐次马尔科夫链假设，即$i_t$只依赖于$i_{t-1}$，<ul><li>这样转移概率矩阵表示成$A=\Big [a_{ij}\Big ]_{N \times N}$，其中$a_{ij} = P(i_{t+1} = q_j | i_t= q_i)$</li></ul></li><li>观测独立性假设，即任意时刻的观测序列值仅依赖当前时刻的隐藏序列值<ul><li>这样观测序列概率矩阵表示成$B = \Big [b_j(k) \Big ]_{N \times M}$，其中$b_j(k) = P(o_t = v_k | i_t= q_j)$</li></ul></li><li>除此之外需要一组在时刻$t=1$的隐藏序列集合的概率分布$\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)$</li></ol><h4 id="HMM定义"><a href="#HMM定义" class="headerlink" title="HMM定义"></a>HMM定义</h4><ul><li>则HMM表示为$\lambda = (A, B, \Pi)$</li></ul><h2 id="HMM观测序列的生成"><a href="#HMM观测序列的生成" class="headerlink" title="HMM观测序列的生成"></a>HMM观测序列的生成</h2><ul><li>输入：$\lambda = (A, B, \Pi)$</li><li>输出：$O =\{o_1,o_2,…o_T\}$，长度为$T$</li><li>生成步骤：<ol><li>根据初始状态概率分布$\Pi$生成隐藏状态$i_1$</li><li>for t in range(1, T+1)<ol><li>按照隐藏状态$i_t$的观测状态分布$b_{i_t}(k)$生成观测状态$o_t$</li><li>按照隐藏状态$i_t$的转移概率分布$a_{i_t,i_{t+1}}$生成隐藏状态$i_{t+1}$</li></ol></li></ol></li></ul><h2 id="HMM模型需要解决的三个问题"><a href="#HMM模型需要解决的三个问题" class="headerlink" title="HMM模型需要解决的三个问题"></a>HMM模型需要解决的三个问题</h2><ol><li>评估观测序列概率，给定模型和观测序列，计算观测序列出现的概率，前向后向算法</li><li>模型参数学习，给定观测序列，估计模型的参数使观测序列出现的概率最大，基于EM算法的鲍姆-韦尔奇算法</li><li>预测，给定模型和观测序列，求给定观测序列后，最可能出现的隐藏序列，基于动态规划的维特比算法</li></ol><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h3 id="问题一：评估观测序列概率"><a href="#问题一：评估观测序列概率" class="headerlink" title="问题一：评估观测序列概率"></a>问题一：评估观测序列概率</h3><h4 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h4><h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol><li>我们可以列举出所有可能出现的长度为$T$的隐藏序列$I = \{i_1,i_2,…,i_T\}$，任意一个隐藏序列$I = \{i_1,i_2,…,i_T\}$出现的概率为：$P(I|\lambda) = \pi_{i_1} a_{i_1i_2} a_{i_2i_3}… a_{i_{T-1}\;\;i_T}$</li><li>对于固定的隐藏序列$I = \{i_1,i_2,…,i_T\}$，我们要求的观测序列$O =\{o_1,o_2,…o_T\}$出现的概率为：$P(O|I, \lambda) = b_{i_1}(o_1)b_{i_2}(o_2)…b_{i_T}(o_T)$</li><li>则$O,T$的联合概率为：$P(O,I|\lambda) = P(I|\lambda)P(O|I, \lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)…a_{i_{T-1}\;\;i_T}b_{i_T}(o_T)$</li><li>最后求观测序列$O$在模型$\lambda$下出现的条件概率$P(O|\lambda)$为：$P(O|\lambda) = \sum\limits_{I}P(O,I|\lambda)  = \sum\limits_{i_1,i_2,…i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)…a_{i_{T-1}\;\;i_T}b_{i_T}(o_T)$</li></ol><h5 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h5><ul><li>暴力求解太耗时，当隐藏状态数$N$太大时</li><li>时间复杂度：$O(TN^T)$</li></ul><h4 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h4><ul><li>前向概率定义：$\alpha_t(i) = P(o_1,o_2,…o_t, i_t =q_i | \lambda)$</li></ul><h5 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h5><ol><li>计算时刻1的各个隐藏状态前向概率：$\alpha_1(i) = \pi_ib_i(o_1),\; i=1,2,…N$</li><li>from t=2 to T：<ol><li>计算时刻$t$的各个隐藏状态前行概率：$\alpha_{t+1}(i) = \Big[\sum\limits_{j=1}^N\alpha_t(j)a_{ji}\Big]b_i(o_{t+1}),\; i=1,2,…N$</li></ol></li><li>计算最终结果：$P(O|\lambda) = \sum\limits_{i=1}^N\alpha_T(i)$</li></ol><h5 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h5><ul><li>时间复杂度：$O(TN^2)$</li></ul><h4 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h4><ul><li>后向概率定义：$\beta_t(i) = P(o_{t+1},o_{t+2},…o_T| i_t =q_i , \lambda)$</li></ul><h5 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h5><ol><li>初始化时刻$T$各个隐藏状态后向概率：$\beta_T(i) = 1,\; i=1,2,…N$</li><li>from t=T-1 to 1：<ol><li>计算时刻$t$的后向概率：$\beta_{t}(i) = \sum\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j),\; i=1,2,…N$</li></ol></li><li>计算最终结果：$P(O|\lambda) = \sum\limits_{i=1}^N\pi_ib_i(o_1)\beta_1(i)$</li></ol><h5 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h5><ul><li>时间复杂度：$O(TN^2)$</li></ul><h3 id="问题二：模型参数学习：学习-A-B-lambda"><a href="#问题二：模型参数学习：学习-A-B-lambda" class="headerlink" title="问题二：模型参数学习：学习$A,B,\lambda$"></a>问题二：模型参数学习：学习$A,B,\lambda$</h3><h4 id="根据前向后向算法计算常用概率"><a href="#根据前向后向算法计算常用概率" class="headerlink" title="根据前向后向算法计算常用概率"></a>根据前向后向算法计算常用概率</h4><ol><li>给定模型$\lambda$和观测序列$O$，在时刻$t$处于状态$q_i$的概率为：$\gamma_t(i) = P(i_t = q_i | O,\lambda) = \frac{P(i_t = q_i ,O|\lambda)}{P(O|\lambda)}= \frac{ \alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N \alpha_t(j)\beta_t(j)}$</li><li>给定模型$\lambda$和观测序列$O$，在时刻$t$处于状态$q_i$，且时刻$t+1$处于状态$q_j$的概率为：$\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j | O,\lambda) = \frac{ P(i_t = q_i, i_{t+1}=q_j , O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum\limits_{r=1}^N\sum\limits_{s=1}^N\alpha_t(r)a_{rs}b_s(o_{t+1})\beta_{t+1}(s)}$</li><li>由1、2可得：<ol><li>在观测序列$O$下状态$i$出现的期望值为：$\sum\limits_{t=1}^T\gamma_t(i)$</li><li>在观测序列$O$下状态$i$转移的期望值为：$\sum\limits_{t=1}^{T-1}\gamma_t(i)$</li><li>在观测序列$O$下状态$i$转移到状态$j$的期望值为：$\sum\limits_{t=1}^{T-1}\xi_t(i,j)$</li></ol></li></ol><h4 id="求解模型参数：简单情况-隐藏序列已知"><a href="#求解模型参数：简单情况-隐藏序列已知" class="headerlink" title="求解模型参数：简单情况-隐藏序列已知"></a>求解模型参数：简单情况-隐藏序列已知</h4><p>已知$D$个长度为$T$的观测序列和对应的隐藏序列，即$\{(O_1, I_1), (O_2, I_2), …(O_D, I_D)\}$，求解如下：</p><ol><li>统计从隐藏状态$q_i$到$q_j$的频率计数为$A_{ij}$，那么可得隐藏状态转移矩阵$A$</li><li>统计样本隐藏状态为$q_j$且观测状态为$v_k$的频率计数为$B_{jk}$，则观测状态矩阵为$B$</li><li>统计样本初始隐藏状态为$q_i$的频率计数为$C(i)$，则初始概率分布为$\Pi$</li></ol><h4 id="求解模型参数：复杂情况-隐藏序列未知"><a href="#求解模型参数：复杂情况-隐藏序列未知" class="headerlink" title="求解模型参数：复杂情况-隐藏序列未知"></a>求解模型参数：复杂情况-隐藏序列未知</h4><p>已知$D$个长度为$T$的观测序列，即$\{(O_1), (O_2), …(O_D)\}$，</p><p><strong>鲍姆-韦尔奇算法</strong></p><ol><li>E步：求出联合分布$P(O,I|\lambda)$基于条件概率$P(I|O,\overline{\lambda})$的期望表达式：$L(\lambda, \overline{\lambda}) = \sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)$</li><li>M步：极大化上式，更新参数：$\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)$</li><li>不断迭代EM步，直到收敛</li></ol><p><strong>具体步骤</strong></p><ul><li><p>输入：$\{(O_1), (O_2), …(O_D)\}$</p></li><li><p>输出：HMM模型参数</p></li><li><p>步骤：</p><ol><li><p>随机初始化所有的$\Pi,A,B$</p></li><li><p>对于每个样本，用前向后向算法计算$\gamma_t^{(d)}(i)，\xi_t^{(d)}(i,j), t =1,2…T$</p></li><li><p>更新模型参数：</p><script type="math/tex; mode=display">\pi_i =  \frac{\sum\limits_{d=1}^D\gamma_1^{(d)}(i)}{D} \\a_{ij} = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}\xi_t^{(d)}(i,j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}\gamma_t^{(d)}(i)} \\b_{j}(k) = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1, o_t^{(d)}=v_k}^{T}\gamma_t^{(d)}(j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}\gamma_t^{(d)}(j)}</script></li><li><p>如果$\Pi,A,B$已经收敛，则算法结束，否则从第二步开始继续迭代</p></li></ol></li></ul><h3 id="问题三：预测-维特比算法"><a href="#问题三：预测-维特比算法" class="headerlink" title="问题三：预测-维特比算法"></a>问题三：预测-维特比算法</h3><h4 id="两个局部状态"><a href="#两个局部状态" class="headerlink" title="两个局部状态"></a>两个局部状态</h4><ol><li><p>在时刻$t$隐藏状态为$i$所有可能的状态转移路径$i_1,i_2,…i_t$中的概率最大值，记为$\delta_t(i) = \max_{i_1,i_2,…i_{t-1}}\;P(i_t=i, i_1,i_2,…i_{t-1},o_t,o_{t-1},…o_1|\lambda),\; i =1,2,…N$，且递推表达式为：</p><script type="math/tex; mode=display">\begin{align} \delta_{t+1}(i) & =  \max_{i_1,i_2,...i_{t}}\;P(i_{t+1}=i, i_1,i_2,...i_{t},o_{t+1},o_{t},...o_1|\lambda) \\ & = \max_{1 \leq j \leq N}\;[\delta_t(j)a_{ji}]b_i(o_{t+1})\end{align}</script></li><li><p>在时刻$t$隐藏状态为$i$所有单个状态转移路径$(i_1,i_2,…,i_{t-1},i)$中概率最大的转移路径中第$t-1$个节点的隐藏状态为$\Psi_t(i)$，其递推表达式为：</p><script type="math/tex; mode=display">\Psi_t(i) = arg \; \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}]</script></li></ol><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ul><li><p>输入：HMM模型$\lambda = (A,B,\Pi)$，观测序列$O=(o_1,o_2,…o_T)$</p></li><li><p>输出：最有可能的隐藏状态序列$I^<em>= \{i_1^</em>,i_2^<em>,…i_T^</em>\}$</p></li><li><p>步骤：</p><ol><li><p>初始化局部状态：</p><script type="math/tex; mode=display">\delta_1(i) = \pi_ib_i(o_1),\;i=1,2...N \\\Psi_1(i)=0,\;i=1,2...N</script></li><li><p>from t=2 to T：</p><script type="math/tex; mode=display">\delta_{t}(i) = \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}]b_i(o_{t}),\;i=1,2...N \\\Psi_t(i) = arg \; \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}],\;i=1,2...N</script></li><li><p>计算时刻$T$最大的$\delta_T(i)$以及对应的$\Psi_T(i)$，开始回溯得到最可能的隐藏状态序列$I^<em>= \{i_1^</em>,i_2^<em>,…i_T^</em>\}$</p></li></ol></li></ul><h4 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h4><ul><li><p>输入：</p><script type="math/tex; mode=display">\Pi = (0.2,0.4,0.4)^T  \\A = \left( \begin{array} {ccc} 0.5 & 0.2 & 0.3 \\ 0.3 & 0.5 & 0.2 \\ 0.2 & 0.3 &0.5 \end{array} \right)  \\B = \left( \begin{array} {ccc} 0.5 & 0.5 \\ 0.4 & 0.6 \\ 0.7 & 0.3 \end{array} \right)  \\\text{观测序列：}O=\{红，白，红\}</script></li><li><p>计算：</p><ul><li><p>观测状态为<code>红</code>：</p><script type="math/tex; mode=display">\delta_1(1) = \pi_1b_1(o_1) = 0.2 \times 0.5 = 0.1  \\\delta_1(2) = \pi_2b_2(o_1) = 0.4 \times 0.4 = 0.16  \\\delta_1(3) = \pi_3b_3(o_1) = 0.4 \times 0.7 = 0.28  \\\Psi_1(1)=\Psi_1(2) =\Psi_1(3) =0</script></li><li><p>然后观测状态为<code>白</code>：</p><script type="math/tex; mode=display">\delta_2(1) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j1}]b_1(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.5, 0.16 \times 0.3, 0.28\times 0.2] \times 0.5 = 0.028  \\\Psi_2(1)=\max_{1\leq j \leq 3}[0.1 \times 0.5, 0.16 \times 0.3, 0.28\times 0.2]=0.56, \text{对应的隐藏状态是3} \\\delta_2(2) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j2}]b_2(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.2, 0.16 \times 0.5, 0.28\times 0.3] \times 0.6 = 0.0504  \\\Psi_2(2)=3  \\\delta_2(3) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j3}]b_3(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.3, 0.16 \times 0.2, 0.28\times 0.5] \times 0.3 = 0.042  \\\Psi_2(3)=3</script></li><li><p>然后观测状态为<code>红</code>：</p><script type="math/tex; mode=display">\delta_3(1) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j1}]b_1(o_3) = \max_{1\leq j \leq 3}[0.028 \times 0.5, 0.0504 \times 0.3, 0.042\times 0.2] \times 0.5 = 0.00756  \\\Psi_3(1)=2 \\\delta_3(2) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j2}]b_2(o_3) = \max_{1\leq j \leq 3}[0.028  \times 0.2, 0.0504\times 0.5, 0.042\times 0.3] \times 0.4 = 0.01008 \\\Psi_3(2)=2  \\\delta_3(3) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j3}]b_3(o_3) = \max_{1\leq j \leq 3}[0.028  \times 0.3, 0.0504 \times 0.2, 0.042\times 0.5] \times 0.7 = 0.0147  \\\Psi_3(3)=3</script></li><li><p>现在开始回溯，此时最大概率为$\delta_3(3)$，从而得到$i_3^* =3$</p></li><li><p>由于$\Psi_3(3)=3,\Psi_2(3)=3$，所以最终最可能的隐藏状态序列为$(3,3,3)$</p></li></ul></li></ul><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><ul><li><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1></li></ul><h2 id="为什么这里的维特比和中文分词那里不太相同？"><a href="#为什么这里的维特比和中文分词那里不太相同？" class="headerlink" title="为什么这里的维特比和中文分词那里不太相同？"></a>为什么这里的维特比和中文分词那里不太相同？</h2><ul><li>中文分词时，我们没有观测序列和隐藏序列的区别，只有一种状态</li></ul><h2 id="维特比算法与dijkstra算法区别？"><a href="#维特比算法与dijkstra算法区别？" class="headerlink" title="维特比算法与dijkstra算法区别？"></a>维特比算法与dijkstra算法区别？</h2><ul><li>维特比算法使用动态规划，而dijkstra是贪心算法</li><li>维特比算法仅仅局限于求序列最短路径，而dijkstra是通用的求最短路径方法</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.cnblogs.com/pinard/p/6945257.html" target="_blank" rel="noopener">隐马尔科夫模型HMM（一）HMM模型</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之Flink</title>
      <link href="/2021/01/20/shi-yong-gong-ju-zhi-flink/"/>
      <url>/2021/01/20/shi-yong-gong-ju-zhi-flink/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink基础"><a href="#Flink基础" class="headerlink" title="Flink基础"></a>Flink基础</h1><h2 id="Flink是什么"><a href="#Flink是什么" class="headerlink" title="Flink是什么"></a>Flink是什么</h2><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul><li>Apache Flink 是一个框架和分布式处理引擎，用于在==无边界和有边界数据流==上进行有状态的计算。</li><li>Flink 能在所有常见集群环境中运行，并能以==内存速度和任意规模==进行计算。</li></ul><h4 id="处理无界和有界数据"><a href="#处理无界和有界数据" class="headerlink" title="处理无界和有界数据"></a>处理无界和有界数据</h4><ol><li><strong>无界流</strong> 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。</li><li><strong>有界流</strong> 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理</li><li><strong>Apache Flink 擅长处理无界和有界数据集</strong> 精确的==时间控制和状态化==使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。</li></ol><h4 id="部署应用到任意地方"><a href="#部署应用到任意地方" class="headerlink" title="部署应用到任意地方"></a>部署应用到任意地方</h4><ol><li>Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。</li><li>Flink 集成了所有常见的集群资源管理器，例如 <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Hadoop YARN</a>、 <a href="https://mesos.apache.org/" target="_blank" rel="noopener">Apache Mesos</a> 和 <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>，但==同时也可以作为独立集群==运行。</li><li>部署 Flink 应用程序时，Flink 会==根据应用程序配置的并行性自动标识==所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。</li></ol><h4 id="运行任意规模应用"><a href="#运行任意规模应用" class="headerlink" title="运行任意规模应用"></a>运行任意规模应用</h4><ol><li>Flink 旨在==任意规模==上运行==有状态流式==应用</li><li>因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO</li><li>而且 Flink 很容易维护非常大的应用程序状态。其==异步和增量的检查点算法==对处理延迟产生最小的影响，同时==保证精确一次状态==的一致性。<ol><li>处理<strong>每天处理数万亿的事件</strong></li><li>应用维护<strong>几TB大小的状态</strong></li><li>应用<strong>在数千个内核上运行</strong></li></ol></li></ol><h4 id="利用内存性能"><a href="#利用内存性能" class="headerlink" title="利用内存性能"></a>利用内存性能</h4><ol><li>有状态的 Flink 程序针对本地状态访问进行了优化</li><li>任务的状态始终保留在内存中，如果==状态大小超过可用内存==，则会保存在能高效访问的磁盘数据结构中</li><li>任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟</li><li>Flink 通过==定期和异步==地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性</li></ol><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="流处理应用的基本组件"><a href="#流处理应用的基本组件" class="headerlink" title="流处理应用的基本组件"></a>流处理应用的基本组件</h4><ol><li><a href="#流">流</a></li><li><a href="#状态">状态</a></li><li>时间</li></ol><h5 id="流"><a href="#流" class="headerlink" title="流"></a>流</h5><ol><li><strong>有界</strong> 和 <strong>无界</strong> 的数据流：流可以是无界的；也可以是有界的，例如固定大小的数据集。Flink 在无界的数据流处理上拥有诸多功能强大的特性，同时也针对有界的数据流开发了专用的高效算子。</li><li><strong>实时</strong> 和 <strong>历史记录</strong> 的数据流：所有的数据都是以流的方式产生，但用户通常会使用两种截然不同的方法处理数据。或是==在数据生成时进行实时的处理==；亦或是==先将数据流持久化到存储系统中==——例如文件系统或对象存储，然后再进行批处理。Flink 的应用能够同时支持处理实时以及历史记录数据流。</li></ol><h5 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h5><ol><li><strong>不需要状态</strong>：在每一个单独的事件上进行转换操作的应用</li><li><strong>需要状态</strong>：每一个具有一定复杂度的流处理应用</li><li>任何运行基本业务逻辑的流处理应用都需要在一定时间内存储所接收的事件或中间结果，以供后续的某个时间点（例如收到下一个事件或者经过一段特定时间）进行访问并进行后续处理</li><li><strong>状态管理相关的特性支持</strong>：<ol><li><strong>多种状态基础类型</strong>：Flink 为多种不同的数据结构提供了相对应的状态基础类型，例如原子值（value），列表（list）以及映射（map）。开发者可以基于处理函数对状态的访问方式，选择最高效、最适合的状态基础类型。</li><li><strong>插件化的State Backend</strong>：State Backend 负责管理应用程序状态，并在需要的时候进行 checkpoint。Flink 支持多种 state backend，可以将状态存在内存或者 <a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>。RocksDB 是一种高效的嵌入式、持久化键值存储引擎。Flink 也支持插件式的自定义 state backend 进行状态存储。</li><li><strong>精确一次语义</strong>：Flink 的 checkpoint 和故障恢复算法保证了故障发生后应用状态的一致性。因此，Flink 能够在应用程序发生故障时，对应用程序透明，不造成正确性的影响。</li><li><strong>超大数据量状态</strong>：Flink 能够利用其异步以及增量式的 checkpoint 算法，存储数 TB 级别的应用状态。</li><li><strong>可弹性伸缩的应用</strong>：Flink 能够通过在更多或更少的工作节点上对状态进行重新分布，支持有状态应用的分布式的横向伸缩。</li></ol></li></ol><h5 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h5><ol><li>时间是流处理应用另一个重要的组成部分。因为事件总是在特定时间点发生，所以大多数的事件流都拥有事件本身所固有的时间语义。</li><li>进一步而言，许多常见的==流计算都基于时间语义==，例如窗口聚合、会话计算、模式检测和基于时间的 join。</li><li>流处理的一个重要方面是应用程序如何衡量时间，即区分事件时间（event-time）和处理时间（processing-time）<ol><li><strong>事件时间模式</strong>：使用事件时间语义的流处理应用根据事件本身自带的时间戳进行结果的计算。因此，无论处理的是历史记录的事件还是实时的事件，事件时间模式的处理总能保证结果的准确性和一致性</li><li><strong>Watermark 支持</strong>：Flink 引入了 watermark 的概念，用以衡量事件时间进展。Watermark 也是一种平衡处理延时和完整性的灵活机制。</li><li><strong>迟到数据处理</strong>：当以带有 watermark 的事件时间模式处理数据流时，在计算完成之后仍会有相关数据到达。这样的事件被称为迟到事件。Flink 提供了多种处理迟到数据的选项，例如将这些数据重定向到旁路输出（side output）或者更新之前完成计算的结果。</li><li><strong>处理时间模式</strong>：除了事件时间模式，Flink 还支持处理时间语义。处理时间模式根据处理引擎的机器时钟触发计算，一般适用于有着严格的低延迟需求，并且能够容忍近似结果的流处理应用。</li></ol></li></ol><h4 id="分成API"><a href="#分成API" class="headerlink" title="分成API"></a>分成API</h4><p><img src="https://flink.apache.org/img/api-stack.png" alt="img：图片来源flink官网"></p><h5 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a><code>ProcessFunction</code></h5><ol><li>ProcessFunction 可以处理==一或两条==输入数据流中的单个事件或者归入一个特定窗口内的多个事件</li><li>它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的某一时刻触发回调函数</li><li>因此，你可以利用 ProcessFunction 实现许多<a href="https://flink.apache.org/zh/usecases.html#eventDrivenApps" target="_blank" rel="noopener">有状态的事件驱动应用</a>所需要的基于单个事件的复杂业务逻辑</li><li><a href="https://flink.apache.org/zh/flink-applications.html" target="_blank" rel="noopener">例子</a></li></ol><h5 id="DataStreamAPI"><a href="#DataStreamAPI" class="headerlink" title="DataStreamAPI"></a><code>DataStreamAPI</code></h5><ol><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/datastream_api.html" target="_blank" rel="noopener">DataStream API</a> 为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等</li><li>DataStream API 支持 Java 和 Scala 语言，预先定义了例如<code>map()</code>、<code>reduce()</code>、<code>aggregate()</code> 等函数。你可以通过扩展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。</li><li><a href="https://flink.apache.org/zh/flink-applications.html" target="_blank" rel="noopener">例子</a></li></ol><h5 id="SQL-amp-Table-API"><a href="#SQL-amp-Table-API" class="headerlink" title="SQL &amp; Table API"></a><code>SQL &amp; Table API</code></h5><ol><li>Flink 支持两种关系型的 API，<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/index.html" target="_blank" rel="noopener">Table API 和 SQL</a>。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果</li><li>Table API 和 SQL 借助了 <a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a> 来进行查询的解析，校验以及优化</li><li>它们可以与 DataStream 和 DataSet API 无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数</li><li>Flink 的关系型 API 旨在简化<a href="https://flink.apache.org/zh/usecases.html#analytics" target="_blank" rel="noopener">数据分析</a>、<a href="https://flink.apache.org/zh/usecases.html#pipelines" target="_blank" rel="noopener">数据流水线和 ETL 应用</a>的定义</li></ol><h4 id="库"><a href="#库" class="headerlink" title="库"></a>库</h4><ul><li>Flink 具有数个适用于常见数据处理应用场景的扩展库。这些库通常嵌入在 API 中，且并不完全独立于其它 API。它们也因此可以受益于 API 的所有特性，并与其他库集成<ol><li><strong><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/cep.html" target="_blank" rel="noopener">复杂事件处理(CEP)</a></strong>：模式检测是事件流处理中的一个非常常见的用例。Flink 的 CEP 库提供了 API，使用户能够以例如正则表达式或状态机的方式指定事件模式。CEP 库与 Flink 的 DataStream API 集成，以便在 DataStream 上评估模式。CEP 库的应用包括网络入侵检测，业务流程监控和欺诈检测。</li><li><strong><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/index.html" target="_blank" rel="noopener">DataSet API</a></strong>：DataSet API 是 Flink 用于批处理应用程序的核心 API。DataSet API 所提供的基础算子包括<em>map</em>、<em>reduce</em>、<em>(outer) join</em>、<em>co-group</em>、<em>iterate</em>等。所有算子都有相应的算法和数据结构支持，对内存中的序列化数据进行操作。如果数据大小超过预留内存，则过量数据将存储到磁盘。Flink 的 DataSet API 的数据处理算法借鉴了传统数据库算法的实现，例如混合散列连接（hybrid hash-join）和外部归并排序（external merge-sort）</li><li><strong><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/gelly/index.html" target="_blank" rel="noopener">Gelly</a></strong>: Gelly 是一个可扩展的图形处理和分析库。Gelly 是在 DataSet API 之上实现的，并与 DataSet API 集成。因此，它能够受益于其可扩展且健壮的操作符。Gelly 提供了<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/gelly/library_methods.html" target="_blank" rel="noopener">内置算法</a>，如 label propagation、triangle enumeration 和 page rank 算法，也提供了一个简化自定义图算法实现的 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/gelly/graph_api.html" target="_blank" rel="noopener">Graph API</a>。</li></ol></li></ul><h3 id="运维"><a href="#运维" class="headerlink" title="运维"></a>运维</h3><ul><li>Apache Flink 非常注重流数据处理的可运维性。因此在这一小节中，我们将详细介绍 Flink 的故障恢复机制，并介绍其管理和监控应用的功能</li></ul><h4 id="7-24小时稳定运行"><a href="#7-24小时稳定运行" class="headerlink" title="7*24小时稳定运行"></a>7*24小时稳定运行</h4><ol><li>它(这类流处理器)不仅要能在服务出现故障时候能够重启服务，而且还要当故障发生时，保证能够持久化服务内部各个组件的当前状态，只有这样才能保证在故障恢复时候，服务能够继续正常运行，好像故障就没有发生过一样</li><li>Flink通过几下多种机制维护应用可持续运行及其一致性<ol><li><strong>检查点的一致性</strong>: Flink的故障恢复机制是通过建立分布式应用服务状态一致性检查点实现的，当有故障产生时，应用服务会重启后，再重新加载上一次成功备份的状态检查点信息。结合可重放的数据源，该特性可保证<em>精确一次（exactly-once）</em>的状态一致性</li><li><strong>高效的检查点</strong>: 如果一个应用要维护一个TB级的状态信息，对此应用的状态建立检查点服务的资源开销是很高的，为了减小因检查点服务对应用的延迟性（SLAs服务等级协议）的影响，Flink采用==异步及增量==的方式构建检查点服务。</li><li><strong>端到端的精确一次</strong>: Flink 为某些特定的存储支持了事务型输出的功能，及时在发生故障的情况下，也能够保证精确一次的输出</li><li><strong>集成多种集群管理服务</strong>: Flink已与多种集群管理服务紧密集成，如 <a href="https://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop YARN</a>, <a href="https://mesos.apache.org/" target="_blank" rel="noopener">Mesos</a>, 以及 <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>。当集群中某个流程任务失败后，一个新的流程服务会自动启动并替代它继续执行</li><li><strong>内置高可用服务</strong>: Flink内置了为解决单点故障问题的高可用性服务模块，此模块是基于<a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">Apache ZooKeeper</a> 技术实现的，<a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">Apache ZooKeeper</a>是一种可靠的、交互式的、分布式协调服务组件</li></ol></li></ol><h4 id="Flink能够更方便地升级、迁移、暂停、恢复应用服务"><a href="#Flink能够更方便地升级、迁移、暂停、恢复应用服务" class="headerlink" title="Flink能够更方便地升级、迁移、暂停、恢复应用服务"></a>Flink能够更方便地升级、迁移、暂停、恢复应用服务</h4><ol><li>驱动关键业务服务的流应用是经常需要维护的。比如需要修复系统漏洞，改进功能，或开发新功能</li><li>然而升级一个有状态的流应用并不是简单的事情，因为在我们为了升级一个改进后版本而简单停止当前流应用并重启时，我们还不能丢失掉当前流应用的所处于的状态信息</li><li>而==Flink的 <em>Savepoint</em> 服务==就是为解决升级服务过程中记录流应用状态信息及其相关难题而产生的一种唯一的、强大的组件</li><li>一个 Savepoint，就是一个应用服务状态的一致性快照，因此其与checkpoint组件的很相似，但是与checkpoint相比，Savepoint 需要手动触发启动，而且当流应用服务停止时，它并不会自动删除。Savepoint 常被应用于启动一个已含有状态的流服务，并初始化其（备份时）状态。</li><li>Savepoint 有以下特点：<ol><li><strong>便于升级应用服务版本</strong>: Savepoint 常在应用版本升级时使用，当前应用的新版本更新升级时，可以根据上一个版本程序记录的 Savepoint 内的服务状态信息来重启服务。它也可能会使用更早的 Savepoint 还原点来重启服务，以便于修复由于有缺陷的程序版本导致的不正确的程序运行结果</li><li><strong>方便集群服务移植</strong>: 通过使用 Savepoint，流服务应用可以自由的在不同集群中迁移部署</li><li><strong>方便Flink版本升级</strong>: 通过使用 Savepoint，可以使应用服务在升级Flink时，更加安全便捷</li><li><strong>增加应用并行服务的扩展性</strong>: Savepoint 也常在增加或减少应用服务集群的并行度时使用</li><li><strong>便于A/B测试及假设分析场景对比结果</strong>: 通过把同一应用在使用不同版本的应用程序，基于同一个 Savepoint 还原点启动服务时，可以测试对比2个或多个版本程序的性能及服务质量</li><li><strong>暂停和恢复服务</strong>: 一个应用服务可以在新建一个 Savepoint 后再停止服务，以便于后面任何时间点再根据这个实时刷新的 Savepoint 还原点进行恢复服务</li><li><strong>归档服务</strong>: Savepoint 还提供还原点的归档服务，以便于用户能够指定时间点的 Savepoint 的服务数据进行重置应用服务的状态，进行恢复服务</li></ol></li></ol><h4 id="监控和控制应用服务"><a href="#监控和控制应用服务" class="headerlink" title="监控和控制应用服务"></a>监控和控制应用服务</h4><ol><li><p>如其它应用服务一样，持续运行的流应用服务也需要监控及集成到一些基础设施资源管理服务中，例如一个组件的监控服务及日志服务等</p></li><li><p>监控服务有助于预测问题并提前做出反应，日志服务提供日志记录能够帮助追踪、调查、分析故障发生的根本原因</p></li><li><p>==便捷易用的访问控制应用服务运行的接口==也是Flink的一个重要的亮点特征</p><ol><li><strong>Web UI方式</strong>: Flink提供了一个web UI来观察、监视和调试正在运行的应用服务。并且还可以执行或取消组件或任务的执行。</li><li><strong>日志集成服务</strong>:Flink实现了流行的slf4j日志接口，并与日志框架<a href="https://logging.apache.org/log4j/2.x/" target="_blank" rel="noopener">log4j</a>或<a href="https://logback.qos.ch/" target="_blank" rel="noopener">logback</a>集成</li><li><strong>指标服务</strong>: Flink提供了一个复杂的度量系统来收集和报告系统和用户定义的度量指标信息。度量信息可以导出到多个报表组件服务，包括 <a href="https://en.wikipedia.org/wiki/Java_Management_Extensions" target="_blank" rel="noopener">JMX</a>, Ganglia, <a href="https://graphiteapp.org/" target="_blank" rel="noopener">Graphite</a>, <a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a>, <a href="https://github.com/etsy/statsd" target="_blank" rel="noopener">StatsD</a>, <a href="https://www.datadoghq.com/" target="_blank" rel="noopener">Datadog</a>, 和 <a href="https://www.slf4j.org/" target="_blank" rel="noopener">Slf4j</a>.</li><li><strong>标准的WEB REST API接口服务</strong>: Flink提供多种REST API接口，有提交新应用程序、获取正在运行的应用程序的Savepoint服务信息、取消应用服务等接口。REST API还提供元数据信息和已采集的运行中或完成后的应用服务的指标信息。</li></ol></li></ol><h1 id="Flink进阶"><a href="#Flink进阶" class="headerlink" title="Flink进阶"></a>Flink进阶</h1><h2 id="Stateful-Functions：状态函数"><a href="#Stateful-Functions：状态函数" class="headerlink" title="Stateful Functions：状态函数"></a><strong>Stateful Functions：状态函数</strong></h2><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ol><li><a href="https://flink.apache.org/zh/flink-architecture.html" target="_blank" rel="noopener">官网</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/index.html" target="_blank" rel="noopener">官网教程</a></li></ol><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资源整合之实用工具</title>
      <link href="/2020/12/04/zi-yuan-zheng-he-zhi-shi-yong-gong-ju/"/>
      <url>/2020/12/04/zi-yuan-zheng-he-zhi-shi-yong-gong-ju/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/12/04/zi-yuan-zheng-he-zhi-shi-yong-gong-ju/image-20210621143931618.png" alt="image-20210621143931618"></p><h1 id="工具汇总"><a href="#工具汇总" class="headerlink" title="工具汇总"></a>工具汇总</h1><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><ol><li><a href="https://www.python.org/downloads/" target="_blank" rel="noopener">python2</a></li><li><a href="https://www.python.org/downloads/" target="_blank" rel="noopener">python3</a></li><li><a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">pycharm</a></li><li><a href="https://www.jetbrains.com/help/pycharm/quick-start-guide.html" target="_blank" rel="noopener">pycharm配置</a></li></ol><h2 id="java"><a href="#java" class="headerlink" title="java"></a>java</h2><ol><li><a href="https://www.oracle.com/java/technologies/javase-downloads.html" target="_blank" rel="noopener">java</a></li><li><a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener">Intellij Idea</a></li><li><a href="https://www.jetbrains.com/help/idea/discover-intellij-idea.html" target="_blank" rel="noopener">Intellij Idea配置</a></li><li><a href="https://maven.apache.org/" target="_blank" rel="noopener">maven</a></li></ol><h2 id="go"><a href="#go" class="headerlink" title="go"></a>go</h2><ol><li><a href="https://golang.org/" target="_blank" rel="noopener">go</a></li><li><a href="https://golang.org/doc/" target="_blank" rel="noopener">go教程</a></li></ol><h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><ol><li><a href="https://www.scala-lang.org/" target="_blank" rel="noopener">scala</a></li><li><a href="https://spark.apache.org/" target="_blank" rel="noopener">spark</a></li><li><a href="https://spark.apache.org/docs/latest/" target="_blank" rel="noopener">spark教程</a></li></ol><h2 id="R"><a href="#R" class="headerlink" title="R"></a>R</h2><ol><li><a href="https://www.r-project.org/" target="_blank" rel="noopener">R</a></li><li><a href="https://rstudio.com/" target="_blank" rel="noopener">Rstudio</a></li></ol><h2 id="git"><a href="#git" class="headerlink" title="git"></a>git</h2><ol><li><a href="https://git-scm.com/" target="_blank" rel="noopener">git</a></li><li><a href="https://git-scm.com/docs" target="_blank" rel="noopener">git教程</a></li></ol><h2 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h2><ol><li><a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener">MobaXterm</a></li></ol><h2 id="文本编辑与查看"><a href="#文本编辑与查看" class="headerlink" title="文本编辑与查看"></a>文本编辑与查看</h2><ol><li><a href="https://typora.io/" target="_blank" rel="noopener">typora</a></li><li><a href="https://notepad-plus-plus.org/" target="_blank" rel="noopener">Nodepad++</a></li><li><a href="https://www.emeditor.com/" target="_blank" rel="noopener">emeditor</a></li><li><a href="https://www.gaaiho.cn/index.php/zh-cn/" target="_blank" rel="noopener">GaaihoReader</a></li></ol><h2 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h2><ol><li><a href="https://www.xmind.net/" target="_blank" rel="noopener">xmind</a></li></ol><h2 id="爬虫相关"><a href="#爬虫相关" class="headerlink" title="爬虫相关"></a>爬虫相关</h2><ol><li><a href="https://www.postman.com/downloads/" target="_blank" rel="noopener">postman</a></li></ol><h2 id="google"><a href="#google" class="headerlink" title="google"></a>google</h2><ol><li><a href="https://www.google.com/intl/zh-CN/chrome/" target="_blank" rel="noopener">google</a></li></ol><h2 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h2><ol><li><a href="http://pan.baidu.com/download" target="_blank" rel="noopener">百度网盘</a></li><li><a href="http://note.youdao.com/WAP/intro/" target="_blank" rel="noopener">有道云笔记</a></li></ol><h2 id="邮件"><a href="#邮件" class="headerlink" title="邮件"></a>邮件</h2><ol><li><a href="http://mail.163.com/dashi/" target="_blank" rel="noopener">网易邮箱</a></li></ol><h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><ol><li><a href="https://cn.bandisoft.com/bandizip/" target="_blank" rel="noopener">bandzip</a></li></ol><h2 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h2><ol><li><a href="https://zh.snipaste.com/" target="_blank" rel="noopener">snipaste</a></li></ol><h2 id="文件搜索和目录相关"><a href="#文件搜索和目录相关" class="headerlink" title="文件搜索和目录相关"></a>文件搜索和目录相关</h2><ol><li><a href="https://www.voidtools.com/zh-cn/" target="_blank" rel="noopener">everything</a></li><li><a href="http://qttabbar.wikidot.com/" target="_blank" rel="noopener">qttabbar</a></li><li><a href="http://www.wox.one/" target="_blank" rel="noopener">Wox</a></li></ol><h2 id="保护眼睛"><a href="#保护眼睛" class="headerlink" title="保护眼睛"></a>保护眼睛</h2><ol><li><a href="https://justgetflux.com/" target="_blank" rel="noopener">f.lux</a></li></ol><h2 id="时钟屏保"><a href="#时钟屏保" class="headerlink" title="时钟屏保"></a>时钟屏保</h2><ol><li><a href="https://fliqlo.com/" target="_blank" rel="noopener">fliqo</a></li></ol><h2 id="聊天"><a href="#聊天" class="headerlink" title="聊天"></a>聊天</h2><ol><li><a href="https://weixin.qq.com/" target="_blank" rel="noopener">wechat</a></li></ol><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><ol><li><a href="https://www.xunlei.com/" target="_blank" rel="noopener">迅雷</a></li></ol><h2 id="goole扩展程序"><a href="#goole扩展程序" class="headerlink" title="goole扩展程序"></a>goole扩展程序</h2><ol><li><a href="https://chrome.google.com/webstore/category/extensions?hl=zh-CN" target="_blank" rel="noopener">adblock</a></li><li><a href="https://chrome.google.com/webstore/category/extensions?hl=zh-CN" target="_blank" rel="noopener">dark reader</a></li><li><a href="https://chrome.google.com/webstore/category/extensions?hl=zh-CN" target="_blank" rel="noopener">onetab</a></li><li><a href="https://chrome.google.com/webstore/category/extensions?hl=zh-CN" target="_blank" rel="noopener">沙拉查词</a></li><li><a href="https://chrome.google.com/webstore/category/extensions?hl=zh-CN" target="_blank" rel="noopener">octotree</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 资源整合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实用工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之HBase</title>
      <link href="/2020/06/11/shi-yong-gong-ju-zhi-hbase/"/>
      <url>/2020/06/11/shi-yong-gong-ju-zhi-hbase/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/06/11/shi-yong-gong-ju-zhi-hbase/image-20210621152645053.png" alt="image-20210621152645053"></p><h1 id="HBase基础"><a href="#HBase基础" class="headerlink" title="HBase基础"></a>HBase基础</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase是一个开源的非关系型分布式数据库（NoSQL），它参考了谷歌的BigTable建模，实现的编程语言为 Java。它是Apache软件基金会的Hadoop项目的一部分，运行于HDFS文件系统之上，为 Hadoop 提供类似于BigTable 规模的服务。因此，它可以对稀疏文件提供极高的容错率。    HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列簇的而不是基于行的模式。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h2><p><img src="/2020/06/11/shi-yong-gong-ju-zhi-hbase/image-20201218163039823.png" alt="image-20201218163039823"></p><ul><li><p>客户端</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase客户端（Client）提供了Shell命令行接口、原生Java API编程接口、Thrift/REST API编程接口以及MapReduce编程接口。支持所有常见的DML操作以及DDL操作，包括表的创建、删除、修改，以及数据的插入、删除、更新、读取等。其中Thrift/REST API主要用于支持非Java的上层业务需求，MapReduce接口则主要用于批量数据导入以及批量数据读取。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>Master</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    负责处理用户的各种管理请求，包括建表操作、修改表操作、权限操作、切分表操作、合并数据分片操作以及Compaction操作；管理集群中所有RegionServer，包括RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移；负责过期日志以及文件的清理工作，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后会将其删除。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>RegionServer</p><ul><li><p>WAL(HLog)</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HLog在HBase中有两个核心作用，其一用于实现数据高可靠，HBase数据随机写入并不是直接写入文件，而是先写入缓存，再异步刷新落盘。为了防止缓存数据丢失，数据写入缓存之前需要首先顺序写入HLog，通过这种方式，即使缓存数据丢失，仍然可以通过HLog日志恢复。其二用于实现HBase集群间主从复制，从集群通过回放主集群推送过来的HLog日志实现主从复制。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>BlockCache</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase系统中的读缓存，客户端从磁盘读取数据之后通常会将数据缓存到系统内存中，后续访问同一行数据可以直接从内存中获取而不需要访问磁盘。对于带有大量热点读的业务请求来说，缓存机制会带来极大的性能提升。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>Region</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    数据表的一个分片，当数据表大小超过一定阈值就会“水平切分”，分裂为两个Region。Region是集群负载均衡的基本单位。通常一张表的Region会分布在整个集群的多台RegionServer上，一个RegionServer上会管理多个Region，当然，这些Region一般来自不同的数据表<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li><p>HDFS</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase底层依赖于HDFS组件存储实际数据，包括用户数据文件、HLog日志文件等等最终都会写入HDFS落盘。HDFS是Hadoop生态圈内最成熟的组件之一，数据默认三副本存储策略可以有效保证数据的高可靠性。HBase内部封装了一个称为DFSClient的HDFS客户端组件负责对HDFS的实际数据进行读写访问。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h2 id="逻辑视图"><a href="#逻辑视图" class="headerlink" title="逻辑视图"></a>逻辑视图</h2><p><img src="/2020/06/11/shi-yong-gong-ju-zhi-hbase/image-20201218164935058.png" alt="image-20201218164935058"></p><ul><li><p>table</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    表，一个表包含多行数据。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>row</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    行，一行数据包含一个唯一标识rowkey、多个column以及对应的值。在HBase中，一张表中所有row都按照rowkey的字典序由小到大排序。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>column</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    列，与关系型数据库中的列不同，HBase中的column由column family（列簇）以及qualifier（列名）两部分组成，两者中间使用"："相连，比如contents：html，其中contents为列簇，html为列簇下具体一列。column family在表创建的时候需要指定，用户不能随意增减。一个column family下可以设置任意多个qualifier，因此可以理解为HBase中的列可以动态增加，理论上甚至可以扩展到上百万列。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>cell</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    单元格，由5元组（row，column，timestamp，type，value）组成的结构，其中type表示Put/Delete这样的操作类型，timestamp代表这个cell的版本。这个结构在数据库中实际是以KV结构存储的，其中（row，column，timestamp，type）是K，value字段对应KV的V。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>timestamp</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    时间戳，每个cell在写入HBase的时候都会默认分配一个时间戳作为该cell的版本，当然，用户也可以在写入的时候自带时间戳。HBase支持多版本特性，即同一rowkey、column下可以有多个value存在，这些value使用timestamp作为版本号，版本越大，表示数据越新。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li><p>容量巨大</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase的单表可以支持千亿行、百万列的数据规模，数据容量可以达到TB甚至PB级别。传统的关系型数据库，如Oracle和MySQL等，如果单表记录条数超过亿行，读写性能都会急剧下降，在HBase中并不会出现这样的情况。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>良好的可扩展性</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase集群可以非常方便地实现集群容量扩展，主要包括数据存储节点扩展以及读写服务节点扩展。HBase底层数据存储依赖于HDFS系统，HDFS可以通过简单地增加DataNode实现扩展，HBase读写服务节点也一样，可以通过简单的增加RegionServer节点实现计算层的扩展。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>稀疏性</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase支持大量稀疏存储，即允许大量列值为空，并不占用任何存储空间。这与传统数据库不同，传统数据库对于空值的处理是占用一定存储空间的，这会造成一定程度的存储空间浪费。因此可以使用HBase存储多至上百万列的数据，即使表中存在大量的空值，也不需要任何额外空间。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>高性能</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase目前主要擅长于OLTP场景，数据写操作性能强劲，对于随机单点读以及小范围的扫描读性能也能够得到保证。对于大范围的扫描读可以使用MapReduce提供的API实现更高效的并行扫描。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>多版本</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase支持多版本特性，一个KV可以同时保留多个版本，用户可以根据需要选择最新版本或者某个历史版本。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>支持过期</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase支持TTL过期特性，用户只需要设置过期时间，超过TTL的数据就会自动被清理，不需要用户写程序手动删除。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>Hadoop原生支持</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    HBase是Hadoop生态中的核心成员之一，很多生态组件都可以与其直接对接。HBase数据存储依赖于HDFS，这样的架构可以带来很多好处，比如用户可以直接绕过HBase系统操作HDFS文件，高效地完成数据扫描或者数据导入工作；再比如可以利用HDFS提供的多级存储特性（Archival Storage Feature）将HBase根据业务的重要程度进行分级存储，将重要的业务放到SSD，将不重要的业务放到HDD。或者用户可以设置归档时间，进而将最近的数据放在SSD，将归档数据文件放在HDD。另外，HBase对MapReduce的支持也已经有了很多案例，后续还会针对spark做更多的工作。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li><p>HBase本身不支持很复杂的聚合运算（Join、GroupBy等）</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    如果业务中需要使用到聚合运算，可以在HBase之上架设Phoenix组件或者Spark组件，前者主要应用于小规模聚合的OLTP场景，后者应用于大规模聚合的OLAP场景。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>HBase本身并没有实现二级索引功能</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    所以不支持二级索引查找。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>HBase原生不支持全局跨行事务</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    只支持单行事务模型。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>写密集型应用，每天写入量巨大，而相对读数量较小的应用，比如IM的历史消息，游戏的日志等等</li><li>高并发 key-value 存储的场景；例如 HBase 可以作为实时计算( Storm )等的中间表和结果表，存储计算结果</li><li>不需要复杂查询条件来查询数据的应用：HBase只支持基于rowkey的查询，对于HBase来说，单条记录或者小范围的查询是可以接受的，大范围的查询由于分布式的原因，可能在性能上有点影响，而对于像SQL的join等查询，HBase无法支持。</li><li>数据结构比较简单，表结构的列族经常需要调整的场景</li></ul><h1 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>single-node</li><li>standalone</li></ul><h2 id="2-操作"><a href="#2-操作" class="headerlink" title="2. 操作"></a>2. 操作</h2><h3 id="2-1-启动"><a href="#2-1-启动" class="headerlink" title="2.1 启动"></a>2.1 启动</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">$ ./bin/hbase shellhbase(main):001:0><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="2-2-建表"><a href="#2-2-建表" class="headerlink" title="2.2 建表"></a>2.2 建表</h3><ul><li>必须指定表名<code>test</code>和列名<code>cf</code></li></ul><pre class="line-numbers language-lang-hbase"><code class="language-lang-hbase">hbase(main):001:0> create 'test', 'cf'0 row(s) in 0.4170 seconds=> Hbase::Table - test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-3-列出表信息"><a href="#2-3-列出表信息" class="headerlink" title="2.3 列出表信息"></a>2.3 列出表信息</h3><pre class="line-numbers language-lang-hbase"><code class="language-lang-hbase">hbase(main):002:0> list 'test'TABLEtest1 row(s) in 0.0180 seconds=> ["test"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-hbase"><code class="language-lang-hbase">hbase(main):003:0> describe 'test'Table test is ENABLEDtestCOLUMN FAMILIES DESCRIPTION{NAME => 'cf', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}1 row(s)Took 0.9998 seconds<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之FTRL方法</title>
      <link href="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/"/>
      <url>/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h1><h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>【原文】</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    We prove that many mirror descent algorithms for online convex optimization (such as online gradient descent) have an equivalent interpretation as follow-the-regularizedleader (FTRL) algorithms. This observation makes the relationships between many commonly used algorithms explicit, and provides theoretical insight on previous experimental observations. In particular, even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly, it has been observed that the FTRL-style Regularized Dual Averaging (RDA) algorithm is even more e ective at producing sparsity. Our results demonstrate that the key di erence between these algorithms is how they handle the cumulative L1 penalty. While FOBOS handles the L1 term exactly on any given update, we show that it is e ectively using subgradient approximations to the L1 penalty from previous rounds, leading to less sparsity than RDA, which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm, which we introduce, can be seen as a hybrid of these two algorithms, and signi cantly outperforms both on a large, realworld dataset.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>【译文】</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    我们证明了许多用于在线凸优化的镜像下降算法（例如在线梯度下降）具有与follow-the-regularizedleader (FTRL)算法等效的解释。该观察使许多常用算法之间的关系变得明确，并提供了对先前实验观察的理论见解。尤其是，即使FOBOS复合镜像下降算法可以明确处理L1正则化，也已经观察到FTRL样式的正则平均化（RDA）算法在产生稀疏性方面更为有效。我们的结果表明，这些算法之间的关键区别在于它们如何处理累积的L1损失。尽管FOBOS可以在任何给定的更新中精确地处理L1项，但我们证明它有效地使用了前几轮对L1罚分的次梯度近似，导致稀疏性低于RDA，后者以封闭形式处理累积罚分。我们介绍的FTRL-Proximal算法可以看作是这两种算法的混合体，在大型的真实数据集上均远胜过两者。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605145145754.png" alt="image-20200605145145754"></p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605145210366.png" alt="image-20200605145210366"></p><p>【译文】</p><p>​    我们考虑了在线凸优化的问题及其在在线学习中的应用。 在每一轮中，$t=1,…,T$，我们选择一个点$x_t\in R^n$。 然后揭示了凸损失函数$f_t$以及损失$f_t(x_t)$。在这项工作中，我们研究了在线凸优化的两个最重要且成功的低regret算法家族之间的关系。 从表面上看，诸如Regularized Dual Averaging [Xiao，2009]之类的正规化领导算法与FOBOS [Duchi and Singer，2009]等梯度下降（更普遍的是镜像下降）风格的算法似乎有所不同。 但是，这里我们表明，在二次稳定正则化的情况下，算法之间基本上只有两个区别：</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605150118526.png" alt="image-20200605150118526"></p><p>【译文】</p><ul><li>它们如何选择将额外的强凸性用于保证低regret：RDA将正则化集中在原点，而FOBOS将正则化集中在当前可行点。</li><li>它们如何处理任意的非光滑正则化函数。这包括投影到可行集的机制以及如何处理L1正则化。</li></ul><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605150139243.png" alt="image-20200605150139243"></p><p>【译文】</p><p>​    为了使这些区别更精确，同时也说明这些家族实际上是密切相关的，我们考虑第三种算法，FTRL-Proximal。当省略非光滑项时，该算法实际上与FOBOS是相同的。另一方面，其更新与对偶平均的更新基本相同，只是增加的强凸性以当前可行点为中心(见表1)。</p><p>【表1】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605152908088.png" alt="image-20200605152908088"></p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605150155117.png" alt="image-20200605150155117"></p><p>【译文】</p><p>​    先前的工作已通过实验表明，采用L1正则化进行对偶平均比引入FOBOS更有效地引入稀疏性[Xiao,2009,Duchi et al.,2010a]。我们的等价定理对此提供了理论上的解释：RDA考虑了第t轮的累积L1惩罚$ t\lambda ||x||_1 $，FOBOS（当使用等价定理视为全局优化时）考虑$\phi_{1:t-1}*x+\lambda||x||_1$，其中$\phi_s$是确定的$\lambda||x_s||_1$的次梯度近似（我们用$\phi_{1:t-1}$代表$\sum_{s=1}^{t-1}{\phi_s}$，并根据需要将表示法扩展为矩阵和函数的和）。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605150209723.png" alt="image-20200605150209723"></p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605150311078.png" alt="image-20200605150311078"></p><p>【译文】</p><p>​    在第2节中，我们考虑了镜像下降和遵循正则化领导的一般公式，并证明了与两者有关的定理。 在第3节中，我们将通过实验比较FOBOS，RDA和FTRL-Proximal。 FTRL-Proximal算法在稀疏性方面的行为与RDA非常相似，认为是L1罚则的累积次梯度近似导致FOBOS稀疏性降低。<br>​    近年来，在线梯度下降和随机梯度下降（其批次模拟）已证明自己是用于大规模机器学习的出色算法。 在最简单的情况下，FTLR-Proximal是相同的，但是当需要L1或其他不平滑的正则化时，FTRL-Proximal的性能明显优于FOBOS，并且也可以优于RDA。 由于FTRL-Proximal和RDA的实现仅需要几行代码，因此我们建议您尝试两者并在实践中选择性能最佳的代码。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200605162626080.png" alt="image-20200605162626080"></p><p>【译文】</p><p>​    我们首先建立符号并更正式地介绍我们考虑的算法。 虽然我们的定理适用于这些算法的更通用版本，但在这里我们集中于我们在实验中使用的特定实例。我们考虑损失函数$f_t(x)=l_t(x)+\Phi(x)$，其中$\Phi(x)$（通常不平滑）是固定的正则化函数。在典型的在线学习环境中，给定一个样本$(\theta_t,y_t)$，$\theta_t \in R^n$是特征向量，$y_t \in \{-1,1\}$是label，我们得到$l_t(x)=loss(\theta_tx,y_t)$。例如，对逻辑回归来说用的是log-loss，$loss(\theta_tx,y_t)=log(1+\exp(-y_t\theta_tx))$。对线性函数我们使用标准规约法，令$g_t=\nabla{l_t(x_t)}$。我们考虑的所有算法都支持复合更新（显示考虑$\Phi$而不是通过梯度$\nabla{f_t(x_t)}$以及可自适应选择的正半定学习率矩阵Q（将这些矩阵解释为学习率将在第2节中有说明））</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200611093316088.png" alt="image-20200611093316088"><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200611093355502.png" alt="image-20200611093355502"></p><p>【译文】</p><p>​    我们考虑的第一个算法是来自梯度下降家族的<code>FOBOS</code>，如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x \min {g_t*x+t\lambda||x||_1+\frac{1}{2}\sum_{s=1}^{t}{||Q_{1:t}^{1/2}(x-x_t)||_2^2}}</script><p>​    我们隐式地将该算法表述为一种优化，但也可以给出一种梯度下降式的封闭形式更新[Duchi和Singer, 2009]。Duchi等人将该算法描述为一种特殊的复合物镜下降(COMID)算法[2010b]。</p><p>​    Xiao[2009]的正则对偶平均算法（<code>RDA</code>）如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x \min {g_{1:t}*x+t\lambda||x||_1+\frac{1}{2}\sum_{s=1}^{t}{||Q_s^{1/2}(x-0)||_2^2}}</script><p>​    对比<code>FOBOS</code>，<code>RDA</code>是用累积梯度$g_{1:t}$，而不仅仅是$g_t$。我们将在定理4证明当$\lambda=0$以及$l_t$不是强凸时，该算法（<code>RDA</code>）实际上等价于在线自适应梯度下降法（<code>AOGD</code>）[Bartlett et al., 2007]</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200622094912113.png" alt="image-20200622094912113"></p><p>【译文】</p><p>​    FTRL-Proximal算法如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x \min {g_{1:t}*x+t\lambda||x||_1+\frac{1}{2}\sum_{s=1}^{t}{||Q_s^{1/2}(x-x_s)||_2^2}}</script><p>​    该算法在 [McMahan and Streeter, 2010]的论文中有介绍，但是没有支持显示的$\Phi$，而[McMahan, 2011]证明了处理固定$\Phi$函数的更一般算法的遗憾界</p><p>​    我们的主要贡献之一就是展示了这四种算法之间的紧密联系;表1总结了定理2和定理4的关键结果，以使与RDA和FTRL-Proximal的关系显式的形式编写了AOGD和FOBOS。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200702143901089.png" alt="image-20200702143901089"></p><p>【译文】</p><p>​    在我们的分析中，我们将用任意凸函数$R_t$和$\hat{R_t}$来代替上面出现的$\frac{1}{2}||Q_t^{1/2}x||_2^2$和$\frac{1}{2}||Q_t^{1/2}(x-x_t)||_2^2$，同时用任意凸函数$\Phi(x)$代替$\lambda||x||_1$。对这些算法，矩阵$Q_t$被自适应的选择。我们在实验中使用<code>per-coordinate adaptation</code>例如$Q_t$是形如$Q_{1:t}=diag(\hat{\sigma_{t,1}},…,\hat{\sigma_{t,n}})$的对角矩阵，其中$\hat{\sigma_{t,i}}=\frac{1}{\gamma}\sqrt{\sum_{s=1}^t{g_{t,i}^2}}$，详情请见<code>McMahan and Streeter [2010] and Duchi et al. [2010a]</code>。由于所有算法都受益于这种方法，因此即使在大多数情况下以标量学习率引入它们，我们也使用原始算法的更熟悉的名称。 是学习率比例参数，我们在实验中对其进行了调整。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200702163652233.png" alt="image-20200702163652233"></p><p>【译文】</p><p><strong>高效的实施度：</strong>所有这些算法都可以有效实施，因为$g_t$的更新能在$O(K)$时间内完成，其中$K$非零。FTRL-Proximal和RDA可以通过存储每个属性的两个积分值，一个二次项和一个线性项来实现（对角学习率）。 当需要$x_{t,i}$时，可以用封闭形式的懒惰式求解（例如参见<code>[Xiao，2009]</code>）。</p><p>对FOBOS来说，更新中$\lambda||x||_1$的存在意味着所有系数$x_{t.i}$（即使在$g_{t,i}=0$时也）需要更新。然而，但是，通过存储$g_{t,i}$不为零的最后一轮的索引<code>t</code>，L1 的一部分更新是懒惰的<code>[Duchi and Singer，2009]</code>。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200703161502635.png" alt="image-20200703161502635"></p><p>【译文】</p><p><strong>可行集：</strong> 在某些应用中，我们可能只用来自受限凸可行集$F \sub R^n$的点，例如，图中两个节点之间的路径集。由于我们考虑的所有算法都支持复合更新，因此可以通过选择在$F$上的指标函数$\Phi_F$来选择$\Phi$来完成更新，也就是$\Phi_F(x)=0 , if(x \in F),else(\infty)$。可以直接证明$\arg\min_{x\in R^n}{g_{1:t}x+R_{1:t}(x)+\Phi_F(x)}$等价于$\arg\min_{x\in F}{g_{1:t}x+R_{1:t}(x)}$，并且我们无需明确讨论$F$，而是考虑任意扩展的凸函数，就可以推广特定可行集的结果<code>[McMahanand Streeter, 2010]</code></p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200703171520426.png" alt="image-20200703171520426"></p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200703171600989.png" alt="image-20200703171600989"></p><p>【译文】</p><p>​    我们记$x,y\in R^n$的内积为$x^Ty,or,xy$。$x_{t,i}\in R$表示时间$t$的第$i$个变量。$B$是半正定矩阵，有$||B^{1/2}x||_2^2=x^TBx$。除非另有说明，否则假定凸函数是域$R^n$和范围$R^n\cup\{\infty\}$的扩展。对凸函数$f$，$\partial{f}$表示$f$在$x$上的导数。通过定义，对所有$y$，有$f(y)\ge f(x)+g^T(y-x)$，其中$g\in \partial f(x)$。当$f$可导时，记$\nabla f(x)$为$f$在$x$的导数。这种情况下，$\partial f(x) = \{\nabla f(x)\}$。除非特别指定，不然所有的$mins$和$argmins$都是在$R^n$上。我们经常使用以下标准结果，总结如下：</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706145943582.png" alt="image-20200706145943582"></p><p>【译文】</p><p><strong>定理1：</strong>令$R:R^n \rightarrow R$是具有连续一阶偏导数的强凸函数以及令$\Psi:R^n \rightarrow R\cup \{\infty\}$是任意凸函数。定义$g(x)=R(x)+\Psi(x)$。则，存在一个如下等式的唯一对$(x^*,\phi^*)$：</p><script type="math/tex; mode=display">\phi^*\in \partial{\Psi(x^*)} \\x^*=\arg_x\min{R(x)+\phi^*x}\leftrightarrow \nabla R(x^*)+\phi^*=0</script><p>且$x^*$是$g$的唯一最小值。</p><p>【证明】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706152306207.png" alt="image-20200706152306207"></p><p>【译文】</p><p>​    由于$R$是强凸的，所以$g$也是强凸的。则$g$拥有唯一最小值$x^*$。令$r=\nabla R$，则存在$\phi^* \in \partial \Psi(x^*)$使得$r(x^*)+\phi^*=0$（因为这是$0\in \partial g(x^*)$的必要充分条件）。且$x^*=\arg_x\min{R(x)+\phi^*x}$（因为$r(x^*)+\phi^*$是其在$x^*$处的梯度）。假定有另外一组$(x^1,\phi^1)$满足这个定理，即$r(x^{1})+\phi^{1}=0$以及$0 \in \partial g(x^1)$，且$x^1$是$g$的唯一最小值。由于最小值是唯一的，则$x^1=x^*,\phi^1=-r(x^*)=\phi^*$。</p><h2 id="2-MIRROR-DESCENT-FOLLOWS-THE-LEADER"><a href="#2-MIRROR-DESCENT-FOLLOWS-THE-LEADER" class="headerlink" title="2 MIRROR DESCENT FOLLOWS THE LEADER"></a>2 MIRROR DESCENT FOLLOWS THE LEADER</h2><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706155201563.png" alt="image-20200706155201563"></p><p>【译文】</p><p>​    在本节中，我们考虑了镜像下降算法（最简单的示例是在线梯度下降）和FTRL算法之间的关系。令$f_t(x)=g_tx+\Psi(x)$，其中$g_t \in \partial l_t(x_t)$。令$R_1$强凸，所有$R_t$凸。 除非另有说明，否则我们假设$\min_xR_1(x)= 0$，并假设$x = 0$是唯一的最小值。</p><h3 id="FTRL"><a href="#FTRL" class="headerlink" title="FTRL"></a>FTRL</h3><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706160046298.png" alt="image-20200706160046298"></p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706160105113.png" alt="image-20200706160105113"></p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706160119257.png" alt="image-20200706160119257"></p><p>【译文】</p><p><strong>FTRL：</strong>最简单的遵循正规领导算法如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_{1:t}x+\frac{\sigma_{1:t}}{2}||x||_2^2}</script><p>对$t=1$，我们通常取$x_1=0$。我们可以将$\frac{1}{2}||x||_2^2$替换成任意强凸函数$R$，如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_{1:t}x+\sigma_{1:t}R(x)}</script><p>我们可以为每个$t$独立选择$\sigma_{1:t}$，但是需要它是非减的，因此将其写成每轮增量$\sigma_t \ge 0$的总和是合理的。更一般的更新如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_{1:t}x+R_{1:t}(x)}</script><p>在每个回合上添加一个额外的凸函数$R_t$。 令$R_t(x)=\sigma_tR(x)$恢复先前的公式(6)。</p><p>​    当$\arg_{x\in R^n}\min{R_t(x)=0}$，我们称函数$R_t$为<strong>origin-centered</strong>。我们还可以定义$FTRL^1$的<strong>proximal</strong>版本，将其他正则化放在当前点而不是原点的中心。在这节，我们保留$R_t$的定义并写出其<strong>origin-centered</strong>函数$\tilde{R}_t(x)=R_t(x-x_t)$。请注意，仅需要$\tilde{R}_t$来选择$x_{t+1}$，并且此时$x_t$是算法已知的，从而确保算法在计算$x_{t+1}$时仅需要访问第一个t损失函数（根据需要）。 一般更新如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_{1:t}x+\tilde{R}_{1:t}(x)}</script><p>最简单的情形如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_{1:t}x+\sum_{s=1}^t{\frac{\sigma_s}{2}||x-x_s||_2^2}}</script><h3 id="Mirror-Descent"><a href="#Mirror-Descent" class="headerlink" title="Mirror Descent"></a>Mirror Descent</h3><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200706173107216.png" alt="image-20200706173107216"></p><p>【译文】</p><p><strong>Mirror Descent：</strong>最简单的mirror descent版本是以同一步长$\eta$的梯度下降算法，如下：</p><script type="math/tex; mode=display">x_{t+1}=x_t-\eta g_t = -\eta g_{1:t}</script><p>为了得到低惩罚，$T$必须事先知道，以便可以相应的选择$\eta$。但是，由于有一个关于在点$x_{t+1}$上的$g_{1:t}$和$\eta$的封闭形式解决方案，我们将此归纳为一个“revisionist”算法，该算法在每一轮中都扮演着这样的观点：如果在第1到$t-1$轮中使用了步长$\eta_t$，则具有恒定步长的梯度下降将发挥作用。也就是$x_{t+1}=-\eta_tg_{1:t}$。当$R_t(x)=\frac{\sigma_t}{2}||x||_2^2$且$\eta_t=\frac{1}{\sigma_{1:t}}$，这等价于FTRL。</p><p>​    更一般的，我们将对梯度下降算法更感兴趣，该算法使用的自适应步长至少（取决于）轮数$t$。 在每个回合中使用可变步长$t$进行梯度下降如下：</p><script type="math/tex; mode=display">x_{t+1}=x_t-\eta_t g_t</script><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200707143210278.png" alt="image-20200707143210278"></p><p>【译文】</p><p>​    这种更新的直觉来自于它可以被重写为如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_tx+\frac{1}{2\eta_t}||x-x_t||_2^2}</script><p>这个版本抓住了这样一个概念(在线学习术语)，即我们不想过多地改变我们的假设$x_t$(因为害怕对我们已经看到的例子做出错误的预测)，但我们确实想朝着减少我们最近看到的例子的假设损失的方向前进(这是由线性函数$g_t$近似得出的)。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200707144406205.png" alt="image-20200707144406205"></p><p>【译文】</p><p>​    镜像下降算法利用了这种直觉，用任意的Bregman离散收敛替换了$L_2$平方惩罚。对于可微的，严格凸的，响应的Bregman散度为：</p><script type="math/tex; mode=display">\Beta_R(x,y)=R(x)-(R(y)+\nabla R(y)*(x-y))</script><p>对任意的$x,y\in R^n$，我们得到以下更新：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_tx+\frac{1}{\eta_t}\Beta_R(x,x_t)}</script><p>或者更明确地（通过将（14）的梯度设置为0）：</p><script type="math/tex; mode=display">x_{t+1}=r^{-1}(r(x_t)-\eta_tg_t)</script><p>其中$r=\nabla R$。令$R(x)=\frac{1}{2}||x||_2^2$以使$\Beta_R(x, x_t)=\frac{1}{2}||x-x_t||_2^2$恢复公式（13）的算法。看到这种情况的一种方式是注意在这种情况下$r(x)=r^{-1}(x)=x$。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200707151519532.png" alt="image-20200707151519532"></p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200707160925823.png" alt="image-20200707160925823"></p><p>【译文】</p><p>​    我们可以通过在每一轮的Bergman散度中加入一个新的强凸函数$R_t$来进一步推广它。也就是说，让：</p><script type="math/tex; mode=display">\Beta_{1:t}(x,y)=\sum_{s=1}^t{\Beta_{R_s}(x,y)}</script><p>所以更新变成：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_tx+\Beta_{1:t}(x,x_t)}</script><p>或等价于$x_{t+1}=(r_{1:t})^{-1}(r_{1:t}(x_t)-g_t)$，其中$r_{1:t}=\sum_{s=1}^t{\nabla R_t}=\nabla R_{1:t}$以及$(r_{1:t})^{-1}$是$r_{1:t}$的反函数。步长$\eta_t$隐藏编码在$R_t$的选择中。</p><p>​    COMID将$\Psi$函数加入每一轮迭代：$f_t(x)=g_tx+\Psi(x)$。然后更新如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{\eta g_tx+\Beta(x,x_t)+\eta \Psi(x)}</script><p>可以推广成如下：</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_tx+\Psi(x)+\Beta_{1:t}(x,x_t)}</script><p>其中学习率$\eta$被定义在$R_1,…,R_t$。如果选择$\Psi$作为凸集上的指标函数，则COMID可通过贪婪投影将标准下降到镜面下降。</p><h3 id="2-1-An-Equivalence-Theorem-for-Proximal-Regularization"><a href="#2-1-An-Equivalence-Theorem-for-Proximal-Regularization" class="headerlink" title="2.1 An Equivalence Theorem for Proximal Regularization"></a>2.1 An Equivalence Theorem for Proximal Regularization</h3><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200707161708239.png" alt="image-20200707161708239"></p><p>【译文】</p><p>​    在定理2中，我们表明镜像下降算法可以被视为FTRL算法。</p><p><strong>定理2：</strong>令$R_t$是一组可微分的原点中心凸函数（$\nabla R_t(0)=0$），$R_1$是强凸的，且令$\Psi$是任意凸函数。令$x_1=\hat{x}_1=0$。对一组损失函数$f_t(x)=g_tx+\Psi(x)$，在一组点上的复合目标镜像梯度算法如下：</p><script type="math/tex; mode=display">\hat{x}_{t+1}=\arg_x\min{g_t+\Psi(x)+\tilde{\Beta}_{1:t}(x,\hat{x}_t)}</script><p>其中$\tilde{R}_t(x)=R_t(x-\hat{x}_t)$，以及$\tilde{\Beta}_t=\Beta_{\tilde{R}_t}$，所以$\tilde{\Beta}_{1:t}$是关于$\tilde{R}_1+··+ \tilde{R}_t$的Bregman散度。考虑在另外一组点$x_t$上的FTRL算法，</p><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{(g_{1:t}+\phi_{1:t-1})x+\tilde{R}_{1:t}(x)+\Psi(x)}</script><p>其中$\phi_t\in\partial{\Psi(x_{t+1})}$以致于$g_{1:t}+\phi_{1:t-1}+\nabla{\tilde{R}_{1:t}(x_{t+1})}+\phi_t=0$。那么，这些算法是等价的，即对所有$t&gt;0$，有$x_t=\hat{x}_t$。</p><p>【原文】</p><p><img src="/2020/06/04/ji-qi-xue-xi-zhi-ftrl-fang-fa/image-20200707172308338.png" alt="image-20200707172308338"></p><p>【译文】</p><p>​    镜像下降定理中使用的Bregman散度是关于近端函数$\tilde{R}_{1:t}$，而通常(如方程(17))这些函数将不依赖于之前的点发挥。我们将证明当$R_t(x)=\frac{1}{2}||Q_t^{1/2}x||_2^2$时，这些问题将不存在。考虑任意$\Psi$函数也会使定理陈述有些复杂。以下推论回避了这些复杂性，以陈述简单的直接等价结果</p><p><strong>推论3：</strong>令$f_t(x)=g_tx$。然后，以下算法发挥相同的作用：</p><ul><li><p>使用半正定学习速率$Q_t$的梯度下降算法：</p><script type="math/tex; mode=display">x_{t+1}=x_t-Q_{1:t}^{-1}{g_t}</script></li><li><p>FTRL算法</p></li></ul><script type="math/tex; mode=display">x_{t+1}=\arg_x\min{g_{1:t}x+\tilde{R}_{1:t}(x)}\\\tilde{R}_t(x)=\frac{1}{2}||Q_t^{1/2}(x-x_t)||_2^2</script><hr><h1 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h1>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FTRL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之MapReduce</title>
      <link href="/2020/06/03/shi-yong-gong-ju-zhi-mapreduce/"/>
      <url>/2020/06/03/shi-yong-gong-ju-zhi-mapreduce/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/06/03/shi-yong-gong-ju-zhi-mapreduce/image-20210621145110769.png" alt="image-20210621145110769"></p><h1 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h1><p><img src="/2020/06/03/shi-yong-gong-ju-zhi-mapreduce/image-20200603093104618.png" alt="image-20200603093104618"></p><p>【译文】</p><ul><li>MapReduce：大型集群上的简化数据处理</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>【原文】</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.    Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system.    Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>【译文】</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    MapReduce是用于处理和生成大数据集的编程模型（相关的实现）。 用户指定key\value对以生成一组中间key\value对的map函数，以及指定归纳与同一中间key\value关联的所有中间key\value的reduce函数。 如本文所示，许多现实世界的任务在这种模型中都是可以表达的。    用这种函数式编写的程序会自动并行化，并在大型计算机集群上执行。运行时系统负责对输入数据进行分区、安排跨机器的程序执行、处理机器故障和管理所需的机器间通信等细节。这使得没有任何并行和分布式系统经验的程序员可以轻松地利用大型分布式系统的资源。    我们的MapReduce实现运行在大量的普通机器上，并且具有高度的可伸缩性:典型的MapReduce计算在数千台机器上处理许多TB级的数据。程序员发现这个系统很容易使用:已经实现了数百个MapReduce程序，每天在谷歌集群上执行的MapReduce任务都超过1000个。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>【重点】</p><ul><li>MapReduce是用于处理和生成大数据集的编程模型（相关的实现）</li><li>包含map函数和reduce函数，使用key\value对</li><li>高度的可伸缩性</li></ul><h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>【原文】</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    Over the past five years, the authors and many others at Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web request logs, etc., to compute various kinds of derived data, such as inverted indices, various representations of the graph structure of web documents, summaries of the number of pages crawled per host, the set of most frequent queries in a given day, etc. Most such computations are conceptually straightforward. However, the input data is usually large and the computations have to be distributed across hundreds or thousands of machines in order to finish in a reasonable amount of time. The issues of how to parallelize the computation, distribute the data, and handle failures conspire to obscure the original simple computation with large amounts of complex code to deal with these issues.    As a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution and load balancing in a library. Our abstraction is inspired by the map and reduce primitives present in Lisp and many other functional languages. We realized that most of our computations involved applying a map operation to each logical “record” in our input in order to compute a set of intermediate key/value pairs, and then applying a reduce operation to all the values that shared the same key, in order to combine the derived data appropriately. Our use of a functional model with userspecified map and reduce operations allows us to parallelize large computations easily and to use re-execution as the primary mechanism for fault tolerance.    The major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high performance on large clusters of commodity PCs.    Section 2 describes the basic programming model and gives several examples. Section 3 describes an implementation of the MapReduce interface tailored towards our cluster-based computing environment. Section 4 describes several refinements of the programming model that we have found useful. Section 5 has performance measurements of our implementation for a variety of tasks. Section 6 explores the use of MapReduce within Google including our experiences in using it as the basis for a rewrite of our production indexing system. Section 7 discusses related and future work.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>【译文】</p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">    在过去的五年中，Google的作者和许多其他人已经实现了数百种特殊用途的计算，这些计算处理大量的原始数据（例如抓取的文档，Web请求日志等），以计算各种派生数据，例如：作为反向索引，Web文档的图形结构的各种表示形式，每个主机爬取的网页摘要，给定一天中最频繁的查询集等。大多数此类计算在概念上都很简单。 但是，输入数据通常很大，并且必须在数百或数千台计算机上分布计算，才能在合理的时间内完成计算。 如何并行化计算，分配数据和处理故障的问题，用大量复杂的代码来处理这些问题，使原来简单的计算变得模糊不清。    为了应对这种复杂性，我们设计了一个新的抽象，该抽象使我们能够表达我们试图执行的简单计算，但在库中隐藏了并行化，容错，数据分发和负载平衡的混乱细节。 Lisp和许多其他功能语言中的map和reduce原语启发了我们的抽象。 我们意识到，大多数计算都涉及对输入中的每个逻辑“记录”应用映射操作，以便计算一组key/value键/值对，然后对共享同一key的所有值应用归约操作，适当地组合得出的数据。我们使用具有用户指定的映射和归约运算的功能模型，使我们能够轻松地并行进行大型计算，并将重新执行用作容错的主要机制。    这项工作的主要贡献是一个简单而强大的界面，该界面可实现大规模计算的自动并行化和分配，并结合了该界面的实现，可在大型商用PC集群上实现高性能。    第2节描述了基本的编程模型，并给出了一些示例。 第3节介绍了针对我们基于集群的计算环境量身定制的MapReduce接口的实现。 第4节描述了一些有用的编程模型改进。 第5节对我们执行各种任务的性能进行了度量。 第6节探讨了MapReduce在Google中的用法，包括我们使用它作为重写生产索引系统基础的经验。 第7节讨论相关和未来的工作。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之flask</title>
      <link href="/2020/05/18/shi-yong-gong-ju-zhi-flask/"/>
      <url>/2020/05/18/shi-yong-gong-ju-zhi-flask/</url>
      
        <content type="html"><![CDATA[<h1 id="flask基础"><a href="#flask基础" class="headerlink" title="flask基础"></a>flask基础</h1><h1 id="flask进阶"><a href="#flask进阶" class="headerlink" title="flask进阶"></a>flask进阶</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.w3cschool.cn/flask/" target="_blank" rel="noopener">参考链接</a></li><li><a href="https://www.yiibai.com/flask" target="_blank" rel="noopener">参考链接2</a>——比较靠谱</li></ol><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读书笔记之金字塔原理</title>
      <link href="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/"/>
      <url>/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20210621161324696.png" alt="image-20210621161324696"></p><h1 id="第一篇：表达的逻辑"><a href="#第一篇：表达的逻辑" class="headerlink" title="第一篇：表达的逻辑"></a>第一篇：表达的逻辑</h1><ul><li>口头沟通能力</li><li>培训讲课能力</li><li>演讲能力</li><li>书面沟通能力</li><li>写作能力</li></ul><h2 id="第1章：为什么要用金字塔结构"><a href="#第1章：为什么要用金字塔结构" class="headerlink" title="第1章：为什么要用金字塔结构"></a>第1章：为什么要用金字塔结构</h2><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg1.jpg" alt="例子1"></p><h3 id="人类思维的基本规律"><a href="#人类思维的基本规律" class="headerlink" title="人类思维的基本规律"></a>人类思维的基本规律</h3><ul><li>大脑自动将信息归纳到金字塔结构的各组中，以便于理解和记忆</li><li>预先归纳到金字塔结构中的沟通内容，都更容易被人理解和记忆</li><li>应该有意将沟通内容组织成金字塔结构，包括口头和书面表达</li><li>==大脑的两个需求——只有金字塔结构满足==<ul><li>一次记忆不超过7个思想、概念或项目</li><li>自动寻找分析文章逻辑关系</li></ul></li></ul><h2 id="第2章：金字塔内部结构"><a href="#第2章：金字塔内部结构" class="headerlink" title="第2章：金字塔内部结构"></a>第2章：金字塔内部结构</h2><h3 id="纵向关系"><a href="#纵向关系" class="headerlink" title="纵向关系"></a>纵向关系</h3><h3 id="横向关系"><a href="#横向关系" class="headerlink" title="横向关系"></a>横向关系</h3><p>$\beta$</p><h3 id="单一思想统领的金字塔结构"><a href="#单一思想统领的金字塔结构" class="headerlink" title="单一思想统领的金字塔结构"></a>单一思想统领的金字塔结构</h3><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg2.jpg" alt="例子2"></p><h2 id="第3章：如何构建金字塔"><a href="#第3章：如何构建金字塔" class="headerlink" title="第3章：如何构建金字塔"></a>第3章：如何构建金字塔</h2><h3 id="归类分组，将思想组织成金字塔"><a href="#归类分组，将思想组织成金字塔" class="headerlink" title="归类分组，将思想组织成金字塔"></a>归类分组，将思想组织成金字塔</h3><h3 id="自上而下表达，结论先行"><a href="#自上而下表达，结论先行" class="headerlink" title="自上而下表达，结论先行"></a>自上而下表达，结论先行</h3><h3 id="自下而上思考，总结概括"><a href="#自下而上思考，总结概括" class="headerlink" title="自下而上思考，总结概括"></a>自下而上思考，总结概括</h3><h3 id="初学者注意事项"><a href="#初学者注意事项" class="headerlink" title="初学者注意事项"></a>初学者注意事项</h3><h3 id="梳理思想"><a href="#梳理思想" class="headerlink" title="梳理思想"></a>梳理思想</h3><ul><li>不要一坐下来就开始将思想往金字塔结构里面套</li><li>而是应该先梳理你想要表达的思想</li><li>为了吸引读者的全部注意力，你必须在做好回答问题的准备之前，==避免引起读者的疑问==；也必须在引起读者疑问之前，避免先给出对该问题的答案</li></ul><h5 id="金字塔思想的关联方式"><a href="#金字塔思想的关联方式" class="headerlink" title="金字塔思想的关联方式"></a>金字塔思想的关联方式</h5><h6 id="纵向：文章中任一层次上的思想必须是其下一层次思想的概括"><a href="#纵向：文章中任一层次上的思想必须是其下一层次思想的概括" class="headerlink" title="纵向：文章中任一层次上的思想必须是其下一层次思想的概括"></a>纵向：文章中任一层次上的思想必须是其下一层次思想的概括</h6><ul><li>向上</li><li>向下</li></ul><h6 id="横向"><a href="#横向" class="headerlink" title="横向"></a>横向</h6><ul><li>每组思想必须属于同一逻辑范畴</li><li>每组思想必须按逻辑顺序组织，具体的顺序取决于该组思想的逻辑关系是==演绎推理关系还是归纳推理关系==</li><li>演绎推理与归纳推理的区别如下图</li><li>演绎推理的3个步骤<ul><li>阐述世界上已存在的某种情况</li><li>阐述世界上同时存在的相关情况。如果第二个表述是针对第一个表述的主语或者谓语，则说明两个表述相关</li><li>说明这两个表述同时存在时隐含的意义</li></ul></li><li>归纳推理的步骤<ul><li>找到一个能够表示该组所有思想共同点的名词，即正确定义该组思想</li><li>识别并剔除不属该组，不具有共同点的思想</li></ul></li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg10.jpg" alt="区别"></p><h4 id="组织思想的逻辑顺序"><a href="#组织思想的逻辑顺序" class="headerlink" title="组织思想的逻辑顺序"></a>组织思想的逻辑顺序</h4><h5 id="演绎关系"><a href="#演绎关系" class="headerlink" title="演绎关系"></a>演绎关系</h5><ul><li>大前提</li><li>小前提</li><li>结论</li></ul><h5 id="归纳关系"><a href="#归纳关系" class="headerlink" title="归纳关系"></a>归纳关系</h5><h6 id="时间顺序：确定前因后果关系，代表一个过程或系统的一组行为"><a href="#时间顺序：确定前因后果关系，代表一个过程或系统的一组行为" class="headerlink" title="时间顺序：确定前因后果关系，代表一个过程或系统的一组行为"></a>时间顺序：确定前因后果关系，代表一个过程或系统的一组行为</h6><ul><li>根据结果找原因，区分原因和结果</li><li>结果<ul><li>第一（原因）</li><li>第二</li><li>第三</li></ul></li><li>揭示隐含的逻辑思路</li><li>在按照时间顺序组织的思想组中，你要按照采取行动的顺序（第一步、第二步、第三步）依次表述达到某一个结果必须采取的行动</li></ul><h6 id="空间顺序：将整体分割成部分，或将部分组成整体"><a href="#空间顺序：将整体分割成部分，或将部分组成整体" class="headerlink" title="空间顺序：将整体分割成部分，或将部分组成整体"></a>空间顺序：将整体分割成部分，或将部分组成整体</h6><ul><li>结构顺序就是当你使用示意图、地图、图画或照片想象某事务时的顺序</li><li>创建逻辑结构<ul><li>各部分之间相互独立，没有重叠，有排他性</li><li>所有部分完全穷尽，没有遗漏</li></ul></li><li>划分组织活动的方式<ul><li>根据活动本身（研发、生产、市场）</li><li>根据活动发生地点（国家东部、国家中西部）</li><li>根据针对特定产品、市场或客户活动的集合（各事业部、各业务单元）</li></ul></li><li>描述逻辑结构<ul><li>自上而下</li><li>自作而右</li></ul></li><li>修改逻辑结构</li><li>用结构顺序概念检查思路</li><li>地图<ul><li>北京</li><li>上海</li><li>广州</li><li>深圳</li></ul></li></ul><h6 id="重要性顺序：将类似事务按重要性归为一组"><a href="#重要性顺序：将类似事务按重要性归为一组" class="headerlink" title="重要性顺序：将类似事务按重要性归为一组"></a>重要性顺序：将类似事务按重要性归为一组</h6><ul><li>明确指明每组中的项目具有共同特性，确保将所有具有该特性的项目列入该组</li><li>在每组中，根据各个问题具有该特性的高低排序——最具有该特性的问题排第一，先强后弱，先重要后次要</li><li>创建适当的分组</li><li>最重要</li><li>次重要</li><li>不重要</li></ul><h3 id="自上而下法"><a href="#自上而下法" class="headerlink" title="自上而下法"></a>自上而下法</h3><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg3.jpg" alt="自上而下搭建步骤"></p><h4 id="画出主题方框——提出主题思想"><a href="#画出主题方框——提出主题思想" class="headerlink" title="画出主题方框——提出主题思想"></a>画出主题方框——提出主题思想</h4><ul><li>要讨论什么主题，知道则填入，不知道则到步骤2</li></ul><h4 id="设想主要疑问——设想受众的主要疑问"><a href="#设想主要疑问——设想受众的主要疑问" class="headerlink" title="设想主要疑问——设想受众的主要疑问"></a>设想主要疑问——设想受众的主要疑问</h4><ul><li>确定文章的读者，将面对哪些对象</li><li>希望回答关于主题的哪些疑问，没有则到步骤4</li></ul><h4 id="写出对该疑问的回答——写序言：背景、冲突、疑问、回答"><a href="#写出对该疑问的回答——写序言：背景、冲突、疑问、回答" class="headerlink" title="写出对该疑问的回答——写序言：背景、冲突、疑问、回答"></a>写出对该疑问的回答——写序言：背景、冲突、疑问、回答</h4><ul><li>如果写不出，则注明有能力回答该疑问</li></ul><h4 id="说明“背景”"><a href="#说明“背景”" class="headerlink" title="说明“背景”"></a>说明“背景”</h4><ul><li>把要讨论的主题与“背景”相结合，作出关于该主题的第一个不会引起争议的表述</li></ul><h4 id="指出“冲突”——与受众进行疑问-回答式对话"><a href="#指出“冲突”——与受众进行疑问-回答式对话" class="headerlink" title="指出“冲突”——与受众进行疑问/回答式对话"></a>指出“冲突”——与受众进行疑问/回答式对话</h4><ul><li>进行疑问/回答式对话</li><li>考虑“背景”中发生了哪些能使读者产生疑问的“冲突”</li><li>例如发生了读者不知道的意外，出现了某个问题等</li></ul><h4 id="检查“主要疑问”和“答案”——重复进行疑问-回答式对话"><a href="#检查“主要疑问”和“答案”——重复进行疑问-回答式对话" class="headerlink" title="检查“主要疑问”和“答案”——重复进行疑问/回答式对话"></a>检查“主要疑问”和“答案”——重复进行疑问/回答式对话</h4><ul><li>对“背景”中“冲突”的介绍，应当直接导致读者提出主要疑问</li><li>否则应该重新介绍“背景”中的“冲突”，使之可以直接导致读者提出主要疑问</li></ul><h3 id="自下而上法"><a href="#自下而上法" class="headerlink" title="自下而上法"></a>自下而上法</h3><ul><li>列出你想表达的所有思想要点</li><li>找出各要点之间的逻辑关系</li><li>得出结论</li></ul><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>一定先搭结构，先尝试自上而下法</li><li>序言先写背景，将背景作为序言的起点</li><li>先多花时间思考序言，不要省略</li><li>将历史背景放在序言中</li><li>序言仅涉及读者不会对其真实性提出质疑的内容</li><li>在关键句层次上，更宜选择归纳推理法而非演绎论证法</li></ul><h2 id="第4章：序言的结构以及如何写"><a href="#第4章：序言的结构以及如何写" class="headerlink" title="第4章：序言的结构以及如何写"></a>第4章：序言的结构以及如何写</h2><h3 id="序言的结构"><a href="#序言的结构" class="headerlink" title="序言的结构"></a>序言的结构</h3><h3 id="序言如何写"><a href="#序言如何写" class="headerlink" title="序言如何写"></a>序言如何写</h3><h3 id="序言的常见模式"><a href="#序言的常见模式" class="headerlink" title="序言的常见模式"></a>序言的常见模式</h3><ul><li>为什么序言必须是将故事？<ul><li>让读者抛开复杂的思想，专注于你的文章</li><li>引起读者的兴趣，吸引注意力</li></ul></li><li>为什么序言必须是读者已知的信息？<ul><li>这样才不会让读者产生质疑</li></ul></li></ul><h3 id="序言的结构-1"><a href="#序言的结构-1" class="headerlink" title="序言的结构"></a>序言的结构</h3><h4 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg4.jpg" alt="冲突"></p><ul><li>背景（S）<ul><li>何时引入？</li><li>在你能够作出关于文章主题独立、无争议的表述的时候</li><li>所有引出“背景”的句子都能锁定在特定的时间和空间</li></ul></li><li>冲突（C）<ul><li>能够促使读者提出“疑问”的即是冲突</li><li>推动故事情节发展</li></ul></li><li>疑问（Q）</li><li>回答（A）——关键句要点<ul><li>不仅要回答==文章主题思想==引起读者的疑问</li><li>还要呈现文章的框架结构</li><li>然后本章开始以关键句要点一章一章的讲述——同样采取背景+冲突+回答的格式</li></ul></li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg5.jpg" alt="序言结构"></p><h4 id="基本结构引出的形式"><a href="#基本结构引出的形式" class="headerlink" title="基本结构引出的形式"></a>基本结构引出的形式</h4><ul><li>标准式：背景——冲突——答案</li><li>开门见山式：答案——背景——冲突</li><li>突出忧虑式：冲突——背景——答案</li><li>突出信心式：疑问——背景——冲突——答案</li></ul><h3 id="好序言的原则"><a href="#好序言的原则" class="headerlink" title="好序言的原则"></a>好序言的原则</h3><ul><li>序言的目的是==提示==读者而不是==告诉==读者某些信息<ul><li>不应该含有读者需要证明后才能接受的信息</li><li>不应当含有图表</li></ul></li><li>序言必须包含讲故事的3个要素+背景+冲突+答案</li><li>序言的长度取决于读者和主题的需要</li></ul><h3 id="序言的常见模式-1"><a href="#序言的常见模式-1" class="headerlink" title="序言的常见模式"></a>序言的常见模式</h3><h4 id="发出指示式"><a href="#发出指示式" class="headerlink" title="发出指示式"></a>发出指示式</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg6.jpg" alt="发出指示式"></p><h4 id="请求支持式"><a href="#请求支持式" class="headerlink" title="请求支持式"></a>请求支持式</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg7.jpg" alt="请求支持式"></p><h4 id="解释做法式"><a href="#解释做法式" class="headerlink" title="解释做法式"></a>解释做法式</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg8.jpg" alt="解释做法式"></p><h4 id="比较选择式"><a href="#比较选择式" class="headerlink" title="比较选择式"></a>比较选择式</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg9.jpg" alt="比较选择式"></p><h2 id="第5章：演绎推理与归纳推理"><a href="#第5章：演绎推理与归纳推理" class="headerlink" title="第5章：演绎推理与归纳推理"></a>第5章：演绎推理与归纳推理</h2><h3 id="演绎推理"><a href="#演绎推理" class="headerlink" title="演绎推理"></a>演绎推理</h3><h3 id="归纳推理"><a href="#归纳推理" class="headerlink" title="归纳推理"></a>归纳推理</h3><h3 id="它们的区别"><a href="#它们的区别" class="headerlink" title="它们的区别"></a>它们的区别</h3><h1 id="第二篇：思考的逻辑"><a href="#第二篇：思考的逻辑" class="headerlink" title="第二篇：思考的逻辑"></a>第二篇：思考的逻辑</h1><ul><li>如何才能从原文罗列的信息，整理出修改后罗列的结论呢？</li><li>本篇将讨论有关的实用技巧</li><li>这个过程称为==冷静思考==</li></ul><h2 id="第6章：应用逻辑顺序"><a href="#第6章：应用逻辑顺序" class="headerlink" title="第6章：应用逻辑顺序"></a>第6章：应用逻辑顺序</h2><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg11.jpg" alt="大脑归纳分析活动"></p><h2 id="第7章：概括各组思想"><a href="#第7章：概括各组思想" class="headerlink" title="第7章：概括各组思想"></a>第7章：概括各组思想</h2><ul><li>概括各组思想</li><li>确保思想属于同一组，应抽象、提炼、概括思想精华</li><li><p>总结句避免使用“缺乏思想”的句子，总结句应该包含概括的思想</p><ul><li>例如：该公司存在2个问题</li></ul></li><li><p>思想的表达方式可以是==行动性语句==，即告诉读者什么事，也可以是==描述性语句==，即告诉读者关于某些事的情况</p><ul><li><p>概括行动性思想（介绍采取的行动、行为、步骤、流程）时，应说明采取行动后取得的“结果”（效果、达到的目标）</p><ul><li>在将各行动（步骤、流程等）联系起来之前，先用明确的语句描述各行动</li><li>找出明显的因果关系组合，尽量将每一组中的行动、步骤控制在5个以下<ul><li>区分行动步骤的层次<br><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg12.jpg" alt="区分层次"></li></ul></li><li>直接从这些行动、步骤、流程，总结、概括行动的结果、目标</li></ul></li><li><p>概括描述性思想（介绍背景、信息）时，应该说明这些思想具有的“共同点的含义”</p></li></ul></li></ul><h3 id="了解某一组思想真正想表达的思想观点"><a href="#了解某一组思想真正想表达的思想观点" class="headerlink" title="了解某一组思想真正想表达的思想观点"></a>了解某一组思想真正想表达的思想观点</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ul><li>确定该组思想的类型（类别）</li><li>将同一类别的思想归类、分组</li><li>找出各类别思想之间的顺序</li></ul><h3 id="总结句避免使用“缺乏思想”的句子"><a href="#总结句避免使用“缺乏思想”的句子" class="headerlink" title="总结句避免使用“缺乏思想”的句子"></a>总结句避免使用“缺乏思想”的句子</h3><h3 id="总结句要说明行动产生的结果-目标"><a href="#总结句要说明行动产生的结果-目标" class="headerlink" title="总结句要说明行动产生的结果/目标"></a>总结句要说明行动产生的结果/目标</h3><h3 id="找出各结论之间的共性"><a href="#找出各结论之间的共性" class="headerlink" title="找出各结论之间的共性"></a>找出各结论之间的共性</h3><h1 id="第三篇：解决问题的逻辑"><a href="#第三篇：解决问题的逻辑" class="headerlink" title="第三篇：解决问题的逻辑"></a>第三篇：解决问题的逻辑</h1><h2 id="第8章：界定问题"><a href="#第8章：界定问题" class="headerlink" title="第8章：界定问题"></a>第8章：界定问题</h2><ul><li>“问题”是指你不喜欢某一结果-非期望结果R1（比如销售额降低），想得到其他结果-期望结果R2（比如销售额增长）</li><li>解决方案则是指如何从现状R1到目标R2</li></ul><ol><li>界定问题的框架</li><li>展开问题的各要素</li><li>发掘读者的疑问</li><li>开始写序言</li><li>实战案例</li></ol><h3 id="界定问题的框架"><a href="#界定问题的框架" class="headerlink" title="界定问题的框架"></a>界定问题的框架</h3><h4 id="展开说明框架中的各要素"><a href="#展开说明框架中的各要素" class="headerlink" title="展开说明框架中的各要素"></a>展开说明框架中的各要素</h4><ul><li>界定问题的框架需要回答3个问题<ul><li>发生了什么事情？（背景【切入点+困扰】）</li><li>我们不喜欢它什么？（非期望结果R1）</li><li>我们想要什么？（期望结果R2）</li></ul></li></ul><h4 id="把“界定的问题”写成序言"><a href="#把“界定的问题”写成序言" class="headerlink" title="把“界定的问题”写成序言"></a>把“界定的问题”写成序言</h4><ul><li>界定问题的框架可以遵循以下5个步骤<ul><li>展开问题的基本部分</li><li>你的解决方案处于哪一阶段（已经提出了，还是已经被接受了）</li><li>提出适当的疑问</li><li>检查序言是否呈现了界定的问题</li><li>检查金字塔是否回答了疑问</li></ul></li></ul><h3 id="展开问题的各要素"><a href="#展开问题的各要素" class="headerlink" title="展开问题的各要素"></a>展开问题的各要素</h3><ul><li>确定以下4个要素才能界定问题<ul><li>切入点/序幕</li><li>困扰/困惑</li><li>现状，非期望结果</li><li>目标，期望结果</li></ul></li></ul><h4 id="切入点-序幕"><a href="#切入点-序幕" class="headerlink" title="切入点/序幕"></a>切入点/序幕</h4><ul><li>设想自己静静的坐在一个黑暗的剧场里。大幕拉开，舞台布景描绘的是某一特定时刻的某一特定地点，这就是切入点或序幕</li><li>在序幕阶段，尽量作简单的设想和简短的描述，等到写序言时再展开</li></ul><h4 id="困扰-困惑"><a href="#困扰-困惑" class="headerlink" title="困扰/困惑"></a>困扰/困惑</h4><ul><li>某一事件的发生使剧情得以展开，并引发了非期望结果R1，这就是困扰/困惑</li><li>产生的原因有以下3种<ul><li>外部原因</li><li>内部原因</li><li>近期认识到的其他原因</li></ul></li></ul><h4 id="R1（现状、非期望结果）"><a href="#R1（现状、非期望结果）" class="headerlink" title="R1（现状、非期望结果）"></a>R1（现状、非期望结果）</h4><ul><li>R1是指读者需要设法解决或有可能面临的问题，或者是有可能抓住的机会</li><li>困扰带来的R1可能不止一个，可以在示意图里尽可能简短的文字加以说明</li></ul><h4 id="R2（目标，期望结果）"><a href="#R2（目标，期望结果）" class="headerlink" title="R2（目标，期望结果）"></a>R2（目标，期望结果）</h4><ul><li>对R2准确到位的描述，可以用具体的数字，也可以用具体的最终结果</li><li>有时你可能无法具体描述R2的最终结果，或者根本不能描述，这种情况，只需在R2部分写：如果问题得到解决，你希望达到的状态</li></ul><h3 id="发掘读者的疑问"><a href="#发掘读者的疑问" class="headerlink" title="发掘读者的疑问"></a>发掘读者的疑问</h3><ul><li>展开问题的基本内容后，下一步就是寻找读者的疑问</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg13.jpg" alt="读者希望解决的问题大概"></p><h3 id="开始写序言"><a href="#开始写序言" class="headerlink" title="开始写序言"></a>开始写序言</h3><ul><li>针对上图7种问题，写出不同的序言</li></ul><h4 id="1、我们应该做什么"><a href="#1、我们应该做什么" class="headerlink" title="1、我们应该做什么"></a>1、我们应该做什么</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg14.jpg" alt="1"></p><h4 id="2、我们是否应该做我们想做的事"><a href="#2、我们是否应该做我们想做的事" class="headerlink" title="2、我们是否应该做我们想做的事"></a>2、我们是否应该做我们想做的事</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg14.jpg" alt="2"></p><h4 id="3、我们应该如何做我们想做的事"><a href="#3、我们应该如何做我们想做的事" class="headerlink" title="3、我们应该如何做我们想做的事"></a>3、我们应该如何做我们想做的事</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg16.jpg" alt="3"></p><h4 id="4、解决方案行不通，我们应该做什么"><a href="#4、解决方案行不通，我们应该做什么" class="headerlink" title="4、解决方案行不通，我们应该做什么"></a>4、解决方案行不通，我们应该做什么</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg17.jpg" alt="4"></p><h4 id="5、我们应该选哪种方案"><a href="#5、我们应该选哪种方案" class="headerlink" title="5、我们应该选哪种方案"></a>5、我们应该选哪种方案</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg18.jpg" alt="5"></p><h4 id="6、我们应该采用哪些战略"><a href="#6、我们应该采用哪些战略" class="headerlink" title="6、我们应该采用哪些战略"></a>6、我们应该采用哪些战略</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg19.jpg" alt="6"></p><h4 id="7、我们存在问题吗"><a href="#7、我们存在问题吗" class="headerlink" title="7、我们存在问题吗"></a>7、我们存在问题吗</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg20.jpg" alt="7"></p><h3 id="实战案例"><a href="#实战案例" class="headerlink" title="实战案例"></a>实战案例</h3><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg21.jpg" alt="8"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg22.jpg" alt="9"></p><h2 id="第9章：结构化分析问题"><a href="#第9章：结构化分析问题" class="headerlink" title="第9章：结构化分析问题"></a>第9章：结构化分析问题</h2><ul><li>分析问题的标准流程：<ol><li>收集信息</li><li>描述发现</li><li>得出结论</li><li>提出方案</li></ol></li></ul><h3 id="从信息资料入手-一般的方法"><a href="#从信息资料入手-一般的方法" class="headerlink" title="从信息资料入手-一般的方法"></a>从信息资料入手-一般的方法</h3><ul><li>不论客户存在什么问题，都从全公司或全行业分析入手</li><li>收集大量资料再进行分析会耗费大量的时间和金钱</li></ul><h3 id="设计诊断框架-作者提出的结构化分析方法"><a href="#设计诊断框架-作者提出的结构化分析方法" class="headerlink" title="设计诊断框架-作者提出的结构化分析方法"></a>设计诊断框架-作者提出的结构化分析方法</h3><ul><li>结构化分析的方法有3种<ol><li>呈现有形的结构</li><li>寻找因果关系和归类分组</li><li>为了找出产生问题的原因</li></ol></li></ul><h4 id="呈现有形的结构"><a href="#呈现有形的结构" class="headerlink" title="呈现有形的结构"></a>呈现有形的结构</h4><ul><li>任何一家企业或行业的具体领域都应该有清晰的结构</li><li>即包括由不同单位组成的系统，各自完成特定的功能</li><li>画一幅系统的现况或理想状况的图，帮助你分析和解决问题</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg23.jpg" alt="行业业务流程"></p><h4 id="寻找因果关系"><a href="#寻找因果关系" class="headerlink" title="寻找因果关系"></a>寻找因果关系</h4><ul><li>诊断问题的第二种方法是寻找具有因果关系的要素，行为或任务，得出诊断结果</li></ul><h4 id="将产生问题的可能原因分类"><a href="#将产生问题的可能原因分类" class="headerlink" title="将产生问题的可能原因分类"></a>将产生问题的可能原因分类</h4><ul><li>第三种方法是将所有可能的原因按相似性分类</li></ul><h3 id="使用诊断框架"><a href="#使用诊断框架" class="headerlink" title="使用诊断框架"></a>使用诊断框架</h3><h3 id="建立逻辑树"><a href="#建立逻辑树" class="headerlink" title="建立逻辑树"></a>建立逻辑树</h3><ul><li>逻辑树可以解决我们能做什么？我们应该做什么的问题</li><li>使用逻辑树可以从逻辑上找出解决问题的可能方案</li><li>用逻辑树展示各组活动的组与组之间的相互关系的方法，也可以进行逻辑分析，找出各组思想的缺陷</li></ul><h3 id="是非问题分析"><a href="#是非问题分析" class="headerlink" title="是非问题分析"></a>是非问题分析</h3><ul><li>建立诊断框架的过程有时候被称为“是非问题分析”。但是“是非问题分析”一词广义上指几乎所有的逻辑树</li></ul><h1 id="第四篇：演示的逻辑"><a href="#第四篇：演示的逻辑" class="headerlink" title="第四篇：演示的逻辑"></a>第四篇：演示的逻辑</h1><h2 id="第10章：在书面上呈现金字塔"><a href="#第10章：在书面上呈现金字塔" class="headerlink" title="第10章：在书面上呈现金字塔"></a>第10章：在书面上呈现金字塔</h2><p>写长篇文章时，在页面上呈现金字塔层级的方法很多，常见的有：</p><ol><li>多级标题法</li><li>下划线法</li><li>数字编号法</li><li>行首缩进法</li><li>项目符号法</li></ol><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg24.jpg" alt="多级标题"></p><h3 id="突出显示文章的框架结构"><a href="#突出显示文章的框架结构" class="headerlink" title="突出显示文章的框架结构"></a>突出显示文章的框架结构</h3><ul><li>文章短，支持每一关键句要点的段落少于2个，读者很容易明白文中要点及其相互关系，则给==要点加上下划线==即可</li><li>文章长，支持每一关键句要点的段落多于2个，则使用==标题介绍==来阐述观点</li></ul><h4 id="多级标题法"><a href="#多级标题法" class="headerlink" title="多级标题法"></a>多级标题法</h4><ul><li>不同层次的思想用不同的标记区分</li><li>层次越低的思想离页面的右端越近</li><li>同一层次的思想采用同一表现形式</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg25.jpg" alt="多级标题法"></p><p><strong>注意点：</strong></p><ol><li>每一层级的标题不可能只有一个，文章里不可能只有一个章标题，一个节标题，一个小节标题，一个编号段落等等</li><li>相同的思想应使用相同的句型，每一组思想都是同一类的思想，为了强调一致性，一组中的所有标题应该采用同样的句子顺序，即对仗。如果第一小节的标题中第一个词是名词，其他各小节标题的第一个词也应该是名词</li><li>标题用词应提炼思想的精髓，尽量简明扼要</li><li>标题与正文应分开考虑，标题为眼球，正文为头脑</li><li>每组标题应提前集中介绍，为了说明该组标题将解释或讨论的注意论点，以及将要提出的思想观点</li><li>不要滥用标题，只有当标题有助于说清楚你想表达的信息，有助于读者领会你思想的细节时才使用</li></ol><h4 id="下划线法"><a href="#下划线法" class="headerlink" title="下划线法"></a>下划线法</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg26.jpg" alt="下划线法"></p><ul><li>图中排版格式比较难看，很多人采用下面的下划线法</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg27.jpg" alt="下划线法-1"></p><p><strong>注意：</strong></p><ol><li>必须严格使用疑问/回答结构</li><li>必须注意论点的措辞，说明论点的信息越少越好</li><li>必须坚决把论点限制在演绎推理和归纳推理的框架内</li></ol><h4 id="数字编号法"><a href="#数字编号法" class="headerlink" title="数字编号法"></a>数字编号法</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg28.jpg" alt="数字编号法"></p><h4 id="行首缩进法"><a href="#行首缩进法" class="headerlink" title="行首缩进法"></a>行首缩进法</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg29.jpg" alt="行首缩进法"></p><ul><li>采用行首缩进法时，要用相同的句型表达观点</li></ul><h4 id="项目符号法"><a href="#项目符号法" class="headerlink" title="项目符号法"></a>项目符号法</h4><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/eg30.jpg" alt="项目符号法"></p><h3 id="上下文之间要有过渡"><a href="#上下文之间要有过渡" class="headerlink" title="上下文之间要有过渡"></a>上下文之间要有过渡</h3><ul><li>写完序言后，进入文章正文，需要写一段简短的文字介绍每一关键句要点</li><li>不要把两个章节在做什么连接在一起，而应该把它们说的内容（即主要思想观点）连接在一起</li></ul><h4 id="讲故事"><a href="#讲故事" class="headerlink" title="讲故事"></a>讲故事</h4><ul><li>一种把读者带入关键句要点的好方法是向他讲述背景-冲突-疑问三部曲，自然而然地引出关键句要点作为故事的答案</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200528092834833.png" alt="image-20200528092834833"></p><h4 id="承上启下"><a href="#承上启下" class="headerlink" title="承上启下"></a>承上启下</h4><ul><li>从金字塔结构的前一部分挑选一个词、一句短语或总结其中心思想，把它用在下一部分的起始句中</li></ul><h4 id="总结各部分内容"><a href="#总结各部分内容" class="headerlink" title="总结各部分内容"></a>总结各部分内容</h4><ul><li>有时遇到一个章节特别长或特别复杂，希望停下来作一个完整总结后再继续往下写。这时记住其目的是尽量凝练地复述前文的主要论点和基调</li></ul><h4 id="得出完整结论"><a href="#得出完整结论" class="headerlink" title="得出完整结论"></a>得出完整结论</h4><ul><li>亚里士多德关于如何下结论的忠告</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200528094014157.png" alt="image-20200528094014157"></p><h4 id="说明下一步行动"><a href="#说明下一步行动" class="headerlink" title="说明下一步行动"></a>说明下一步行动</h4><ul><li>如果你的文章是建议读者采取一系列你认为他有可能采取的行动，而且篇幅较长，就有必要说明下一步行动</li></ul><h2 id="第11章：在PPT中呈现金字塔"><a href="#第11章：在PPT中呈现金字塔" class="headerlink" title="第11章：在PPT中呈现金字塔"></a>第11章：在PPT中呈现金字塔</h2><p>设计PPT演示文稿的人必须了解和掌握的最基本规则：</p><ol><li>文字幻灯片应只包含最重要的、经过适当分组和总结的思想，叙述时应尽量简洁</li><li>演示文稿应图文并茂，使用各种图表相配合</li><li>演示文稿应呈现经过深思熟虑后的故事梗概和剧本</li></ol><p>演示文稿包括两类幻灯片：</p><ol><li>文字</li><li>图表</li></ol><p>理想的比例是图表占90%，文字占10%，其各自的作用是：</p><ol><li>说明演示文稿的框架结构（文字幻灯片）</li><li>强调重要的思想、观点、结论、论点、建议或要采取的措施等（文字幻灯片）</li><li>阐明单用文字难以说清楚的数据、关系（图表）</li></ol><h3 id="设计文字PPT幻灯片"><a href="#设计文字PPT幻灯片" class="headerlink" title="设计文字PPT幻灯片"></a>设计文字PPT幻灯片</h3><ul><li>你所说的和你在屏幕上演示的应该有明显的区别</li></ul><h4 id="清楚你要说的内容"><a href="#清楚你要说的内容" class="headerlink" title="清楚你要说的内容"></a>清楚你要说的内容</h4><ul><li>好的幻灯片总是尽可能直接简单地传递信息，不把文字浪费在那些可以通过口头表达的转折性或介绍性语言上</li><li>文字幻灯片最好只用于强调金字塔中的主要论点，如图所示</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200529094300768.png" alt="image-20200529094300768"></p><h4 id="清楚你要演示的内容"><a href="#清楚你要演示的内容" class="headerlink" title="清楚你要演示的内容"></a>清楚你要演示的内容</h4><p>指导性原则</p><ol><li>每次只演示和说明一个论点</li><li>论点应使用完整的陈述句（销售前景看好），而不是标题性语言（销售前景）</li><li>文字应尽量简短</li><li>使用简单的词汇和数字、比如490万美元就比4876987美元容易记</li><li>字号应足够大，最远一位观众到屏幕的距离（英尺）除以32，得到的是你的PPT可以用的最小字号（英寸）</li><li>注意幻灯片的趣味性</li><li>用逐级展开呈现，提高趣味性，如下图</li></ol><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200601092702072.png" alt="image-20200601092702072"></p><h3 id="设计图表PPT幻灯片"><a href="#设计图表PPT幻灯片" class="headerlink" title="设计图表PPT幻灯片"></a>设计图表PPT幻灯片</h3><ul><li>图表幻灯片传递的信息应该尽量简单易懂，因为观众没有机会对它们仔细研究并找出各个部分的含义</li><li>图表幻灯片通常用饼图、条形图、柱状图、曲线图或散点图，表示某一结构或流程的组成部分</li><li>制作图表幻灯片的诀窍是：确定你想用图表回答的问题，把答案作为图表的标题，然后选择最合适表达论点的图表样式</li></ul><h3 id="故事梗概"><a href="#故事梗概" class="headerlink" title="==故事梗概=="></a>==故事梗概==</h3><h4 id="具体怎么做"><a href="#具体怎么做" class="headerlink" title="具体怎么做"></a>具体怎么做</h4><p>从金字塔结构到演示文稿，作者通常采用以下方法：</p><ol><li>序言尽量写得详细，把每个想说的词汇按照希望的顺序写下来</li><li>用写故事梗概的形式。每张幻灯片写作的顺序，从上往下依次为<ol><li>序言各要素、</li><li>关键句要点</li><li>关键句下一层次的论点</li></ol></li><li>初步决定你准备采用的呈现方法，只要知道数据的类型和想要表现何种关系即可</li><li>准备好每张幻灯片的讲稿，确保整个演示像讲故事一样流畅</li><li>完成幻灯片的设计和绘图</li><li>排练，排练，再排练！</li></ol><p><strong>提示：</strong>可以用一张白纸写最简单的故事梗概，把它分成若干个区域，每个区域代表一张空白幻灯片，写上你想要阐述的要点，并说明哪些需要用文字幻灯片，哪些需要用图片幻灯片</p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><ul><li>将以下金字塔结构写成幻灯片</li><li>金字塔结构</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200601094943816.png" alt="image-20200601094943816"></p><ul><li>幻灯片</li></ul><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200601095046018.png" alt="image-20200601095046018"><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200601095102552.png" alt="image-20200601095102552"></p><h3 id="本章总结"><a href="#本章总结" class="headerlink" title="本章总结"></a>本章总结</h3><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200601095235815.png" alt="image-20200601095235815"></p><h2 id="第12章：在字里行间呈现金字塔"><a href="#第12章：在字里行间呈现金字塔" class="headerlink" title="第12章：在字里行间呈现金字塔"></a>第12章：在字里行间呈现金字塔</h2><p>做到条理清晰的两个步骤：</p><ul><li>首先决定要说明的思想或要证明的论点</li><li>然后用文字表现</li><li>强迫自己想象各种思想观点之间的内在关系，头脑里有了清晰的图像后，就能立刻把它转换成清楚的句子</li></ul><h3 id="画脑图"><a href="#画脑图" class="headerlink" title="画脑图"></a>画脑图</h3><ul><li>利用脑图帮助自己记忆，也让自己的文章能被别人记住</li></ul><h3 id="把图像复制成文字"><a href="#把图像复制成文字" class="headerlink" title="把图像复制成文字"></a>把图像复制成文字</h3><ul><li>文章要让读者在阅读时感到愉悦。简单的方法就是：主观臆想那些当初用来得出思想的图像</li></ul><h1 id="附录：本书要点汇总"><a href="#附录：本书要点汇总" class="headerlink" title="附录：本书要点汇总"></a>附录：本书要点汇总</h1><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602093917094.png" alt="image-20200602093917094"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094149099.png" alt="image-20200602094149099"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094212128.png" alt="image-20200602094212128"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094234885.png" alt="image-20200602094234885"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094252755.png" alt="image-20200602094252755"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094323021.png" alt="image-20200602094323021"><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094340478.png" alt="image-20200602094340478"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094421615.png" alt="image-20200602094421615"><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094447994.png" alt="image-20200602094447994"><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094855941.png" alt="image-20200602094855941"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602094944007.png" alt="image-20200602094944007"><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602095019423.png" alt="image-20200602095019423"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602095102189.png" alt="image-20200602095102189"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602095118549.png" alt="image-20200602095118549"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602095128872.png" alt="image-20200602095128872"></p><p><img src="/2020/04/24/du-shu-bi-ji-zhi-jin-zi-ta-yuan-li/image-20200602095138041.png" alt="image-20200602095138041"></p>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 职场能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算广告之细节营销</title>
      <link href="/2020/04/14/ji-suan-guang-gao-zhi-xi-jie-ying-xiao/"/>
      <url>/2020/04/14/ji-suan-guang-gao-zhi-xi-jie-ying-xiao/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/04/14/ji-suan-guang-gao-zhi-xi-jie-ying-xiao/image-20210621155424363.png" alt="image-20210621155424363"></p><h1 id="第一章：何为市场营销"><a href="#第一章：何为市场营销" class="headerlink" title="第一章：何为市场营销"></a>第一章：何为市场营销</h1><h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><ul><li>市场营销只是常识吗？显然不是的</li><li>市场营销原则适用于所有行业</li><li>市场营销不是销售</li></ul><h2 id="语录"><a href="#语录" class="headerlink" title="语录"></a>语录</h2><ul><li>你可以借鉴任何行业任何公司的成功经验</li><li>市场营销工具（4P）转化为客户体验（4C）</li><li>帮助你的客户，让他们的生活更方便、更舒适，这样做会帮你省钱</li><li>一个问题以及该问题的解决方法未必处于同一个P的范畴</li><li>寻找无题之解，开拓创新之道</li></ul><h2 id="4P-生产商的视角"><a href="#4P-生产商的视角" class="headerlink" title="4P-生产商的视角"></a>4P-生产商的视角</h2><ul><li>产品</li><li>渠道</li><li>价格</li><li>促销</li></ul><h2 id="4C-客户的视角"><a href="#4C-客户的视角" class="headerlink" title="4C-客户的视角"></a>4C-客户的视角</h2><ul><li>客户的需求，而不是产品</li><li>成本，而不是价格</li><li>沟通，而不是促销</li><li>便利，而不是渠道</li></ul><p><img src="/2020/04/14/ji-suan-guang-gao-zhi-xi-jie-ying-xiao/4p_4c.jpg" alt="4P-4C"></p><h1 id="第二章：市场营销的研发"><a href="#第二章：市场营销的研发" class="headerlink" title="第二章：市场营销的研发"></a>第二章：市场营销的研发</h1><h2 id="语录-1"><a href="#语录-1" class="headerlink" title="语录"></a>语录</h2><ul><li>找出世界上最成功的公司，研究它们的市场营销方法，调整修改，为你所用</li><li>市场营销中，小办法可以解决大问题，但是你必须得做个有心人</li><li>你的产品离开工厂后情况如何呢？得去做研发</li><li>公司至少要有几个真正得市场营销经理</li><li>寻找该问得问题</li><li>不要让市场营销研发成为管理者事业发展得绊脚石</li><li>把这一章得内容印发给员工，人手一册</li><li>愿意去观察、尝试、接受新方法的人是很少的</li></ul><h1 id="第三章：作为一种经营理念的市场营销"><a href="#第三章：作为一种经营理念的市场营销" class="headerlink" title="第三章：作为一种经营理念的市场营销"></a>第三章：作为一种经营理念的市场营销</h1><h2 id="语录-2"><a href="#语录-2" class="headerlink" title="语录"></a>语录</h2><ul><li>一家公司的价值取决于它的客户</li><li>如果你和你的客户生活在两个完全不同的世界，那么你很难见其所见</li><li>爱君者唯君耳</li><li>感同身受，体会客户的痛苦</li><li>看清现实</li><li>找你的前客户谈谈</li><li>从客户中招贤纳士</li><li>最了解你的客户的专家是客户自己</li><li>做做你自己的客户</li><li>也做做你竞争对手的客户</li><li>双赢是很好，但到底你赢多少，我赢多少呢？</li><li>我的市场营销做得很好，客户满意度就越不重要</li></ul><h1 id="第四章：做营销需要营销知识吗"><a href="#第四章：做营销需要营销知识吗" class="headerlink" title="第四章：做营销需要营销知识吗"></a>第四章：做营销需要营销知识吗</h1><h2 id="语录-3"><a href="#语录-3" class="headerlink" title="语录"></a>语录</h2><ul><li>市场营销中有很多正确答案，也有很多错误答案</li><li>确定一下你准备聘任的那位才华横溢的新市场营销经理是否懂市场营销</li><li>==告诉潜在客户如果他不买会失去什么==</li><li>“讨厌失去”的倾向强于“讨厌风险”的倾向</li><li>分散客户的注意力，不让他用脑，让他跟着感觉走</li><li>悲伤也有助于降低客户的理性决策能力</li><li>两性大战是鬼话</li><li>让美女去卖威士忌；让你的叔叔去卖保险</li><li>小恩小惠引出大恩大惠</li><li>发现、测试、完善。一个优秀的市场营销经理应是一个冷静实干、不断试验的科学家</li><li>市场营销成功的秘诀就是学以致用</li><li>在市场营销中，“正确”的意思是“足够正确”</li><li>市场营销就像赌博，赢面有利于你的时候，赌博并不危险</li></ul><h1 id="第五章：市场营销者的职责"><a href="#第五章：市场营销者的职责" class="headerlink" title="第五章：市场营销者的职责"></a>第五章：市场营销者的职责</h1><h2 id="语录-4"><a href="#语录-4" class="headerlink" title="语录"></a>语录</h2><ul><li>根据一个公司的客户判断这个公司的情况</li><li>常识告诉我们供不应求时市场营销的必要性下降==（恰恰相反——此时是最需要市场营销的。供不应求正好给我们提供一个绝好的机会提升我们客户组合的质量和价值。客户是我们最重要的资产）==</li><li>常识错误：供不应求时市场营销回报最高==（在供不应求时，市场营销投入越多，回报越多）==</li><li>易行之途则智者先至</li><li>当你的产品不能够给你带来优势时，你的客户能给你带来优势</li><li>向人力资源学习：考评客户，排序分类</li></ul><h1 id="第六章：管理你的市场"><a href="#第六章：管理你的市场" class="headerlink" title="第六章：管理你的市场"></a>第六章：管理你的市场</h1><h2 id="语录-5"><a href="#语录-5" class="headerlink" title="语录"></a>语录</h2><ul><li>如果你不了解竞争对手的客户，那么你就不了解你自己的客户</li><li>了解市场，了解自己，检验假设</li><li>要确保公司的CEO和市场营销经理们知道谁是你们的客户</li><li>不要只是阅读有关客户的资料，直接去寻访客户，眼见为实</li><li>检验你的假设，测量你的市场</li><li>当你觉得数据不可置信时，也许它们在讲述着一个你所不知道的故事</li><li>积极主动地管理影响你在行业细分市场上获利能力的五力<ul><li>进入壁垒：竞争对手进入你的市场难度如何？——不要进入别人也能进入的市场（==现在腾讯阿里哪个市场不能进入？==）</li><li>现有竞争：在有些行业，龙头老大之间相敬如宾——别找打，特别时在个儿比你大的人面前</li><li>替代品：竞争对手能伤你，而替代品能毁了你——技术壁垒</li><li>买方力量：如果买方有机会将你的钱弄到他自己的腰包，他是不会错过这个机会的</li><li>卖方力量：你若不能控制供应商的议价能力，你就赚不了钱——如果供应商有机会将你的钱弄到他自己的腰包，他也不会错过这个机会的</li></ul></li><li>寻找哀鸿遍野的战场——寻找最糟糕的行业进入</li><li>竞争并不发生在行业层面，而是发生在细分市场层面——==进入某个行业先问以下问题：==<ul><li>你进入这个行业后准备怎么做？</li><li>你会创建哪些新的细分市场？</li><li>你的蓝海在哪里？</li></ul></li><li>不求更好，但求不同，自立门户，自创山头</li><li>另外三大标准<ul><li>规模合适：一个细分市场，规模必须足够大，以纳你雄心，但也不能太大——==要么第一、要么第二，不然修、关、卖==</li><li>增长能力和获利能力</li><li>易得性：与其先确定目标人群得年龄再找寻以这个年龄为受众的媒体做广告，不如去找寻目标客户，直接问他们看到什么媒体</li></ul></li><li>如果不再有人能够指出你的错误，说明你该退休了</li></ul><h1 id="第七章：市场细分新思想"><a href="#第七章：市场细分新思想" class="headerlink" title="第七章：市场细分新思想"></a>第七章：市场细分新思想</h1><h2 id="语录-6"><a href="#语录-6" class="headerlink" title="语录"></a>语录</h2><ul><li>你需要谁成为你的客户？</li><li>更好的产品需要具备能够吸引更好的客户的属性</li><li>客户与品牌关系如下：<ul><li>忠诚的客户</li><li>于我有利的品牌转换者</li><li>于我不利的品牌转换者</li><li>竞争对手品牌的忠诚客户</li><li>不用此类产品的人</li></ul></li><li>如果你挪动一个客户，那么从他站的地方开始</li><li>重点突出，做强做大</li><li>宁为鸡首，别做凤尾</li><li>新产品的研发是好事，但是新市场的研发更好</li><li>失去好客户是坏事，扔掉坏客户是好事</li><li>战争是地狱，能躲就躲</li></ul><h1 id="第八章：失去你的客户"><a href="#第八章：失去你的客户" class="headerlink" title="第八章：失去你的客户"></a>第八章：失去你的客户</h1><h2 id="语录-7"><a href="#语录-7" class="headerlink" title="语录"></a>语录</h2><ul><li>找出你的前客户是谁，找到他们的下落</li><li>客户流失率降低一半，公司价值翻一番</li><li>找出谁是你最好的客户，确保他们有爱你的理由</li><li>向MBNA学习——MBNA（美信银行）1982年将降低客户流失率作为首要任务来抓，努力赢得前客户</li><li>提醒客户你的存在</li><li>不要让你的忠诚计划取代忠诚本身</li><li>设计高粘度的客户关系</li></ul><h1 id="第九章：你我都不知道什么是好广告"><a href="#第九章：你我都不知道什么是好广告" class="headerlink" title="第九章：你我都不知道什么是好广告"></a>第九章：你我都不知道什么是好广告</h1><h2 id="语录-8"><a href="#语录-8" class="headerlink" title="语录"></a>语录</h2><ul><li>告诉客户你是做什么的</li><li>和客户谈谈</li><li>不要自言自语</li><li>公司和它的广告公司是最不能评判其广告好坏的，它们知道得太多了</li><li>评判广告的最佳人选是你的潜在客户</li><li>试验好比买便宜的保险，不犯错误，就不必吸取教训了</li></ul><h1 id="第十章：好广告的要素"><a href="#第十章：好广告的要素" class="headerlink" title="第十章：好广告的要素"></a>第十章：好广告的要素</h1><ul><li>不同的销售情形适用不同的广告</li><li>品牌知名度是第一大挑战</li><li>品牌知名度有两种：记得和认得</li><li>认得：<ul><li>确保品牌包装和名称在广告中得到充分展示</li><li>必须阐述此类产品的功能用途</li><li>第一次宣传到做得声势浩大</li></ul></li><li>记得：<ul><li>广告正文主题句必须包括产品用途、品牌名词和第二人称呼语</li><li>不断重复主题句</li><li>保持较高的出镜率</li></ul></li><li>品牌态度是第二大挑战——品牌态度是指客户对其所知产品的好恶态度<br><img src="/2020/04/14/ji-suan-guang-gao-zhi-xi-jie-ying-xiao/4_imp.jpg" alt="4_imp"></li><li>品牌态度可以分为以下四类以及对应广告规则：<ul><li>重要 &amp; 消极动机：<ul><li>在产品生命周期的早期，正确描绘与购买动机相吻合的情绪</li><li>得让目标客户接受广告得主要意思</li><li>目标受众对品牌的已有态度必须是首要考虑的问题</li></ul></li><li>重要 &amp; 积极动机： <ul><li>情感的真实性是最为重要的，广告必须符合目标受众所在的群体的情绪状态和生活方式</li><li>必须让人们把自己和广告中的产品在一起，产生归属感</li><li>当受众缺乏有关产品的事实信息时，就应该给他们提供信息</li><li>要夸张渲染，言过其实，不要谦虚</li></ul></li><li>不重要 &amp; 消极动机<ul><li>使用“问题与解决办法”的简单哦公式，保持广告单刀直入</li><li>未必要让客户喜欢你的广告</li><li>关于产品的好处裨益，要竭尽渲染之能事</li></ul></li><li>不重要 &amp; 积极动机<ul><li>情感的共鸣很重要</li><li>品牌所产生的联想，情感必须是独特的——万宝路就是个很好例子</li><li>目标受众必须喜欢这个广告</li><li>通过联想塑造品牌形象</li></ul></li></ul></li><li>了解你面临的销售情形，选择相应的广告规则，因势利导</li><li>要打破常规，首先必须掌握常规——马克吐温</li><li>如果你的广告代理公司只会故弄玄虚，设计一个广告的目的是激发受众的好奇心，那就解雇他</li></ul><h1 id="第十一章：应该在哪里做广告"><a href="#第十一章：应该在哪里做广告" class="headerlink" title="第十一章：应该在哪里做广告"></a>第十一章：应该在哪里做广告</h1><h2 id="语录-9"><a href="#语录-9" class="headerlink" title="语录"></a>语录</h2><ul><li>在哪里做广告？——问问你的（潜在）客户</li><li>不要向同一个客户重复做广告</li><li>如果你的广告集中在单个媒体渠道，那么重叠和浪费产生的概率更大</li><li>分清主次：媒体-信息-执行</li></ul><h1 id="第十二章：你的广告公司不是你的合作伙伴"><a href="#第十二章：你的广告公司不是你的合作伙伴" class="headerlink" title="第十二章：你的广告公司不是你的合作伙伴"></a>第十二章：你的广告公司不是你的合作伙伴</h1><h2 id="语录-10"><a href="#语录-10" class="headerlink" title="语录"></a>语录</h2><ul><li>如果你相信你的广告公司，那么你将一无所获</li><li>广告公司所谓的免费调研是“羊毛出在羊身上”，不值得去占这个便宜</li><li>如果广告公司是你的战略宣传伙伴，那么汽车销售员就是你的战略交通伙伴——这岂不荒唐</li><li>不要只用一家广告公司</li><li>举办广告大赛，获得更好得创意</li><li>比照测试多个创意方案</li></ul><h1 id="第十三章：不要问你的品牌能为你做些什么"><a href="#第十三章：不要问你的品牌能为你做些什么" class="headerlink" title="第十三章：不要问你的品牌能为你做些什么"></a>第十三章：不要问你的品牌能为你做些什么</h1><h2 id="语录-11"><a href="#语录-11" class="headerlink" title="语录"></a>语录</h2><ul><li>品牌使得决策更容易</li><li>品牌可以降低风险</li><li>品牌可以让我获得他人得尊重</li><li>品牌帮助客户创造更理想得现实世界</li><li>你的产品和服务让你有钱，让你的品牌出名</li><li>品牌不是丰厚利润的唯一来源</li><li>当品牌对你很重要时，品牌价值不高的代价十分高昂</li><li>当品牌对你很重要时，你必须身体力行你的品牌</li><li>品牌说：“我能做的你都能做，但你不是我”——当其他公司仿照你的品牌时，你的品牌能反过来保护你</li><li>公司战略和品牌战略不谋而合——公司战略和品牌战略应该是统一的</li><li>小心读市场营销的工程师——技术性市场营销</li></ul><h1 id="第十四章：营销数学"><a href="#第十四章：营销数学" class="headerlink" title="第十四章：营销数学"></a>第十四章：营销数学</h1><h2 id="语录-12"><a href="#语录-12" class="headerlink" title="语录"></a>语录</h2><ul><li>在市场营销中没有比提高效率更容易的事情了</li><li>市场营销杠杆底线是营销决策要考虑的关键比例，是营销投资决策的试金石</li><li>降价是最昂贵的市场营销策略</li><li>保持低成本是生产成本的职责</li><li>保持高价格是营销部门的职责</li><li>利润高于市场份额，也会带来市场份额</li><li>追求短期利润无可厚非，有了短期才有长期</li><li>你的销售代表干活卖力不卖力，会有什么区别？——如果卖力干活加薪拿奖金不划算的话，他就不会卖力</li></ul><h1 id="第十五章：定价策略"><a href="#第十五章：定价策略" class="headerlink" title="第十五章：定价策略"></a>第十五章：定价策略</h1><h2 id="语录-13"><a href="#语录-13" class="headerlink" title="语录"></a>语录</h2><ul><li>打折时将绝对金额</li><li>收费时讲百分比</li><li>开发价值，宣传价值</li><li>要谈价值，而不是价格</li><li>价格永远不是第一个P</li><li>低价格信号对买方管用，所以对卖方也管用</li><li>价格实际是低的，但是看着不低，这招可能对你管用</li><li>价格尾数是9，很好，5也不错，在亚洲，8也很好</li><li>不要自作聪明——以为自己可以占便宜</li><li>价格要因人而异——价格歧视</li><li>运用创造力、新发明、研发来寻找价格歧视新方法</li><li>定价很重要</li></ul><h1 id="第十六章：分销"><a href="#第十六章：分销" class="headerlink" title="第十六章：分销"></a>第十六章：分销</h1><h2 id="语录-14"><a href="#语录-14" class="headerlink" title="语录"></a>语录</h2><ul><li>当众多公司合作共事时，高效率并非是自然而然地结果</li><li>个性理性地决策未必能给集体带来好结果</li><li>成为你的渠道系统地“黑手党教父”</li><li>把钱给经销商并能证明你的支配力</li></ul><h1 id="第十七章：支配力"><a href="#第十七章：支配力" class="headerlink" title="第十七章：支配力"></a>第十七章：支配力</h1><h2 id="语录-15"><a href="#语录-15" class="headerlink" title="语录"></a>语录</h2><ul><li>==工作和收入是负相关关系==</li><li>公司应该提高对权力管理的重视程度</li><li>效率提高对奴隶而言没有任何好处</li><li>积极管理关系，尽量增强渠道伙伴对你的依赖性</li><li>应自己去做那些能增强渠道支配力的分销工作</li><li>任何一个P都可以带来任何一个C：促销可以增强渠道支配力</li></ul><h1 id="第十八章：营销新概念——诚信"><a href="#第十八章：营销新概念——诚信" class="headerlink" title="第十八章：营销新概念——诚信"></a>第十八章：营销新概念——诚信</h1><h2 id="语录-16"><a href="#语录-16" class="headerlink" title="语录"></a>语录</h2><ul><li>诚实是公司最宝贵的资产</li><li>你的客户、投资者、员工认为你的公司有多诚实，就有多卖命</li><li>不诚实是滑坡式的灾难</li><li>像折磨客户那样折磨自己，感同身受</li><li>偷钱是不对的，偷时间也是不对的</li><li>偷你客户的时间和金钱是很危险的生意</li><li>检验客户服务的简单标准：你会这样对待你的母亲吗？</li><li>诚实——一个全新的概念</li></ul><h1 id="第十九章：外面的世界"><a href="#第十九章：外面的世界" class="headerlink" title="第十九章：外面的世界"></a>第十九章：外面的世界</h1><h2 id="语录-17"><a href="#语录-17" class="headerlink" title="语录"></a>语录</h2><ul><li>组织架构图应该显示所有参与营销你的产品的人和公司</li><li>任何一个对于你产品的成功有着直接或间接影响的人都应该是你宝贵的客户</li><li>你帮助了渠道也就是帮助了自己</li><li>小事做好了，大事自然水到渠成</li><li>管理你的商店</li><li>如果你偷朋友的钱，那么你将没朋友</li><li>在价值链各个环节提供增值，首先关心你客户的商业模式</li><li>解决了客户的问题也就解决了自己的问题</li><li>复杂的公式帮不了你，如果输入是错误参数的话</li><li>帮助客户达到利润最大化，你的利润自然也就最大化了</li><li>不要忙于管理，而忘了去寻找并留住客户</li><li>市场营销的成败取决于公司外部，而不是内部</li><li>不要为了节省小钱，搞得客户民怨沸腾</li></ul><h1 id="第二十章：最新的思考"><a href="#第二十章：最新的思考" class="headerlink" title="第二十章：最新的思考"></a>第二十章：最新的思考</h1><h2 id="语录-18"><a href="#语录-18" class="headerlink" title="语录"></a>语录</h2><ul><li>你考虑过你的营销对象吗</li><li>改变你的收费标准</li><li>经济低迷时的市场营销</li><li>做到兼容并蓄</li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算广告 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读书笔记之富爸爸穷爸爸</title>
      <link href="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/"/>
      <url>/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/image-20210621161440823.png" alt="image-20210621161440823"></p><h1 id="第一章：富人不为钱工作"><a href="#第一章：富人不为钱工作" class="headerlink" title="第一章：富人不为钱工作"></a>第一章：富人不为钱工作</h1><h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻"><a href="#左脑时刻" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>尽管有一份高收入的工作，像穷爸爸一样的人仍只能勉强维持着收支平衡</li></ul><h3 id="右脑时刻"><a href="#右脑时刻" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>用一个新颖的、具有创造性的方法看待废弃的连环画可以带来商机</li></ul><h3 id="潜意识时刻"><a href="#潜意识时刻" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>人们让恐惧和贪婪控制了自己的人生</li></ul><h2 id="语录"><a href="#语录" class="headerlink" title="语录"></a>语录</h2><ul><li>穷人和中产阶级为钱而工作，富人让钱为他工作</li><li>生活推着我们所有的人，有些人放弃了，有些人在抗争，少数人学会了这门课程，取得了进步，他们欢迎生活推动着他们</li><li>说到钱，大多数人都希望稳稳妥妥的赚钱，这样他们感到安全。关于钱，他们没有激情，有的只是恐惧</li><li>正是出于恐惧的心理，人们才想找一份安稳的工作。这些恐惧有：害怕付不起账单，害怕被解雇，害怕没有足够的钱，害怕重新开始。为了寻求保障，他们学习某种专业，或是做生意，拼命为钱工作。大多数人成了钱的奴隶，然后把怒气发泄到他们老板身上</li><li>大多数人并不知道是他们的感情代替了他们进行思考</li><li>就像一幅画表现的：驴子拉车，因为主人在它面前挂了个胡萝卜。主人清楚自己想要去哪里，而驴子却只是在追逐一个幻影。但第二天驴子依旧会去拉车，因为又有胡萝卜放在它的面前==我们大多数人就是那只驴子==</li></ul><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><ul><li>从小的教育就是让我们认真学习，通过高考进入好的大学，然后学习一门热门的专业，再进入社会为钱工作，然后工作、赚钱、支付账单、再工作。工资越高、花费越高，慢慢的就变成了那只驴子</li><li>恐惧是我们害怕重新开始，害怕去冒险。家庭责任、支付不完的账单、没钱让我们越来越畏畏缩缩。但是我们又贪婪，渴望有钱，所以很多人买彩票，渴望一夜暴富，却很少去学习理财知识，投资自己</li></ul><h1 id="第二章：为什么要教授财务知识"><a href="#第二章：为什么要教授财务知识" class="headerlink" title="第二章：为什么要教授财务知识"></a>第二章：为什么要教授财务知识</h1><h2 id="本章小结-1"><a href="#本章小结-1" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻-1"><a href="#左脑时刻-1" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>观察这些数字并学习他们所讲的故事</li><li>==资产把钱放入你的口袋==</li><li>如果有东西把钱从你的口袋里拿出来，它就是负债</li></ul><h3 id="右脑时刻-1"><a href="#右脑时刻-1" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>资产负债表清楚的解释了钱在不同人的生活中的活动规律</li></ul><h3 id="潜意识时刻-1"><a href="#潜意识时刻-1" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>被排斥的恐惧导致人们顺从，从不提出质疑，普遍接受别人的观点和流行趋势，最后却使自己的财物受到损害</li></ul><h3 id="几张图"><a href="#几张图" class="headerlink" title="几张图"></a>几张图</h3><p><img src="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/zichan.jpg" alt="资产现金流模式图"></p><p><img src="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/fuzhai.jpg" alt="负债现金流模式图"></p><p><img src="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/qiongzhongfu.jpg" alt="穷人中产富人图"></p><h2 id="语录-1"><a href="#语录-1" class="headerlink" title="语录"></a>语录</h2><ul><li>重要的不是你挣了多少钱，而是你能留下多少钱</li><li>只有知识才能解决问题并创造财富，那些不是靠财务知识挣来的钱也不会长久</li><li>富人获得资产，而穷人和中产阶级获得负债，只不过他们以为那些负债是资产</li></ul><h2 id="反思-1"><a href="#反思-1" class="headerlink" title="反思"></a>反思</h2><ul><li>我的财务教育可以说几乎没有，因为以前没钱，所以也就没想着去学习这些知识，现在看来是多么的愚蠢</li><li>作者对资产和负债的定义相当简单透彻，我们大多数人的一生是工作、买房、买车、娶妻、生子，但是贷款买的房车让自己背上了30年的负债，况且小孩的花费，只会让自己压力越来越大，越来越穷</li><li>现在的我就是穷人的现金流模式，争取往富人方向发展</li></ul><h1 id="第三章：关注自己的事业"><a href="#第三章：关注自己的事业" class="headerlink" title="第三章：关注自己的事业"></a>第三章：关注自己的事业</h1><h2 id="本章小结-2"><a href="#本章小结-2" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻-2"><a href="#左脑时刻-2" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>当资产产生的收入能够支付奢侈品时，才是你可以去购买它们的时候</li></ul><h3 id="右脑时刻-2"><a href="#右脑时刻-2" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>创造性的去想一想什么才是你的事业，而不是你的职业</li></ul><h3 id="潜意识时刻-2"><a href="#潜意识时刻-2" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>获得你所喜欢的资产，这样你才能很好的打理他们，享受其中的学习过程</li></ul><h3 id="资产分类"><a href="#资产分类" class="headerlink" title="资产分类"></a>资产分类</h3><ul><li>不需要我到场就可以正常运作的业务。我拥有它们，但是由别人经营和管理。如果==我必须在那工作，那它就不是我的事业，而是我的职业==</li><li>股票</li><li>债券</li><li>能够产生收入的房地产</li><li>票据（借据）</li><li>版税，如音乐、手稿、专利</li><li>其他任何有价值、可产生收入或有增值潜力并且有很好销路的东西</li></ul><h2 id="语录-2"><a href="#语录-2" class="headerlink" title="语录"></a>语录</h2><ul><li>为了财务安全，人们需要关注自己的事业，而不是职业</li><li>富人关心的焦点是资产而其他人关心的是收入</li><li>存在财务问题的人经常耗费一生为别人工作，其中许多人在他们不能工作时就变得一无所有</li><li>一旦把1美元投入了资产项，就不要让它出来。你应该这么想：这1美元进了你的资产项，它就成了你的雇员。关于钱，最妙的就是让它可以一天24小时不间断工作，还能为你的子孙后代服务</li><li>一个重要的区别就是：富人最后才买奢侈品，穷人和中产阶级会先买奢侈品</li></ul><h2 id="反思-2"><a href="#反思-2" class="headerlink" title="反思"></a>反思</h2><ul><li>我的职业是数据挖掘工程师，我的事业暂时没有，得找一份事业了</li><li>目前为止拥有的净资产也就是我脑子里面的知识了</li><li>大多数时候我的现金流并不适合购买奢侈品，但是我还是去购买了，并且还经常去旅游等</li></ul><h1 id="第四章：税收的历史和公司的力量"><a href="#第四章：税收的历史和公司的力量" class="headerlink" title="第四章：税收的历史和公司的力量"></a>第四章：税收的历史和公司的力量</h1><h2 id="本章小结-3"><a href="#本章小结-3" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻-3"><a href="#左脑时刻-3" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>会计就是财务知识或者解读数字的能力。如果你想建立一个商业帝国，这项能力至关重要。你管理的钱越多，就越要精确，否则你的大厦就会倒塌</li></ul><h3 id="右脑时刻-3"><a href="#右脑时刻-3" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>投资就是“钱生钱”的科学。这包含策略和方案，这要使用属于创造的右脑来做</li></ul><h3 id="潜意识时刻-3"><a href="#潜意识时刻-3" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>了解市场是供给与需求的科学。在市场基本面或者说一项投资的经济意义基础上，你需要了解受感情驱动的市场的“技术面”</li></ul><h3 id="拥有公司的好处"><a href="#拥有公司的好处" class="headerlink" title="拥有公司的好处"></a>拥有公司的好处</h3><ul><li>拥有公司的富人是：挣钱、支出、缴税</li><li>为公司工作的人是：挣钱、缴税、支出</li><li>两者的区别就是税赋的压力让为公司工作的人损失很多</li></ul><h3 id="财商的组成"><a href="#财商的组成" class="headerlink" title="财商的组成"></a>财商的组成</h3><ul><li>会计（财务知识）</li><li>投资（钱生钱的科学和策略）</li><li>了解市场（供给与需求的科学以及市场条件）</li><li>法律（减税优惠和在诉讼中获得保护）</li></ul><h2 id="语录-3"><a href="#语录-3" class="headerlink" title="语录"></a>语录</h2><ul><li>事实上富人并未被征税，是中产阶级尤其是受过良好教育的高收入的中产阶级在为穷人支付税金</li><li>如果你为钱工作，你就把力量给了你的老板，如果让钱为你工作，你就能控制这种力量</li><li>了解减税优惠政策和公司法的人会比雇员和小业主更快致富</li></ul><h2 id="反思-3"><a href="#反思-3" class="headerlink" title="反思"></a>反思</h2><ul><li>中国的税收政策可能跟书中所说不一样，现在对个人也有很多减税优惠</li></ul><h1 id="第五章：富人的投资"><a href="#第五章：富人的投资" class="headerlink" title="第五章：富人的投资"></a>第五章：富人的投资</h1><h2 id="本章小结-4"><a href="#本章小结-4" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻-4"><a href="#左脑时刻-4" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>一小笔钱可以通过精明的、好的投资变成一大笔钱</li></ul><h3 id="右脑时刻-4"><a href="#右脑时刻-4" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>作者常常鼓励他的成人学生要从《富爸爸现金流》游戏中发现哪些情况是他们所知道的，哪些是他们还需要学习的。最重要的是，这个游戏能反映一个人的行为方式，它是一个实时的反馈系统。它不需要老师们不停的交接，它就像是一场个人间的对话，完全按照你的习惯定制</li></ul><h3 id="潜意识时刻-4"><a href="#潜意识时刻-4" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>我们都拥有巨大的潜能——这是上天赏赐的礼物。我们都或多或少地存在着某种自我怀疑的心理，从而阻碍前行的步伐。==这种障碍很少是缺乏某种技术性的东西，更多是缺乏自信==</li></ul><h3 id="投资者类型"><a href="#投资者类型" class="headerlink" title="投资者类型"></a>投资者类型</h3><ul><li>进行一揽子投资的人（比如财务规划师）</li><li>自己创造投资机会的人（职业投资者），要想成为这种类型的人，必须学会以下三种技能<ul><li>如何找到别人错失的机会</li><li>如何筹措资金</li><li>如何把精明的人组织起来并雇佣他们为你工作。==风险无处不在，要学会驾驭风险，别总想回避风险==</li></ul></li></ul><h2 id="语录-4"><a href="#语录-4" class="headerlink" title="语录"></a>语录</h2><ul><li>在现实生活中，人们往往是依靠勇气而不是智慧去取得领先的位置的</li><li>陈旧的思想是那些在贫困线上苦苦挣扎的人背负的最大债务。原因很简单：他们没有意识到已有的某种思想或方法在昨天还是一种资产，今天却已经变成了负债</li><li>我们唯一的，也是最重要的资产是我们的头脑。如果使其得到良好的训练，它转瞬间就能创造大量的财富。而未经训练的头脑有可能创造无数的贫穷来拖垮一个家庭，甚至几代人</li></ul><h2 id="反思-4"><a href="#反思-4" class="headerlink" title="反思"></a>反思</h2><ul><li>作者说得对，阻碍我们前进的并非技术性知识的缺乏，而是自我怀疑和自信的缺乏。我平时总是尽量避免风险，总是害怕血本无归，归根结底还是自己没摸清其套路</li><li>如果减少对失败的恐惧，这是一个问题，待努力</li></ul><h1 id="第六章：学会不为钱工作"><a href="#第六章：学会不为钱工作" class="headerlink" title="第六章：学会不为钱工作"></a>第六章：学会不为钱工作</h1><h2 id="本章小结-5"><a href="#本章小结-5" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻-5"><a href="#左脑时刻-5" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>辞去一份看似有前途的工作而去追求另一份工作，短期来看，你的薪水可能会减少；但从长远来看，你将从中获得巨大的收益</li></ul><h3 id="右脑时刻-5"><a href="#右脑时刻-5" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>==学习专业以外的其他技能将会使你受益==</li></ul><h3 id="潜意识时刻-5"><a href="#潜意识时刻-5" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>面对你需要学习的新技能，请务必克服内心的恐惧和懒惰。而且你必须强迫自己去学习，像去健身房一样，当你做到之后，你会感到很高兴</li></ul><h3 id="成功必需管理技能"><a href="#成功必需管理技能" class="headerlink" title="成功必需管理技能"></a>成功必需管理技能</h3><ul><li>现金流管理</li><li>系统管理</li><li>人员管理</li><li>销售技能</li><li>营销技能</li><li>沟通能力</li></ul><h2 id="语录-5"><a href="#语录-5" class="headerlink" title="语录"></a>语录</h2><ul><li>工作比破产强一点</li><li>年轻人在找工作时要看能从中学到什么，而不是只看能挣多少钱</li><li>付出金钱是那些非常富有的家庭保持富有的秘诀</li></ul><h2 id="反思-5"><a href="#反思-5" class="headerlink" title="反思"></a>反思</h2><ul><li>精通专业技能既是优势也是弱点，找工作的时候只看能挣多少钱，而没有去关注其中能学到的东西，得改</li><li>平时不愿意花钱去学习新的知识，不愿去看书、健身，自己的惰性时刻在作怪，还是没有下定决心的勇气</li></ul><h1 id="第八章：开始行动"><a href="#第八章：开始行动" class="headerlink" title="第八章：开始行动"></a>第八章：开始行动</h1><h2 id="本章小结-6"><a href="#本章小结-6" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="左脑时刻-6"><a href="#左脑时刻-6" class="headerlink" title="左脑时刻"></a>左脑时刻</h3><ul><li>用自律精神践行“首先支付自己”这一法则，以确保永远把“增加资产项”放在第一位</li></ul><h3 id="右脑时刻-6"><a href="#右脑时刻-6" class="headerlink" title="右脑时刻"></a>右脑时刻</h3><ul><li>让你的头脑对新的想法和新的做事方式保持开放，以便让新旧想法和方式交锋，擦出思想的火花</li></ul><h3 id="潜意识时刻-6"><a href="#潜意识时刻-6" class="headerlink" title="潜意识时刻"></a>潜意识时刻</h3><ul><li>挖掘“我想变得富有”的理由和目标，并不断增强这些深层次的精神动力。它们将帮助你扫除通往财务自由之路上的一切障碍</li></ul><h3 id="唤醒理财天赋的十个步骤"><a href="#唤醒理财天赋的十个步骤" class="headerlink" title="唤醒理财天赋的十个步骤"></a>唤醒理财天赋的十个步骤</h3><h4 id="精神的力量——需要一个超现实的理由"><a href="#精神的力量——需要一个超现实的理由" class="headerlink" title="精神的力量——需要一个超现实的理由"></a>精神的力量——需要一个超现实的理由</h4><ul><li>定一个强有力的理由和目标来扫除一切障碍</li><li>这些理由和目标是“想要”和“不想要”的结合体</li></ul><h4 id="选择的力量——每天做出自己的选择"><a href="#选择的力量——每天做出自己的选择" class="headerlink" title="选择的力量——每天做出自己的选择"></a>选择的力量——每天做出自己的选择</h4><ul><li>首先选择投资教育，因为我们的头脑是最强大的工具</li><li>多听，多学习。以长远的眼光来看待财富，不要有一夜暴富的念头</li><li>选择学习不同新知识，选择运动，选择投资领域等等</li></ul><h4 id="关系的力量——慎重地选择朋友"><a href="#关系的力量——慎重地选择朋友" class="headerlink" title="关系的力量——慎重地选择朋友"></a>关系的力量——慎重地选择朋友</h4><ul><li>不要听贫穷或者胆小的人的话，因为他们总会告诉你这件事为什么不行</li><li>尽量跟“内线人员”打交道，因为他们知道一些内幕消息</li></ul><h4 id="快速学习的力量——掌握一种模式，然后再学习一种新的模式"><a href="#快速学习的力量——掌握一种模式，然后再学习一种新的模式" class="headerlink" title="快速学习的力量——掌握一种模式，然后再学习一种新的模式"></a>快速学习的力量——掌握一种模式，然后再学习一种新的模式</h4><ul><li>你学到了什么，就会成为什么样的人</li><li>大多数人的赚钱模式<ul><li>上班</li><li>赚钱</li><li>支付账单</li><li>平衡支票薄</li><li>购买共同基金</li><li>然后再上班</li></ul></li><li>如果挣钱不够多，则需要探寻新的赚钱模式</li></ul><h4 id="自律的力量——首先支付自己"><a href="#自律的力量——首先支付自己" class="headerlink" title="自律的力量——首先支付自己"></a>自律的力量——首先支付自己</h4><ul><li>能够自律是将富人、穷人分开的首要因素</li><li>开创事业所必备的3种管理技能<ul><li>现金流管理</li><li>人事管理</li><li>时间管理</li></ul></li><li>何为首先支付自己，请看下图</li><li>首先支付自己的两个法则<ul><li>不要让自己背上数额过大的债务包袱。保持低支出。首先建立资产项，然后再用资产项购买大房子和豪车</li><li>当你资金短缺时，让压力去发挥作用，不要动用你的储蓄或投资。利用压力激发你的理财天赋，想出新的赚钱方法，然后支付你的账单。这样不但能让你赚到钱，还可以提高你的财商</li></ul></li></ul><p><img src="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/zhifuziji.jpg" alt="首先支付自己"></p><p><img src="/2020/04/07/du-shu-bi-ji-zhi-fu-ba-ba-qiong-ba-ba/zhifubieren.jpg" alt="首先支付别人"></p><h4 id="好建议的力量——给你的经纪人以优厚的报仇"><a href="#好建议的力量——给你的经纪人以优厚的报仇" class="headerlink" title="好建议的力量——给你的经纪人以优厚的报仇"></a>好建议的力量——给你的经纪人以优厚的报仇</h4><ul><li>信息是无价的，一位好的经纪人不仅为你提供信息，还能让你赚钱</li><li>所有的经纪人的能力并非都一样。与他们交流，了解他们拥有多少财产或投资。要找以为很关心你的利益的经纪人，并且慷慨的对待他</li></ul><h4 id="无私的力量——做一个“印第安给予者”"><a href="#无私的力量——做一个“印第安给予者”" class="headerlink" title="无私的力量——做一个“印第安给予者”"></a>无私的力量——做一个“印第安给予者”</h4><ul><li>让你的初始投资回本，并且要快速回本</li><li>“印第安给予者”：印第安人给白人一条毯子，白人以为是礼物，但是印第安人会要回毯子！</li></ul><h4 id="专注的力量——用资产购买奢侈品"><a href="#专注的力量——用资产购买奢侈品" class="headerlink" title="专注的力量——用资产购买奢侈品"></a>专注的力量——用资产购买奢侈品</h4><ul><li>增加资产以支付自己想要的东西</li></ul><h4 id="神话的力量——对英雄的崇拜"><a href="#神话的力量——对英雄的崇拜" class="headerlink" title="神话的力量——对英雄的崇拜"></a>神话的力量——对英雄的崇拜</h4><ul><li>促使孩子学习的最有效方法之一便是假装自己变成了自己最崇拜的英雄</li><li>通过偶像的模范作用，挖掘我们自身的无限潜能</li></ul><h4 id="给予的力量——先予后取"><a href="#给予的力量——先予后取" class="headerlink" title="给予的力量——先予后取"></a>给予的力量——先予后取</h4><ul><li>如果想获得，就要先给予</li><li>你给别人的越多，你学到的也越多、</li></ul><h2 id="语录-6"><a href="#语录-6" class="headerlink" title="语录"></a>语录</h2><ul><li>金矿到处都是，但是大部分人没有经过相应的培训，所以发现不了他们</li><li>没有强有力的理由和目标，任何事情都会变得非常困难</li><li>从理财角度来说，我们每挣到一美元，就得到一次选择自己称为富人、穷人的机会</li><li>轻松的道路会越走越艰难，而艰难的道路会越走越轻松</li></ul><h2 id="反思-6"><a href="#反思-6" class="headerlink" title="反思"></a>反思</h2><ul><li>为什么要变得富有，因为家庭的责任，父母的建康，孩子的教育，妹子的奢侈</li><li>目前的消费习惯是拿一部分钱买基金，拿一部分钱每个月出去吃一两顿，每年出去两三次旅游，这样的消费习惯应该是没法让自己变富的</li><li>自己的自律习惯还是得好好锻炼一下</li></ul><h1 id="第九章：要做的事情"><a href="#第九章：要做的事情" class="headerlink" title="第九章：要做的事情"></a>第九章：要做的事情</h1><h2 id="本章小结-7"><a href="#本章小结-7" class="headerlink" title="本章小结"></a>本章小结</h2><h3 id="行动指南清单"><a href="#行动指南清单" class="headerlink" title="行动指南清单"></a>行动指南清单</h3><h4 id="停下手头的活儿"><a href="#停下手头的活儿" class="headerlink" title="停下手头的活儿"></a>停下手头的活儿</h4><ul><li>休息一下，评估一下你的做法中哪些有效，哪些无效</li></ul><h4 id="寻找新的思想"><a href="#寻找新的思想" class="headerlink" title="寻找新的思想"></a>寻找新的思想</h4><ul><li>到书店搜寻能提供独特、与众不同这类主张的书</li></ul><h4 id="找一些做过你想做的事情的人"><a href="#找一些做过你想做的事情的人" class="headerlink" title="找一些做过你想做的事情的人"></a>找一些做过你想做的事情的人</h4><ul><li>请他们和你一起共进午餐，向他们请教一些诀窍和技巧</li></ul><h4 id="参加辅导班、阅读和研讨会"><a href="#参加辅导班、阅读和研讨会" class="headerlink" title="参加辅导班、阅读和研讨会"></a>参加辅导班、阅读和研讨会</h4><ul><li>从互联网上搜索新的、有趣的课程并参加</li></ul><h4 id="提出多份报价"><a href="#提出多份报价" class="headerlink" title="提出多份报价"></a>提出多份报价</h4><ul><li>大部分卖主的要价过高，很少有要价低于标的物的实际价值</li><li>提出多份报价，总会有人跟你达成购买意向，例如你提出一个低的报价，说不定对方就同意了呢</li><li>记得使用“免责条款”来做报价。例如：合同上加一条，如果我的合伙人不同意或者其他不可抗力因素，我有权取消合同（虽然你不一定有合伙人，或者合伙人也有可能是你的猫）</li></ul><h4 id="每月在某一地区慢跑、散步或驾车10分钟左右"><a href="#每月在某一地区慢跑、散步或驾车10分钟左右" class="headerlink" title="每月在某一地区慢跑、散步或驾车10分钟左右"></a>每月在某一地区慢跑、散步或驾车10分钟左右</h4><ul><li>在这个过程中发现可以投资的项目</li><li>一桩交易要盈利，得具备两个条件：<ul><li>一是廉价</li><li>二是有变化，价格变动等</li></ul></li></ul><h4 id="为什么消费者总是穷人？"><a href="#为什么消费者总是穷人？" class="headerlink" title="为什么消费者总是穷人？"></a>为什么消费者总是穷人？</h4><ul><li>利润在购买时就已确定，而不是在出售时获得的</li><li>当股市下降时，不要急于逃出</li></ul><h4 id="关注适当的地方"><a href="#关注适当的地方" class="headerlink" title="关注适当的地方"></a>关注适当的地方</h4><ul><li>学习、关注感兴趣的领域</li></ul><h4 id="首先寻找想买进的人，然后才去找想卖出的人"><a href="#首先寻找想买进的人，然后才去找想卖出的人" class="headerlink" title="首先寻找想买进的人，然后才去找想卖出的人"></a>首先寻找想买进的人，然后才去找想卖出的人</h4><ul><li>只盯着小生意的人是不会有很大突破的，得考虑买下一个大的馅饼，划分成几小块，出售给不同人</li></ul><h4 id="考虑大生意"><a href="#考虑大生意" class="headerlink" title="考虑大生意"></a>考虑大生意</h4><ul><li>多考虑大生意</li></ul><h4 id="学习前人经验"><a href="#学习前人经验" class="headerlink" title="学习前人经验"></a>学习前人经验</h4><ul><li>学习富人</li></ul><h4 id="行动起来"><a href="#行动起来" class="headerlink" title="行动起来"></a>行动起来</h4><ul><li>开始动起来</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 理财 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算广告之业务</title>
      <link href="/2020/03/20/ji-suan-guang-gao-zhi-ye-wu/"/>
      <url>/2020/03/20/ji-suan-guang-gao-zhi-ye-wu/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/03/20/ji-suan-guang-gao-zhi-ye-wu/image-20210621155031003.png" alt="image-20210621155031003"></p><h1 id="投放流程图"><a href="#投放流程图" class="headerlink" title="投放流程图"></a>投放流程图</h1><p><img src="/2020/03/20/ji-suan-guang-gao-zhi-ye-wu/toufang.jpg" alt="投放流程图"></p><h1 id="CTR预估"><a href="#CTR预估" class="headerlink" title="CTR预估"></a>CTR预估</h1><h2 id="FTRL"><a href="#FTRL" class="headerlink" title="FTRL"></a>FTRL</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/55135954" target="_blank" rel="noopener">LR+FTRL算法原理以及工程化实现</a></li></ul><h1 id="流量分配"><a href="#流量分配" class="headerlink" title="流量分配"></a>流量分配</h1><h2 id="什么是流量？"><a href="#什么是流量？" class="headerlink" title="什么是流量？"></a>什么是流量？</h2><ul><li>流量就是媒体拥有的用户数据，例如：抖音的用户数据</li></ul><h2 id="什么是流量分配？"><a href="#什么是流量分配？" class="headerlink" title="什么是流量分配？"></a>什么是流量分配？</h2><ul><li><p>DSP根据媒体给的流量给不同的广告主分配其需要的流量</p></li><li><p>结合转化函数与广告主导量要求来分配流量</p></li></ul><h1 id="出价"><a href="#出价" class="headerlink" title="出价"></a>出价</h1>]]></content>
      
      
      <categories>
          
          <category> 计算广告 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 业务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之Kafka</title>
      <link href="/2020/01/02/shi-yong-gong-ju-zhi-kafka/"/>
      <url>/2020/01/02/shi-yong-gong-ju-zhi-kafka/</url>
      
        <content type="html"><![CDATA[<p><img src="/2020/01/02/shi-yong-gong-ju-zhi-kafka/image-20210621145318002.png" alt="image-20210621145318002"></p><h1 id="Kafka基础"><a href="#Kafka基础" class="headerlink" title="Kafka基础"></a>Kafka基础</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="Kafka是什么？"><a href="#Kafka是什么？" class="headerlink" title="Kafka是什么？"></a>Kafka是什么？</h3><ul><li>Apache Kafka是一个分布式流平台</li><li>Kafka作为一个集群运行在一个或多个服务器上，这些服务器可以跨越多个数据中心</li><li>Kafka集群将记录流存储在称为主题<code>Topic</code>的类别中</li><li><strong>每个记录由一个键<code>key</code>、一个值<code>value</code>和一个时间戳<code>timestamp</code>组成</strong></li></ul><h3 id="流平台的关键特性"><a href="#流平台的关键特性" class="headerlink" title="流平台的关键特性"></a>流平台的关键特性</h3><ul><li>发布和订阅记录流，类似于消息队列或企业消息传递系统</li><li>以容错的持久方式存储记录流</li><li>处理记录流（当它们发生时）</li></ul><h3 id="Kafka的应用领域"><a href="#Kafka的应用领域" class="headerlink" title="Kafka的应用领域"></a>Kafka的应用领域</h3><ul><li>构建实时流数据管道，在系统或应用程序之间可靠地获取数据</li><li>构建转换或响应数据流的实时流应用程序</li></ul><h3 id="Kafka的四个核心API"><a href="#Kafka的四个核心API" class="headerlink" title="Kafka的四个核心API"></a>Kafka的四个核心API</h3><p><img src="/2020/01/02/shi-yong-gong-ju-zhi-kafka/kafka-apis.png" alt="Kafka-APIs"></p><ul><li><a href="https://kafka.apache.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a>：允许应用程序将记录流发布到一个或多个Kafka主题<code>Topic</code></li><li><a href="https://kafka.apache.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>：允许应用程序订阅一个或多个主题<code>Topic</code>，并处理产生给它们的记录流。</li><li><a href="https://kafka.apache.org/documentation.html#connectapi" target="_blank" rel="noopener">Streams API</a>：允许应用程序充当流处理器，使用一个或多个主题<code>Topic</code>的输入流，并生成一个或多个输出主题<code>Topic</code>的输出流，从而有效地将输入流转换为输出流</li><li><a href="https://kafka.apache.org/documentation.html#connectapi" target="_blank" rel="noopener">Connector API</a>：允许构建和运行将Kafka主题<code>Topic</code>连接到现有应用程序或数据库的可重用生产者或消费者。例如，到关系数据库的连接器可能捕获对表的每个更改。</li></ul><h3 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h3><ul><li>在Kafka中，客户端和服务器之间的通信是通过一个简单的、高性能的、语言无关的<code>TCP</code>协议来完成的。此协议经过版本控制，并保持与旧版本的向后兼容性</li></ul><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="主题-Topics"><a href="#主题-Topics" class="headerlink" title="主题[Topics]"></a>主题<code>[Topics]</code></h3><ul><li>主题是被发布记录所属类别或订阅源的名词</li><li>一个主题可以有零个，一个或多个消费者来订阅写入该主题的数据</li></ul><h3 id="日志-Logs"><a href="#日志-Logs" class="headerlink" title="日志[Logs]"></a>日志<code>[Logs]</code></h3><ul><li>对于每个主题，Kafka集群维护一个类似下图的分区日志</li></ul><p><img src="/2020/01/02/shi-yong-gong-ju-zhi-kafka/log_anatomy.png" alt="log_anatomy"></p><ul><li>每个分区都是有序的，不可变的记录序列，这些记录连续地附加到结构化的提交日志中</li><li>每个分区中的记录都分配有一个称为<strong>偏移</strong>的顺序ID号，该ID唯一地标识分区中的每个记录</li><li>Kafka集群使用可配置的保留期限持久保留所有已发布记录（无论是否已使用它们），超过保留期限后，记录将被丢弃以释放空间</li><li><p>Kafka的性能在数据大小方面是稳定的，所以<strong>长时间存储数据不是问题</strong></p></li><li><p>如下图，偏移量由使用者控制：通常，使用者在读取记录时会线性地推进其偏移量，但是实际上，由于位置是由使用者控制的，因此它可以按喜欢的任何顺序使用记录。 例如，使用者可以重置到较旧的偏移量以重新处理过去的数据，或者跳到最近的记录并从“现在”开始使用。</p></li></ul><p><img src="/2020/01/02/shi-yong-gong-ju-zhi-kafka/log_consumer.png" alt="log_consumer"></p><ul><li>这些特性的组合意味着Kafka消费者非常便宜，他们可以来去自如，对集群或其他消费者没有太大影响。例如，您可以使用命令行工具“跟踪”任何主题的内容，而不需要更改任何现有使用者所使用的内容</li><li>日志中的分区有多种用途。 首先，它们允许日志扩展到超出单个服务器所能容纳的大小。 每个单独的分区都必须适合托管它的服务器，但是一个主题可能有很多分区，因此它可以处理任意数量的数据。 其次，它们充当并行性的单元</li></ul><h3 id="分布-Distribution"><a href="#分布-Distribution" class="headerlink" title="分布[Distribution]"></a>分布<code>[Distribution]</code></h3><ul><li>日志的分区分布在Kafka群集中的服务器上，每个服务器处理数据并要求共享分区。 </li><li>每个分区都在可配置数量的服务器之间复制，以实现容错功能</li><li>每个分区有一个充当“领导者”的服务器和零个或多个充当“追随者”的服务器</li><li>领导者处理该分区的所有读写请求，而追随者则被动复制领导者</li><li>如果领导者失败，则追随者之一将自动成为新领导者</li><li>每个服务器既充当某些分区的领导者，又充当其他分区的追随者，这样集群中的负载得到了很好的平衡</li></ul><h3 id="异地备份-Geo-Replication"><a href="#异地备份-Geo-Replication" class="headerlink" title="异地备份[Geo-Replication]"></a>异地备份<code>[Geo-Replication]</code></h3><ul><li>Kafka MirrorMaker为集群提供异地备份支持</li><li>使用MirrorMaker，消息可以跨多个数据中心或云区域进行备份</li><li>您可以在主动/被动方案中使用它进行备份和恢复，或在主动/主动方案中将数据放置在离您的用户更近的位置，或支持本地数据需求</li></ul><h3 id="生产者-Producers"><a href="#生产者-Producers" class="headerlink" title="生产者[Producers]"></a>生产者<code>[Producers]</code></h3><ul><li>生产者将数据发布到他们选择的主题</li><li>生产者负责选择将哪个记录分配给主题中的哪个分区</li><li>这可以以循环的方式来完成，只是为了平衡负载，也可以根据某种语义划分函数来完成(比如基于记录中的某个键)</li></ul><h3 id="消费者-Consumers"><a href="#消费者-Consumers" class="headerlink" title="消费者[Consumers]"></a>消费者<code>[Consumers]</code></h3><ul><li>消费者使用消费者组名称标记自己，并且发布到主题的每条记录都会传递到每个订阅消费者组中的一个消费者实例</li><li>消费者实例可以在单独的进程中或在单独的机器上</li><li>如果所有消费者实例具有相同的消费者组，那么记录将有效地在消费者实例上进行负载平衡</li><li>如果所有消费者实例都有不同的消费者组，那么每个记录将被广播到所有消费者进程</li></ul><p><strong>举例如下图：</strong></p><p><img src="/2020/01/02/shi-yong-gong-ju-zhi-kafka/consumer-groups.png" alt="log_consumer"></p><ul><li>一个包含四个分区（P0-P3）和两个消费者组的两台服务器【Kafka集群】</li><li>消费者组A有两个消费者实例（C1、C2），而组B有四个（C3-C6）</li><li>Kafka中实现消费的方法是在消费者实例的日志中划分分区，这样每个实例在任何时候都是分区“公平共享”的唯一消费者</li><li>这个保持组成员身份的过程是由Kafka协议动态处理的</li><li>如果新的实例加入组，它们将从组的其他成员那里接管一些分区</li><li>如果一个实例死亡，它的分区将分配给其余的实例</li><li>如图中：消费者组A中C1负责P0\P3，C2负责P1\P2；消费者组B中每个实例负责一个分区</li><li>Kafka只提供分区内记录的总顺序，而不提供主题中不同分区之间的总顺序</li><li>对大多数应用程序来说，按分区排序和按键分区数据的能力已经足够</li><li>但是，如果需要记录的总顺序，则可以使用只有一个分区的主题来实现</li><li>尽管这意味着每个使用者组只有一个使用者进程（一个使用者实例？）</li></ul><h3 id="多租户-Multi-tenancy"><a href="#多租户-Multi-tenancy" class="headerlink" title="多租户[Multi-tenancy]"></a>多租户<code>[Multi-tenancy]</code></h3><ul><li>可以将Kafka部署为多租户解决方案</li><li>通过配置哪些主题可以生成或使用数据，可以启用多租户</li><li>还有对限额的操作支持</li><li>管理员可以对请求定义和强制配额，以控制客户端使用的代理资源</li><li>有关更多信息，请参阅<a href="https://kafka.apache.org/documentation/#security" target="_blank" rel="noopener">安全文档</a></li></ul><h3 id="保证性-Guarantees"><a href="#保证性-Guarantees" class="headerlink" title="保证性[Guarantees]"></a>保证性<code>[Guarantees]</code></h3><p>在较高级别上，Kafka提供以下保证：</p><ul><li>生产者发送到特定主题分区的消息将按照发送的顺序追加<ol><li>也就是说，如果记录M1是由与记录M2相同的生产者发送的，并且M1是先发送的，那么M1的偏移量将比M2低，并出现在日志的前面</li></ol></li><li>消费者实例按【记录在日志中存储的顺序】查看记录</li><li>对于具有复制因子N的主题，我们将容忍至多N-1个服务器故障，而不会丢失提交到日志的任何记录</li></ul><h1 id="Kafka进阶"><a href="#Kafka进阶" class="headerlink" title="Kafka进阶"></a>Kafka进阶</h1><h2 id="Kafka作为消息传递系统-Kafka-as-a-Messaging-System"><a href="#Kafka作为消息传递系统-Kafka-as-a-Messaging-System" class="headerlink" title="Kafka作为消息传递系统[Kafka as a Messaging System]"></a>Kafka作为消息传递系统<code>[Kafka as a Messaging System]</code></h2><h3 id="传统的企业消息传递系统"><a href="#传统的企业消息传递系统" class="headerlink" title="传统的企业消息传递系统"></a>传统的企业消息传递系统</h3><p>传统的消息传递系统拥有两个模块：</p><h4 id="队列-queuing"><a href="#队列-queuing" class="headerlink" title="队列-queuing"></a>队列-queuing</h4><ul><li>在队列中，消费者池可以从服务器读取数据，而每条记录都将被发送到其中的一位消费者</li><li>队列的优点是它允许在多个消费者实例上划分数据处理</li><li>不幸的是，队列不是多用户的：一旦一个进程读取了数据，其他用户就读取不了了</li><li>传统队列在服务器上按顺序保留记录，如果多个使用者从队列中消费，则服务器按存储记录的顺序分发记录</li><li>然而，尽管服务器按顺序分发记录，但这些记录是异步传递给消费者的，因此它们可能在不同的消费者上不按顺序到达</li><li>这实际上意味着记录的顺序在并行使用时丢失</li><li>消息传递系统通常通过“独占消费者”的概念来解决这个问题，该概念只允许一个进程从队列中消费，但这当然意味着处理中不存在并行性</li></ul><h4 id="发布-订阅-publish-subscribe"><a href="#发布-订阅-publish-subscribe" class="headerlink" title="发布-订阅-publish-subscribe"></a>发布-订阅-publish-subscribe</h4><ul><li>在发布-订阅中，记录被广播给所有消费者</li><li>发布-订阅允许将数据广播到多个进程，但是由于每个消息都传递到每个订阅者，因此无法扩展处理</li></ul><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><ul><li>Kafka中的消费者组概念概括了队列＆发布-订阅</li><li>与队列一样，消费者组允许将处理划分到一组进程(消费者组的成员)上</li><li>与发布-订阅一样，Kafka允许向多个消费者组广播消息</li><li>Kafka也比传统的消息传递系统有更强的订购保证</li><li>通过对主题内分区的并行性的概念，Kafka能够在用户进程池上提供<strong>排序保证</strong>和<strong>负载平衡</strong></li><li>这是通过将主题中的分区分配给消费者组中的消费者实例来实现的，这样每个分区正好由组中的一个消费者使用</li><li>通过这样做，我们可以确保消费者是该分区的唯一读取者，并按顺序使用数据</li><li>由于有许多分区，这仍然可以在许多消费者实例上平衡负载</li><li>但是请注意，在一个消费者组中<strong>不能有比分区更多</strong>的消费者实例</li></ul><h1 id="Kafka操作"><a href="#Kafka操作" class="headerlink" title="Kafka操作"></a>Kafka操作</h1><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="主题列表"><a href="#主题列表" class="headerlink" title="主题列表"></a>主题列表</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">bin/kafka-topics.sh --list --zookeeper localhost:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="消费topic"><a href="#消费topic" class="headerlink" title="消费topic"></a>消费topic</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic topic-name --from-beginning<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="查看topic分区和副本情况"><a href="#查看topic分区和副本情况" class="headerlink" title="查看topic分区和副本情况"></a>查看topic分区和副本情况</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181  --topic test0<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="查看topic消费到的offset"><a href="#查看topic消费到的offset" class="headerlink" title="查看topic消费到的offset"></a>查看topic消费到的offset</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 127.0.0.1:9092 --topic test0 --time -1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="查看topic各个分区的消息的信息"><a href="#查看topic各个分区的消息的信息" class="headerlink" title="查看topic各个分区的消息的信息"></a>查看topic各个分区的消息的信息</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group testgroup --topic test0 --zookeeper 127.0.0.1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><p>运行结果：</p><p>| GROUP    | TOPIC     | PID          | OFFSET           | LOGSIZE | LAG          |<br>| ———— | ————- | —————— | ———————— | ———- | —————— |<br>| 消费者组 | topic名字 | partition id | 当前已消费的条数 | 总条数  | 未消费的条数 |<br>|          |           |              |                  |         |              |</p></li></ul><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul><li><a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">官网英文文档</a></li><li><a href="https://kafka.apachecn.org/" target="_blank" rel="noopener">官网中文文档</a></li><li><a href="https://www.w3cschool.cn/apache_kafka/" target="_blank" rel="noopener">w3school</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之storm</title>
      <link href="/2019/12/17/shi-yong-gong-ju-zhi-storm/"/>
      <url>/2019/12/17/shi-yong-gong-ju-zhi-storm/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/12/17/shi-yong-gong-ju-zhi-storm/image-20210621144322474.png" alt="image-20210621144322474"></p><h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="Storm是什么？"><a href="#Storm是什么？" class="headerlink" title="Storm是什么？"></a>Storm是什么？</h3><ul><li>Storm是Apache下的一个<strong>分布式实时</strong>大数据处理系统</li></ul><h3 id="Storm优势"><a href="#Storm优势" class="headerlink" title="Storm优势"></a>Storm优势</h3><ul><li>实时流</li><li>快</li><li>数据处理保证</li><li>容错</li></ul><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><p><img src="/2019/12/17/shi-yong-gong-ju-zhi-storm/core_concept.jpg" alt="图：storm拓扑"></p><h4 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h4><ul><li>主要数据结构，有序元素的列表</li><li>默认情况支持所有数据类型</li><li>通常是一组逗号分隔的值</li></ul><h4 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h4><ul><li>Tuple的无序序列</li></ul><h4 id="Spouts"><a href="#Spouts" class="headerlink" title="Spouts"></a>Spouts</h4><ul><li>Stream的源头</li><li>通常，Storm从原始数据源（Kafka队列等）接受数据</li><li>也可以通过编写Spouts从数据源读取数据</li><li>“ISpout”是实现Spout的核心接口</li><li>一些特定的接口包括：IRichSpout，BaseRichSpout，KafkaSpout等</li></ul><h4 id="Bolts"><a href="#Bolts" class="headerlink" title="Bolts"></a>Bolts</h4><ul><li>逻辑处理单元</li><li>可以执行过滤、聚合等数据操作</li><li>“IBolt”是实现Bolt的核心接口</li><li>一些特定的接口包括：IRichBolt，IBasicBolt等</li></ul><h3 id="拓扑"><a href="#拓扑" class="headerlink" title="拓扑"></a>拓扑</h3><ul><li>图：storm拓扑就是一个拓扑</li><li>Spouts和Bolts连接就形成拓扑结构</li><li>拓扑是有向图，顶点是计算，边是数据流</li><li>Storm始终保持拓扑运行直到你终止且主要工作就是运行拓扑</li></ul><h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><ul><li>Storm执行的每个Spout和Bolt被称为任务</li></ul><h3 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h3><ul><li>一个拓扑中的顶点被分布到多个<strong>工作节点</strong>上分布式运行</li><li>Storm将所有工作节点上的任务均匀分布</li><li>工作节点的角色是监听作业，并在新作业到达时启动或停止进程</li></ul><h3 id="流分组"><a href="#流分组" class="headerlink" title="流分组"></a>流分组</h3><p>数据流从Spouts流到Bolts，或从一个Bolts流到另一个Bolts时，则需要分组控制，有以下四种分组方式</p><h4 id="随机分组"><a href="#随机分组" class="headerlink" title="随机分组"></a>随机分组</h4><ul><li>相等数量的Tuples随机分布在执行Bolts的所有worker（包含工作节点和工作节点进程）中</li></ul><h4 id="字段分组"><a href="#字段分组" class="headerlink" title="字段分组"></a>字段分组</h4><ul><li>Tuples中具有相同字段的分配给同一个worker</li></ul><h4 id="全局分组"><a href="#全局分组" class="headerlink" title="全局分组"></a>全局分组</h4><ul><li>所有Tuples分配给同一个worker</li></ul><h4 id="所有分组"><a href="#所有分组" class="headerlink" title="所有分组"></a>所有分组</h4><ul><li>将所有Tuples都建立副本，并分配给所有的worker</li></ul><h2 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h2><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="/2019/12/17/shi-yong-gong-ju-zhi-storm/storm_structure.jpg" alt="图：storm集群架构"></p><h3 id="组件-1"><a href="#组件-1" class="headerlink" title="组件"></a>组件</h3><h4 id="Nimbus（主节点）"><a href="#Nimbus（主节点）" class="headerlink" title="Nimbus（主节点）"></a>Nimbus（主节点）</h4><ul><li>集群的主节点</li><li>负责给工作节点分发任务和数据</li><li>监听各工作节点的故障</li></ul><h4 id="Supervisor（工作节点）"><a href="#Supervisor（工作节点）" class="headerlink" title="Supervisor（工作节点）"></a>Supervisor（工作节点）</h4><ul><li>除了主节点的其他节点</li><li>完成主节点分配任务的节点</li><li>至少有一个工作进程</li></ul><h4 id="Worker-process（工作进程）"><a href="#Worker-process（工作进程）" class="headerlink" title="Worker process（工作进程）"></a>Worker process（工作进程）</h4><ul><li>执行与特点拓扑相关的任务</li><li>不会自己运行任务</li><li>通过创建执行器Executor来执行特点的任务</li><li>拥有多个执行器Executor</li></ul><h4 id="Executor（执行器）"><a href="#Executor（执行器）" class="headerlink" title="Executor（执行器）"></a>Executor（执行器）</h4><ul><li>工作进程产生的单个线程</li><li>至少运行一个任务</li><li>仅用于特定的Spout或Bolt</li></ul><h4 id="Task（任务）"><a href="#Task（任务）" class="headerlink" title="Task（任务）"></a>Task（任务）</h4><ul><li>处理实际的数据</li><li>要么是Spout，要么是Bolt</li></ul><h4 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h4><ul><li>Nimbus是无状态的，所以需要ZooKeeper来监视工作节点的状态</li><li>通过ZooKeeper来监视状态，这样故障的网络就可以重新启动</li></ul><h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p><img src="/2019/12/17/shi-yong-gong-ju-zhi-storm/storm_job_process.jpg" alt="图：storm工作流程"></p><h3 id="详细流程介绍"><a href="#详细流程介绍" class="headerlink" title="详细流程介绍"></a>详细流程介绍</h3><ol><li>Nimbus等待客户端提交拓扑任务</li><li>Nimbus创建任务的本地目录并将任务上传至<code>$STORM_HOME/tmp/nimbus/inbox/</code></li><li>Nimbus从ZooKeeper监控工作节点的心跳并计算拓扑的工作量</li><li>Nimbus将任务分配信息写入ZooKeeper</li><li>工作节点监听ZooKeeper的信息，当有任务分配时，启动任务的拓扑</li><li>工作节点启动任务拓扑后，启动相应数目的worker进程</li><li>由worker进程来执行任务（Spout或Bolt）</li></ol><h2 id="分布式消息系统"><a href="#分布式消息系统" class="headerlink" title="分布式消息系统"></a>分布式消息系统</h2><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><ul><li>详情请见</li></ul><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul><li><a href="https://www.w3cschool.cn/apache_storm/" target="_blank" rel="noopener">Apache Storm教程</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之spark</title>
      <link href="/2019/12/12/shi-yong-gong-ju-zhi-spark/"/>
      <url>/2019/12/12/shi-yong-gong-ju-zhi-spark/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/12/12/shi-yong-gong-ju-zhi-spark/image-20210621144836787.png" alt="image-20210621144836787"></p><h1 id="Spark基础"><a href="#Spark基础" class="headerlink" title="Spark基础"></a>Spark基础</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul><li>Spark是UC Berkeley AMP lab所开源的类似Hadoop MapReduce的通用并行框架，它能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。</li></ul><h3 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h3><h4 id="运行速度快："><a href="#运行速度快：" class="headerlink" title="运行速度快："></a>运行速度快：</h4><ul><li>Spark使用先进的DAG（Directed Acyclic Graph，有向无环图）执行引擎，以支持循环数据流与内存计算，基于内存的执行速度可比Hadoop MapReduce快上百倍，基于磁盘的执行速度也能快十倍；</li></ul><h4 id="容易使用："><a href="#容易使用：" class="headerlink" title="容易使用："></a>容易使用：</h4><ul><li>Spark支持使用Scala、Java、Python和R语言进行编程，简洁的API设计有助于用户轻松构建并行程序，并且可以通过Spark Shell进行交互式编程；</li></ul><h4 id="通用性："><a href="#通用性：" class="headerlink" title="通用性："></a>通用性：</h4><ul><li>Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件，这些组件可以无缝整合在同一个应用中，足以应对复杂的计算；</li></ul><h4 id="运行模式多样："><a href="#运行模式多样：" class="headerlink" title="运行模式多样："></a>运行模式多样：</h4><ul><li>Spark可运行于独立的集群模式中，或者运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源。</li><li>Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比MapReduce更灵活；</li><li>Spark提供了内存计算，中间结果直接放到内存中，带来了更高的迭代运算效率；</li><li>Spark基于DAG的任务调度执行机制，要优于MapReduce的迭代执行机制。</li></ul><h2 id="生态系统"><a href="#生态系统" class="headerlink" title="生态系统"></a>生态系统</h2><h3 id="大数据处理类型"><a href="#大数据处理类型" class="headerlink" title="大数据处理类型"></a>大数据处理类型</h3><h4 id="复杂的批量数据处理："><a href="#复杂的批量数据处理：" class="headerlink" title="复杂的批量数据处理："></a>复杂的批量数据处理：</h4><ul><li>时间跨度通常在数十分钟到数小时之间；</li><li>Hadoop MapReduce</li></ul><h4 id="基于历史数据的交互式查询："><a href="#基于历史数据的交互式查询：" class="headerlink" title="基于历史数据的交互式查询："></a>基于历史数据的交互式查询：</h4><ul><li>时间跨度通常在数十秒到数分钟之间；</li><li>Impala：Impala与Hive相似，但底层引擎不同，提供了实时交互式</li><li>SQL查询</li></ul><h4 id="基于实时数据流的数据处理："><a href="#基于实时数据流的数据处理：" class="headerlink" title="基于实时数据流的数据处理："></a>基于实时数据流的数据处理：</h4><ul><li>时间跨度通常在数百毫秒到数秒之间</li><li>Storm：开源流计算框架</li></ul><h4 id="Spark所提供的生态系统足以应对上述三种场景"><a href="#Spark所提供的生态系统足以应对上述三种场景" class="headerlink" title="Spark所提供的生态系统足以应对上述三种场景"></a>Spark所提供的生态系统足以应对上述三种场景</h4><ul><li>即同时支持批处理、交互式查询和流数据处理。</li></ul><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><h4 id="访问和接口"><a href="#访问和接口" class="headerlink" title="访问和接口"></a>访问和接口</h4><h5 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h5><p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p><p>Spark Streaming支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流式计算分解成一系列短小的批处理作业。Spark Streaming支持多种数据输入源，如Kafka、Flume和TCP套接字等；</p><p>kafka：<a href="https://www.w3cschool.cn/apache_kafka/" target="_blank" rel="noopener">https://www.w3cschool.cn/apache_kafka/</a></p><p>Flume：<a href="https://juejin.im/post/5be4e549f265da61441f8dbe" target="_blank" rel="noopener">https://juejin.im/post/5be4e549f265da61441f8dbe</a></p><p>TCP：<a href="http://www.ruanyifeng.com/blog/2017/06/tcp-protocol.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2017/06/tcp-protocol.html</a></p><h5 id="BlinkDB"><a href="#BlinkDB" class="headerlink" title="BlinkDB"></a>BlinkDB</h5><ul><li>BlinkDB 是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎。</li></ul><h5 id="Spark-Sql"><a href="#Spark-Sql" class="headerlink" title="Spark Sql"></a>Spark Sql</h5><ul><li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-programming-guide.html</a></li><li>Spark SQL允许开发人员直接处理RDD，同时也可查询Hive、HBase等外部数据源。Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行查询，并进行更复杂的数据分析；</li></ul><h5 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h5><ul><li><a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/graphx-programming-guide.html</a></li><li>GraphX是Spark中用于图计算的API，可认为是Pregel在Spark上的重写及优化，Graphx性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。</li></ul><h5 id="MLBase"><a href="#MLBase" class="headerlink" title="MLBase"></a>MLBase</h5><ul><li><a href="https://amplab.cs.berkeley.edu/publication/mlbase-a-distributed-machine-learning-system/" target="_blank" rel="noopener">https://amplab.cs.berkeley.edu/publication/mlbase-a-distributed-machine-learning-system/</a></li></ul><h5 id="MlLib"><a href="#MlLib" class="headerlink" title="MlLib"></a>MlLib</h5><ul><li><a href="https://spark.apache.org/docs/latest/ml-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/ml-guide.html</a></li><li>MLlib提供了常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等，降低了机器学习的门槛，开发人员只要具备一定的理论知识就能进行机器学习的工作；</li></ul><h4 id="处理引擎"><a href="#处理引擎" class="headerlink" title="处理引擎"></a>处理引擎</h4><h5 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h5><ul><li><a href="https://blog.csdn.net/bingoxubin/article/details/79076978" target="_blank" rel="noopener">https://blog.csdn.net/bingoxubin/article/details/79076978</a></li><li>Spark Core包含Spark的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。Spark建立在统一的抽象RDD之上，使其可以以基本一致的方式应对不同的大数据处理场景；通常所说的Apache Spark，就是指Spark Core；</li></ul><h4 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h4><h5 id="Tachyon"><a href="#Tachyon" class="headerlink" title="Tachyon"></a>Tachyon</h5><ul><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-tachyon/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-tachyon/</a></li></ul><h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><ul><li><a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_design.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_design.html</a></li></ul><h5 id="Amazon-S3"><a href="#Amazon-S3" class="headerlink" title="Amazon S3"></a>Amazon S3</h5><ul><li><a href="https://aws.amazon.com/cn/s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/s3/</a></li></ul><h4 id="资源管理调度"><a href="#资源管理调度" class="headerlink" title="资源管理调度"></a>资源管理调度</h4><h5 id="Mesos"><a href="#Mesos" class="headerlink" title="Mesos"></a>Mesos</h5><ul><li><a href="http://mesos.apache.org/" target="_blank" rel="noopener">http://mesos.apache.org/</a></li></ul><h5 id="Hadoop-YARN"><a href="#Hadoop-YARN" class="headerlink" title="Hadoop YARN"></a>Hadoop YARN</h5><ul><li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a></li></ul><h2 id="运行架构"><a href="#运行架构" class="headerlink" title="运行架构"></a>运行架构</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="RDD："><a href="#RDD：" class="headerlink" title="RDD："></a>RDD：</h4><ul><li>是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型；</li></ul><h4 id="DAG："><a href="#DAG：" class="headerlink" title="DAG："></a>DAG：</h4><ul><li>是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系；</li></ul><h4 id="Executor："><a href="#Executor：" class="headerlink" title="Executor："></a>Executor：</h4><ul><li>是运行在工作节点（Worker Node）上的一个进程，负责运行任务，并为应用程序存储数据；</li></ul><h5 id="Spark所采用的Executor有两个优点"><a href="#Spark所采用的Executor有两个优点" class="headerlink" title="Spark所采用的Executor有两个优点"></a>Spark所采用的Executor有两个优点</h5><ul><li>一是利用多线程来执行具体的任务（Hadoop MapReduce采用的是进程模型），减少任务的启动开销；</li><li>二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这个存储模块里，下次需要时，就可以直接读该存储模块里的数据，而不需要读写到HDFS等文件系统里，因而有效减少了IO开销；或者在交互式查询场景下，预先将表缓存到该存储系统上，从而可以提高读写IO性能。</li></ul><h4 id="应用："><a href="#应用：" class="headerlink" title="应用："></a>应用：</h4><ul><li>用户编写的Spark应用程序；</li></ul><h4 id="任务："><a href="#任务：" class="headerlink" title="任务："></a>任务：</h4><ul><li>运行在Executor上的工作单元；</li></ul><h4 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h4><ul><li>一个作业包含多个RDD及作用于相应RDD上的各种操作；</li></ul><h4 id="阶段："><a href="#阶段：" class="headerlink" title="阶段："></a>阶段：</h4><ul><li>是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”，或者也被称为“任务集”。</li></ul><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><h4 id="集群资源管理器（Cluster-Manager）"><a href="#集群资源管理器（Cluster-Manager）" class="headerlink" title="集群资源管理器（Cluster Manager）"></a>集群资源管理器（Cluster Manager）</h4><ul><li>可以是Spark自带的资源管理器，也可以是YARN或Mesos等资源管理框架。</li></ul><h4 id="运行作业任务的工作节点（Worker-Node）"><a href="#运行作业任务的工作节点（Worker-Node）" class="headerlink" title="运行作业任务的工作节点（Worker Node）"></a>运行作业任务的工作节点（Worker Node）</h4><h4 id="每个应用的任务控制节点（Driver）"><a href="#每个应用的任务控制节点（Driver）" class="headerlink" title="每个应用的任务控制节点（Driver）"></a>每个应用的任务控制节点（Driver）</h4><h4 id="每个工作节点上负责具体任务的执行进程（Executor）"><a href="#每个工作节点上负责具体任务的执行进程（Executor）" class="headerlink" title="每个工作节点上负责具体任务的执行进程（Executor）"></a>每个工作节点上负责具体任务的执行进程（Executor）</h4><h3 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h3><ol><li>当一个Spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个SparkContext，由SparkContext负责和资源管理器（Cluster Manager）的通信以及进行资源的申请、任务的分配和监控等。SparkContext会向资源管理器注册并申请运行Executor的资源；</li><li>资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上；</li><li>SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器（DAGScheduler）进行解析，将DAG图分解成多个“阶段”（每个阶段都是一个任务集），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器（TaskScheduler）进行处理；Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行，同时，SparkContext将应用程序代码发放给Executor；</li><li>任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</li></ol><h3 id="运行架构特点"><a href="#运行架构特点" class="headerlink" title="运行架构特点"></a>运行架构特点</h3><pre><code>        *  每个应用都有自己专属的Executor进程，并且该进程在应用运行期间一直驻留。Executor进程以多线程的方式运行任务，减少了多进程任务频繁的启动开销，使得任务执行变得非常高效和可靠；        *  Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可；        *  Executor上有一个BlockManager存储模块，类似于键值存储系统（把内存和磁盘共同作为存储设备），在处理迭代计算任务时，不需要把中间结果写入到HDFS等文件系统，而是直接放在这个存储系统上，后续有需要时就可以直接读取；在交互式查询场景下，也可以把表提前缓存到这个存储系统上，提高读写IO性能；        *  任务采用了数据本地性和推测执行等优化机制。数据本地性是尽量将计算移到数据所在的节点上进行，即“计算向数据靠拢”，因为移动计算比移动数据所占的网络资源要少得多。而且，Spark采用了延时调度机制，可以在更大的程度上实现执行过程优化。比如，拥有数据的节点当前正被其他的任务占用，那么，在这种情况下是否需要将数据移动到其他的空闲节点呢？答案是不一定。因为，如果经过预测发现当前节点结束当前任务的时间要比移动数据的时间还要少，那么，调度就会等待，直到当前节点可用。</code></pre><h2 id="RDD的设计与运行原理"><a href="#RDD的设计与运行原理" class="headerlink" title="RDD的设计与运行原理"></a>RDD的设计与运行原理</h2><h3 id="RDD概念"><a href="#RDD概念" class="headerlink" title="RDD概念"></a>RDD概念</h3><ul><li><p>一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。</p></li><li><p>RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和groupBy）而创建得到新的RDD。</p></li><li><p>RDD提供了一组丰富的操作以支持常见的数据运算，分为“行动”（Action）和“转换”（Transformation）两种类型，</p><ol><li>行动：行动操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）</li><li>转换：转换操作（比如map、filter、groupBy、join等）接受RDD并返回RDD</li></ol><p><strong>注意事项：</strong></p><p>适合：对于数据集中元素执行相同操作的批处理式应用</p><p>不适合：不适合用于需要异步、细粒度状态的应用，比如Web应用系统、增量式的网页爬虫等</p></li></ul><h4 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h4><ul><li>RDD读入外部数据源（或者内存中的集合）进行创建；</li><li>RDD经过一系列的“转换”操作，每一次都会产生不同的RDD，供给下一个“转换”使用；</li><li>最后一个RDD经“行动”操作进行处理，并输出到外部数据源（或者变成Scala集合或标量）。</li></ul><h3 id="RDD特性"><a href="#RDD特性" class="headerlink" title="RDD特性"></a>RDD特性</h3><h4 id="高效的容错性。"><a href="#高效的容错性。" class="headerlink" title="高效的容错性。"></a>高效的容错性。</h4><ul><li>在RDD的设计中，数据只读，不可修改，如果需要修改数据，必须从父RDD转换到子RDD，由此在不同RDD之间建立了血缘关系。所以，RDD是一种天生具有容错机制的特殊集合，不需要通过数据冗余的方式（比如检查点）实现容错，而只需通过RDD父子依赖（血缘）关系重新计算得到丢失的分区来实现容错，无需回滚整个系统，这样就避免了数据复制的高开销，而且重算过程可以在不同节点之间并行进行，实现了高效的容错。</li><li>RDD提供的转换操作都是一些粗粒度的操作（比如map、filter和join），RDD依赖关系只需要记录这种粗粒度的转换操作，而不需要记录具体的数据和各种细粒度操作的日志（比如对哪个数据项进行了修改），这就大大降低了数据密集型应用中的容错开销；</li></ul><h4 id="中间结果持久化到内存。"><a href="#中间结果持久化到内存。" class="headerlink" title="中间结果持久化到内存。"></a>中间结果持久化到内存。</h4><ul><li>数据在内存中的多个RDD操作之间进行传递，不需要“落地”到磁盘上，避免了不必要的读写磁盘开销；</li></ul><h4 id="存放的数据可以是Java对象，"><a href="#存放的数据可以是Java对象，" class="headerlink" title="存放的数据可以是Java对象，"></a>存放的数据可以是Java对象，</h4><ul><li>避免了不必要的对象序列化和反序列化开销。</li></ul><h3 id="RDD间的依赖关系"><a href="#RDD间的依赖关系" class="headerlink" title="RDD间的依赖关系"></a>RDD间的依赖关系</h3><h4 id="窄依赖（Narrow-Dependency）"><a href="#窄依赖（Narrow-Dependency）" class="headerlink" title="窄依赖（Narrow Dependency）"></a>窄依赖（Narrow Dependency）</h4><ul><li>表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区；</li><li>窄依赖典型的操作包括map、filter、union等</li><li>对于窄依赖的RDD，可以以流水线的方式计算所有父分区，不会造成网络之间的数据混合。</li></ul><h4 id="宽依赖（Wide-Dependency）"><a href="#宽依赖（Wide-Dependency）" class="headerlink" title="宽依赖（Wide Dependency）"></a>宽依赖（Wide Dependency）</h4><ul><li>表现为存在一个父RDD的一个分区对应一个子RDD的多个分区</li><li>宽依赖典型的操作包括groupByKey、sortByKey等</li><li>对于宽依赖的RDD，则通常伴随着Shuffle操作，即首先需要计算好所有父分区数据，然后在节点之间进行Shuffle。</li></ul><p><strong>比较：</strong></p><ul><li>相对而言，在两种依赖关系中，窄依赖的失败恢复更为高效，它只需要根据父RDD分区重新计算丢失的分区即可（不需要重新计算所有分区），而且可以并行地在不同节点进行重新计算。</li><li>而对于宽依赖而言，单个节点失效通常意味着重新计算过程会涉及多个父RDD分区，开销较大。</li><li>此外，Spark还提供了数据检查点和记录日志，用于持久化中间RDD，从而使得在进行失败恢复时不需要追溯到最开始的阶段。</li><li>在进行故障恢复时，Spark会对数据检查点开销和重新计算RDD分区的开销进行比较，从而自动选择最优的恢复策略。</li></ul><h3 id="阶段的划分"><a href="#阶段的划分" class="headerlink" title="阶段的划分"></a>阶段的划分</h3><ul><li>在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到当前的阶段中；将窄依赖尽量划分在同一个阶段中，可以实现流水线计算（具体的阶段划分算法请参见AMP实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》）</li></ul><h3 id="RDD运行过程"><a href="#RDD运行过程" class="headerlink" title="RDD运行过程"></a>RDD运行过程</h3><ol><li>创建RDD对象；</li><li>SparkContext负责计算RDD之间的依赖关系，构建DAG；</li><li>DAGScheduler负责把DAG图分解成多个阶段，每个阶段中包含了多个任务，每个任务会被任务调度器分发给各个工作节点（Worker Node）上的Executor去执行。</li></ol><h2 id="SPARK的部署模式"><a href="#SPARK的部署模式" class="headerlink" title="SPARK的部署模式"></a>SPARK的部署模式</h2><h3 id="三种部署模式"><a href="#三种部署模式" class="headerlink" title="三种部署模式"></a>三种部署模式</h3><h4 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h4><ul><li>Spark框架本身也自带了完整的资源调度管理服务，可以独立部署到一个集群中，而不需要依赖其他系统来为其提供资源管理调度服务。</li></ul><h4 id="Spark-on-Mesos模式"><a href="#Spark-on-Mesos模式" class="headerlink" title="Spark on Mesos模式"></a>Spark on Mesos模式</h4><ul><li>Spark运行在Mesos上，要比运行在YARN上更加灵活、自然。目前，Spark官方推荐采用这种模式，</li></ul><h4 id="Spark-on-YARN模式"><a href="#Spark-on-YARN模式" class="headerlink" title="Spark on YARN模式"></a>Spark on YARN模式</h4><ul><li>Spark可运行于YARN之上，与Hadoop进行统一部署，即“Spark on YARN”，</li></ul><h3 id="从“Hadoop-Storm”架构转向Spark架构"><a href="#从“Hadoop-Storm”架构转向Spark架构" class="headerlink" title="从“Hadoop+Storm”架构转向Spark架构"></a>从“Hadoop+Storm”架构转向Spark架构</h3><h4 id="采用“Hadoop-Storm”部署方式的一个案例"><a href="#采用“Hadoop-Storm”部署方式的一个案例" class="headerlink" title="采用“Hadoop+Storm”部署方式的一个案例"></a>采用“Hadoop+Storm”部署方式的一个案例</h4><p>​                大数据层<br>​                    数据存储<br>​                        HDFS<br>​                        HBase<br>​                        Cassandra<br>​                    调度器<br>​                        YARN<br>​                        Mesos<br>​                    实时查询<br>​                        Redis<br>​                        Solr<br>​                        HBase<br>​                    离线分析<br>​                        Hive<br>​                        Pig<br>​                        Impala<br>​                        MapReduce<br>​                    用户行为实时分析<br>​                        UV<br>​                        PV<br>​                    Storm实时流处理<br>​                        欺诈监控<br>​                        系统报警<br>​                        点击流推荐<br>​                数据收集<br>​                    业务数据收集<br>​                        Flume<br>​                        Kafka<br>​                        ETL<br>​                    网站数据收集<br>​                        Collector<br>​                    用户行为数据收集<br>​                        PV/UV<br>​                        点击流信息<br>​                        导航数据收集<br>​                业务应用层<br>​                    应用数据<br>​                        导航日志<br>​                        应用日志<br>​                    系统数据<br>​                        系统日志<br>​                        报警数据<br>​                繁琐！</p><h4 id="Spark架构优点"><a href="#Spark架构优点" class="headerlink" title="Spark架构优点"></a>Spark架构优点</h4><ul><li>实现一键式安装和配置、线程级别的任务监控和告警；</li><li>降低硬件集群、软件维护、任务监控和应用开发的难度；</li><li>便于做成统一的硬件、计算平台资源池。</li><li>Spark Streaming的原理是将流数据分解成一系列短小的批处理作业，每个短小的批处理作业使用面向批处理的Spark Core进行处理，通过这种方式变相实现流计算，而不是真正实时的流计算，因而通常无法实现毫秒级的响应</li></ul><h4 id="Hadoop和Spark的统一部署"><a href="#Hadoop和Spark的统一部署" class="headerlink" title="Hadoop和Spark的统一部署"></a>Hadoop和Spark的统一部署</h4><ul><li>一方面，由于Hadoop生态系统中的一些组件所实现的功能，目前还是无法由Spark取代的，比如，Storm可以实现毫秒级响应的流计算，但是，Spark则无法做到毫秒级响应。</li><li>另一方面，企业中已经有许多现有的应用，都是基于现有的Hadoop组件开发的，完全转移到Spark上需要一定的成本。</li><li>实时计算工具<ol><li>storm：<a href="https://www.w3cschool.cn/apache_storm/" target="_blank" rel="noopener">https://www.w3cschool.cn/apache_storm/</a></li><li>flink：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/tutorials/local_setup.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/tutorials/local_setup.html</a></li></ol></li></ul><h1 id="Spark进阶"><a href="#Spark进阶" class="headerlink" title="Spark进阶"></a>Spark进阶</h1><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul><li><a href="https://study.163.com/course/courseMain.htm?courseId=1002887002" target="_blank" rel="noopener">大数据技术原理与应用</a></li><li><a href="https://study.163.com/course/courseMain.htm?courseId=1209408816" target="_blank" rel="noopener">SPARK编程指南-Python</a></li><li><a href="https://www.w3cschool.cn/spark/" target="_blank" rel="noopener">w3school</a></li><li><a href="http://dblab.xmu.edu.cn/blog/1709-2/" target="_blank" rel="noopener">厦门大学博客</a></li><li><a href="https://spark.apache.org/docs/latest/index.html" target="_blank" rel="noopener">SPARK官网</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之HDFS</title>
      <link href="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/"/>
      <url>/2019/11/27/shi-yong-gong-ju-zhi-hdfs/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/image-20210621151650243.png" alt="image-20210621151650243"></p><h1 id="HDFS基础"><a href="#HDFS基础" class="headerlink" title="HDFS基础"></a>HDFS基础</h1><h2 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h2><ul><li>分布式文件系统把文件分布存储在多个计算机节点上，成千上万的计算机节点构成计算机集群</li></ul><h2 id="HDFS简介与相关概念"><a href="#HDFS简介与相关概念" class="headerlink" title="HDFS简介与相关概念"></a>HDFS简介与相关概念</h2><h3 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h3><h4 id="实现目标："><a href="#实现目标：" class="headerlink" title="实现目标："></a>实现目标：</h4><ul><li>兼容廉价的硬件设备</li><li>流数据读写</li><li>大数据集</li><li>简单的文件模型</li><li>强大的跨平台兼容性</li></ul><h4 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h4><ul><li>不适合低延迟数据访问</li><li>无法高效存储大量小文件</li><li>不支持多用户写入及任意修改文件</li></ul><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><h4 id="块"><a href="#块" class="headerlink" title="块"></a>块</h4><ul><li>HDFS默认一个块64MB，一个文件被分成多个块，以块为存储单位</li><li>块的大小远远大于普通文件系统，可以最小化寻址开销</li><li>块带来的好处：<ol><li>支持大规模文件存储：一个大规模文件可以分成若干个块，不同的块分发到不同的节点上</li><li>简化系统设计：大大简化了存储管理以及方便了元数据的管理，元数据管理可以由其他系统负责</li><li>适合数据备份：采用冗余存储到多个节点，提高容错性和可用性</li></ol></li></ul><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><h5 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h5><ul><li>存储元数据</li><li>元数据保存到内存中</li><li>保存文件，block，DataNode之间的映射关系</li></ul><h5 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h5><p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/namenode_architecture.png" alt="图：DataNode数据结构"></p><ul><li>FsImage：维护系统文件树以及所有元数据，包含文件的复制等级、修改、访问时间、访问权限、块大小以及组成文件的块，目录的存储修改时间、权限和配额。没有记录块存储在哪个DataNode，而是存储映射关系到内存。每次增加DataNode到集群时，DataNode都会把自己包含的块列表告知NameNode，确保映射是最新</li><li>EditLog：记录创建、删除、重命名等操作</li></ul><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><ul><li>启动的时候，将FsImage文件中的内容加载到内存，再执行EditLog中的各项操作，使得内存中的元数据和实际的同步</li><li>一旦内存中成功建立文件系统元数据的映射，则创建一个新的FsImage和一个<strong>空的</strong>EditLog</li><li>每次执行写操作之后，且在向客户端发送代码之前，EditLog都需要同步更新</li><li>因为FsImage文件一般很大（GB常见），如果所有的更新操作都往里加，则会导致系统变慢，所以需要一个不断更新的EditLog</li><li>但是随着对文件不断的更新，EditLog也会不断的增大，怎么解决这个问题呢？<strong><em>使用SecondaryNameNode</em></strong>，详情如下：</li></ul><p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/editlog.png" alt="图：EditLog解决"></p><h5 id="内存全景"><a href="#内存全景" class="headerlink" title="内存全景"></a>内存全景</h5><p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/namenode_memory.png" alt="图：DataNode内存全景"></p><ul><li>参考<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">HDFS NameNode内存全景</a></li></ul><h4 id="DatanNode（廉价机器）"><a href="#DatanNode（廉价机器）" class="headerlink" title="DatanNode（廉价机器）"></a>DatanNode（廉价机器）</h4><h5 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h5><ul><li>存储文件内容</li><li>文件内容保存到磁盘</li><li>维护block id到DataNode本地文件的映射关系</li></ul><h2 id="HDFS结构"><a href="#HDFS结构" class="headerlink" title="HDFS结构"></a>HDFS结构</h2><p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/hdfs_architecture.png" alt="图：hdfs结构"></p><h3 id="HDFS命名空间管理"><a href="#HDFS命名空间管理" class="headerlink" title="HDFS命名空间管理"></a>HDFS命名空间管理</h3><ul><li>命名空间包含目录、文件和块</li><li>使用的是传统的分级文件体系</li></ul><h3 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h3><ul><li>在TCP/TP协议之上</li><li>客户端通过一个可配置的端口向NameNode主动发起TCP连接，并使用<strong>客户端协议</strong>与NameNode进行交互</li><li>NameNode与DataNode之间使用<strong>数据节点协议</strong>交互</li><li>客户端与DataNode使用RPC（remote procedure call）实现</li></ul><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><ul><li>HDFS在部署时提供了客户端</li><li>客户端是一个库，暴露HDFS文件系统接口</li><li>客户端支持打开、读取、写入等常见操作，提供类shell的命令行访问数据</li></ul><h3 id="HDFS-1-0-局限性"><a href="#HDFS-1-0-局限性" class="headerlink" title="HDFS-1.0-局限性"></a>HDFS-1.0-局限性</h3><ul><li>命名空间的限制：NameNode保存在内存中，受到内存空间大小限制</li><li>性能的瓶颈：受限单个NameNode的吞吐量</li><li>隔离问题：集群中只有一个NameNode，只有一个命名空间，因此没法对不同程序进行隔离</li><li>集群的可用性：一旦唯一的NameNode发生故障，则导致整个集群不可用</li></ul><h2 id="HDFS存储原理"><a href="#HDFS存储原理" class="headerlink" title="HDFS存储原理"></a>HDFS存储原理</h2><h3 id="冗余数据保存"><a href="#冗余数据保存" class="headerlink" title="冗余数据保存"></a>冗余数据保存</h3><ul><li>多副本对数据进行冗余保存，一般默认冗余保存3份</li><li>优点<ol><li>加快数据传输速度</li><li>容易检查数据错误</li><li>保证数据可靠性</li></ol></li></ul><h3 id="数据存取策略"><a href="#数据存取策略" class="headerlink" title="数据存取策略"></a>数据存取策略</h3><h4 id="数据存放"><a href="#数据存放" class="headerlink" title="数据存放"></a>数据存放</h4><ul><li>第一个副本：放置在上传文件的数据节点；如果在集群外提交，则随机挑选机器存放</li><li>第二个副本：放置在与第一个副本<strong>不同的机架</strong>（rack）的节点上</li><li>第三个副本：与第一个副本<strong>相同机架</strong>的不同节点上</li></ul><h4 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h4><ul><li>当客户端读取数据时，从NameNode获得数据块不同副本的存放位置列表，通过API来获取这些存放位置的机架ID与客户端对应机架ID，如果ID相同，则优先选择该副本，反之随机读取</li></ul><h3 id="数据错误与恢复"><a href="#数据错误与恢复" class="headerlink" title="数据错误与恢复"></a>数据错误与恢复</h3><h4 id="NameNode出错"><a href="#NameNode出错" class="headerlink" title="NameNode出错"></a>NameNode出错</h4><ul><li>使用SecondaryNameNode进行数据恢复</li></ul><h4 id="DataNode出错"><a href="#DataNode出错" class="headerlink" title="DataNode出错"></a>DataNode出错</h4><ul><li>心跳机制：每个DataNode会定期向NameNode发送心跳信息</li><li>当DataNode出错时，NameNode收不到心跳，则会将他们标记为“宕机”，其节点上所有数据标记为“不可读”，也不会再给他们发送任何IO请求</li><li>NameNode还会检查，当某个数据库的副本数量小于冗余因子，就会启动<strong>数据冗余复制</strong>，产生新副本</li></ul><h4 id="数据出错"><a href="#数据出错" class="headerlink" title="数据出错"></a>数据出错</h4><ul><li>网络传输和磁盘错误等因素，会造成数据错误</li><li>当文件本创建时，客户端会对每一个文件进行信息摘录，并写入到同一路径的隐藏文件中</li><li>当客户端读取文件的时候，会先读取信息摘录文件，然后对读取的数据块进行校验，如果检验出错，客户端则会<strong>请求到另外一个DataNode读取文件块</strong>，并向NameNode报告这个文件块有错误，然后NameNode会重新复制这个块</li></ul><h2 id="HDFS数据读写过程"><a href="#HDFS数据读写过程" class="headerlink" title="HDFS数据读写过程"></a>HDFS数据读写过程</h2><h3 id="读过程-JAVA"><a href="#读过程-JAVA" class="headerlink" title="读过程-JAVA"></a>读过程-JAVA</h3><ul><li>JAVA读过程</li></ul><p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/java_read_hdfs.png" alt="图：JAVA读HDFS过程"></p><h3 id="写过程-JAVA"><a href="#写过程-JAVA" class="headerlink" title="写过程-JAVA"></a>写过程-JAVA</h3><ul><li>JAVA写过程</li></ul><p><img src="/2019/11/27/shi-yong-gong-ju-zhi-hdfs/java_write_hdfs.png" alt="图：JAVA写HDFS过程"></p><h1 id="HDFS进阶"><a href="#HDFS进阶" class="headerlink" title="HDFS进阶"></a>HDFS进阶</h1><h2 id="HDFS编程实践"><a href="#HDFS编程实践" class="headerlink" title="HDFS编程实践"></a>HDFS编程实践</h2><ul><li><a href="http://dblab.xmu.edu.cn/blog/290-2/" target="_blank" rel="noopener">编程实践</a></li></ul><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul><li><a href="https://study.163.com/course/courseMain.htm?courseId=1002887002" target="_blank" rel="noopener">大数据技术原理与应用</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算广告之理论</title>
      <link href="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/"/>
      <url>/2019/11/22/ji-suan-guang-gao-zhi-li-lun/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/image-20210621160249789.png" alt="image-20210621160249789"></p><h1 id="广告的基本知识"><a href="#广告的基本知识" class="headerlink" title="广告的基本知识"></a>广告的基本知识</h1><h2 id="广告的目的"><a href="#广告的目的" class="headerlink" title="广告的目的"></a>广告的目的</h2><h3 id="什么是广告？"><a href="#什么是广告？" class="headerlink" title="什么是广告？"></a>什么是广告？</h3><h4 id="广告的定义"><a href="#广告的定义" class="headerlink" title="广告的定义"></a>广告的定义</h4><ul><li>广告是由已确定的出资人通过各种媒介进行的有关产品的、通常有偿、有组织、综合的、劝服性的非人员信息传播活动。</li></ul><h4 id="广告的主体"><a href="#广告的主体" class="headerlink" title="广告的主体"></a>广告的主体</h4><ul><li>出资人：广告主</li><li>媒介</li><li>受众</li></ul><h4 id="广告的本质功能"><a href="#广告的本质功能" class="headerlink" title="广告的本质功能"></a>广告的本质功能</h4><ul><li>借助媒介的力量，完成较低成本的用户接触</li></ul><h2 id="广告的有效性模型"><a href="#广告的有效性模型" class="headerlink" title="广告的有效性模型"></a>广告的有效性模型</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/guanggaoyouxiaoxing.png" alt="图：广告有效性模型"></p><h2 id="广告与营销的区别"><a href="#广告与营销的区别" class="headerlink" title="广告与营销的区别"></a>广告与营销的区别</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/guanggaoyuxiaoshoudifferent.png" alt="图：广告与销售区别"></p><h2 id="在线广告的特点"><a href="#在线广告的特点" class="headerlink" title="在线广告的特点"></a>在线广告的特点</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/zaixianguanggaodutexing.png" alt="图：在线广告独特性"></p><h2 id="在线广告市场"><a href="#在线广告市场" class="headerlink" title="在线广告市场"></a>在线广告市场</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/ads_online.png" alt="图：在线广告结构"></p><p><strong>媒体三种变现方式：</strong></p><ol><li>直接给广告网络</li><li>通过ADX</li><li>通过SSP</li></ol><ul><li>需求方：广告主等等</li><li>供给方：媒体等等</li></ul><h2 id="计算广告核心问题和挑战"><a href="#计算广告核心问题和挑战" class="headerlink" title="计算广告核心问题和挑战"></a>计算广告核心问题和挑战</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/tiaozhan.png" alt="图：核心计算问题"></p><p><strong>挑战：</strong></p><ul><li>大规模：用户量大，高并发在线投放系统</li><li>动态性：用户的兴趣随时变化</li><li>丰富的查询信息：需要把用户和上下文的多样信号一起综合</li><li>探索与发现：需要主动探索未知的数据，例如用户的其他未展示信息</li></ul><h2 id="广告、搜索和推荐的比较"><a href="#广告、搜索和推荐的比较" class="headerlink" title="广告、搜索和推荐的比较"></a>广告、搜索和推荐的比较</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/qubie.png" alt="图：区别"></p><h2 id="投资回报（ROI）分析"><a href="#投资回报（ROI）分析" class="headerlink" title="投资回报（ROI）分析"></a>投资回报（ROI）分析</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/roi.png" alt="图：ROI"></p><p><strong>参数解释：</strong></p><ol><li><p>$a_i$：广告</p></li><li><p>$u_i$：用户</p></li><li>$c_i$：上下文</li></ol><h2 id="在线广告系统结构"><a href="#在线广告系统结构" class="headerlink" title="在线广告系统结构"></a>在线广告系统结构</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/ads_system.png" alt="图：在线广告系统结构"></p><h1 id="合约广告系统"><a href="#合约广告系统" class="headerlink" title="合约广告系统"></a>合约广告系统</h1><h2 id="常用广告系统开源工具"><a href="#常用广告系统开源工具" class="headerlink" title="常用广告系统开源工具"></a>常用广告系统开源工具</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/tool.png" alt="图：开源工具"></p><ul><li>GFS</li><li>Thrift</li></ul><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/thrift.png" alt="图：thrift工具"></p><ul><li>Storm</li><li>Scribe</li><li>Flume</li><li>Chubby</li><li>Hadoop</li><li>HDFS</li><li>MapReduce</li><li>Avro</li><li>Chuhwa</li><li>ZooKeeper</li><li>HBase</li><li>Oozie</li><li>Pig</li><li>Hive</li><li>BigTable</li><li>HYPERTABLE</li><li>Cassandra</li><li>mahout</li><li>Elephant-bird</li></ul><h2 id="合约广告简介"><a href="#合约广告简介" class="headerlink" title="合约广告简介"></a>合约广告简介</h2><h3 id="直接媒体购买"><a href="#直接媒体购买" class="headerlink" title="直接媒体购买"></a>直接媒体购买</h3><ul><li>供给方：广告排期系统，帮助媒体自动执行多个合同的排期，不提供受众定向</li><li>需求方：代理商，帮助广告商策划和执行排期，用经验和人工满足广告商的质量需求</li><li>代表：4A公司</li></ul><h3 id="担保式投送与广告投放"><a href="#担保式投送与广告投放" class="headerlink" title="担保式投送与广告投放"></a>担保式投送与广告投放</h3><ul><li>担保式投送（guaranteed delivery）（保证量）：基于合约的广告机制，量优于质的销售方式，采用CPM的方式结算</li><li>广告投放机（Ad server）（希望帮广告商做一些优化，优化质）：CPM必然要求广告投送由服务器完成决策，受众定向、CTR预测和流量预测是广告投放机的基础，担保式投送下，投放机满足各合约的量，并尽可能优化各广告主流量的质</li></ul><h2 id="在线分配问题"><a href="#在线分配问题" class="headerlink" title="在线分配问题"></a>在线分配问题</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/online_allocation.png" alt="图：在线分配问题"></p><ul><li>用拉格朗日方法求解</li></ul><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/method1.png" alt="图：拉格朗日方法"></p><ul><li>$b_{ia}$：第i个impression（曝光）拿到广告a的收益</li><li>$x_{ia}$：流量分配以及预算控制</li><li>display ad problem：有量的需求，量的下限或预算（上限）的需求</li><li>adwords problem：没有明确的量需求</li><li>NGD：non-guaranteed delivery（上限的需求）</li><li>流量预测下的GD在线分配</li></ul><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/method2.png" alt="图：GD在线分配"></p><h2 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/mapreduce.png" alt="图：mapreduce在统计中应用"></p><h1 id="受众定向（audience-targeting）-在线广告的核心"><a href="#受众定向（audience-targeting）-在线广告的核心" class="headerlink" title="受众定向（audience targeting）-在线广告的核心"></a>受众定向（audience targeting）-在线广告的核心</h1><h2 id="受众定向概念"><a href="#受众定向概念" class="headerlink" title="受众定向概念"></a>受众定向概念</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/audience_targeting.png" alt="图：受众定向"></p><h3 id="常见受众定向方式"><a href="#常见受众定向方式" class="headerlink" title="常见受众定向方式"></a>常见受众定向方式</h3><ul><li>$f(a,u)$：<ol><li><strong>重定向</strong>，如果某个用户访问某个广告，则给该用户或该广告打上某一个标签</li><li><strong>lookalike</strong>，从海量数据中找到与已有种子用户相似的用户</li></ol></li><li>$f(u)$：行为定向</li><li>$f(c)$：上下文定向</li></ul><h2 id="行为定向-对user打标签"><a href="#行为定向-对user打标签" class="headerlink" title="行为定向-对user打标签"></a>行为定向-对user打标签</h2><h3 id="九种重要的原始行为（按信息强度排序）"><a href="#九种重要的原始行为（按信息强度排序）" class="headerlink" title="九种重要的原始行为（按信息强度排序）"></a>九种重要的原始行为（按信息强度排序）</h3><ul><li>transaction：交易信息</li><li>pre-transaction：预交易信息</li><li>paid search click：付款点击信息</li><li>ad click：广告点击信息</li><li>search click：搜索点击信息</li><li>search： 搜索信息</li><li>share：分享信息</li><li>page view：网页浏览信息</li><li>ad view：广告流量信息</li></ul><p><strong>行为定向计算：</strong>为用户的每个行为标签计算一个值（强度）</p><h3 id="行为定向其他标签"><a href="#行为定向其他标签" class="headerlink" title="行为定向其他标签"></a>行为定向其他标签</h3><ul><li>session log：以用户ID为key的形式，可以将targeting变成局部运算</li><li>long-term：多日标签累计计算，采用时间衰减方式，仅需昨天的f和今天的t</li></ul><h2 id="上下文定向-即时标签"><a href="#上下文定向-即时标签" class="headerlink" title="上下文定向-即时标签"></a>上下文定向-即时标签</h2><p><strong>以url举例：</strong></p><ul><li>用在线cache系统存储url</li><li>不预先加载任何cache内容，对cache中不存在的url，立刻返回空特征，同时触发相应的页面爬虫和特征提取</li><li>设置cache系统合适的失效时间以完成特征自动更新</li></ul><h2 id="Topic-Model-上下文（页面主题分析）"><a href="#Topic-Model-上下文（页面主题分析）" class="headerlink" title="Topic Model-上下文（页面主题分析）"></a>Topic Model-上下文（页面主题分析）</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/topic_model.png" alt="图：常用模型图"></p><p><strong>参数解释：</strong></p><ul><li>$\pi$：先验分布</li><li>$z$：隐含主题</li><li>$w$：词word</li></ul><h2 id="数据加工和交易"><a href="#数据加工和交易" class="headerlink" title="数据加工和交易"></a>数据加工和交易</h2><h3 id="精准广告业务是什么？"><a href="#精准广告业务是什么？" class="headerlink" title="精准广告业务是什么？"></a>精准广告业务是什么？</h3><ul><li>数据源——用户行为——定向系统——用户标签——投放机</li></ul><h3 id="有价值的数据"><a href="#有价值的数据" class="headerlink" title="有价值的数据"></a>有价值的数据</h3><ul><li>用户标识</li><li>用户行为</li></ul><h3 id="数据管理平台（DMP）"><a href="#数据管理平台（DMP）" class="headerlink" title="数据管理平台（DMP）"></a>数据管理平台（DMP）</h3><ul><li>目的：<ol><li>为网站提供数据加工和对外交易能力</li><li>加工跨媒体用户标签，在交易市场中售卖</li><li>是否应直接从事广告交易存在争议</li></ol></li><li>关键特征：<ol><li>定制化用户划分</li><li>统一的对外数据接口</li></ol></li></ul><h3 id="data-highway工具"><a href="#data-highway工具" class="headerlink" title="data highway工具"></a>data highway工具</h3><ul><li>scribe：大规模分布式日志收集系统</li><li><strong>flume</strong></li><li>chukwa</li></ul><h1 id="竞价广告系统（supply端）"><a href="#竞价广告系统（supply端）" class="headerlink" title="竞价广告系统（supply端）"></a>竞价广告系统（supply端）</h1><h2 id="位置拍卖理论"><a href="#位置拍卖理论" class="headerlink" title="位置拍卖理论"></a>位置拍卖理论</h2><h4 id="竞价系统理论"><a href="#竞价系统理论" class="headerlink" title="竞价系统理论"></a>竞价系统理论</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/bid_theory.png" alt="图：竞价系统理论"></p><h4 id="定价机制"><a href="#定价机制" class="headerlink" title="定价机制"></a>定价机制</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/bid_theory1.png" alt="图：定价机制"></p><h2 id="广告网络概念"><a href="#广告网络概念" class="headerlink" title="广告网络概念"></a>广告网络概念</h2><h4 id="广告网络"><a href="#广告网络" class="headerlink" title="广告网络"></a>广告网络</h4><ul><li>connect advertisers to web sites that want to host advertisements</li><li>自行估计给定（a,u,c）组合的CTR</li></ul><h4 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h4><ul><li>竞价系统</li><li>淡化广告位概念</li><li>最合适的计价方式是CPC</li><li>不易支持定制化用户划分</li></ul><h2 id="广告检索"><a href="#广告检索" class="headerlink" title="广告检索"></a>广告检索</h2><h4 id="布尔表达式检索"><a href="#布尔表达式检索" class="headerlink" title="布尔表达式检索"></a>布尔表达式检索</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/search.png" alt="图：布尔表达式检索"></p><ul><li>index算法</li></ul><h4 id="长query情况下的相关性检索"><a href="#长query情况下的相关性检索" class="headerlink" title="长query情况下的相关性检索"></a>长query情况下的相关性检索</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/search1.png" alt="图：长query检索"></p><ul><li>weigth-and（wand）检索算法</li></ul><h2 id="流量预测"><a href="#流量预测" class="headerlink" title="流量预测"></a>流量预测</h2><ul><li>可以视query为a，对（u,c）进行检索的反向检索（retrieval）问题</li><li>由于（u,c）联合空间规模过大，需要进行预处理</li></ul><h4 id="预测过程"><a href="#预测过程" class="headerlink" title="预测过程"></a>预测过程</h4><ul><li>给定a，首先通过c的索引找出所有符合条件c的集合</li><li>对每个c估计eCPM(a,c)，并根据eCPM得到a在c上胜出的百分比并累加则得到a的流量预测值</li></ul><h2 id="ZooKeeper介绍-分布式同步服务"><a href="#ZooKeeper介绍-分布式同步服务" class="headerlink" title="ZooKeeper介绍-分布式同步服务"></a>ZooKeeper介绍-分布式同步服务</h2><ul><li>在基于消息传递通信模型的分布式环境下解决一致性问题的基础服务</li><li>用层次式namespace维护同步需要的状态空间</li><li>保证实现特性：timeliness等</li><li>较复杂的同步模式需要利用API编程实现</li><li>Paxos算法</li></ul><h2 id="点击率预测与逻辑回归"><a href="#点击率预测与逻辑回归" class="headerlink" title="点击率预测与逻辑回归"></a>点击率预测与逻辑回归</h2><h4 id="点击率预测"><a href="#点击率预测" class="headerlink" title="点击率预测"></a>点击率预测</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/ctr.png" alt="图：点击率预测"></p><h2 id="逻辑回归优化方法介绍"><a href="#逻辑回归优化方法介绍" class="headerlink" title="逻辑回归优化方法介绍"></a>逻辑回归优化方法介绍</h2><h4 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/lbfgs.png" alt="图：优化方法"></p><h4 id="ADMM方法"><a href="#ADMM方法" class="headerlink" title="ADMM方法"></a>ADMM方法</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/admm.png" alt="图：admm"></p><h2 id="动态特征"><a href="#动态特征" class="headerlink" title="动态特征"></a>动态特征</h2><h4 id="多层次点击反馈"><a href="#多层次点击反馈" class="headerlink" title="多层次点击反馈"></a>多层次点击反馈</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/feature.png" alt="图：多层次点击"></p><h1 id="探索广告与广告网络（suppy端）"><a href="#探索广告与广告网络（suppy端）" class="headerlink" title="探索广告与广告网络（suppy端）"></a>探索广告与广告网络（suppy端）</h1><h2 id="探索与利用"><a href="#探索与利用" class="headerlink" title="探索与利用"></a>探索与利用</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/explore.png" alt="图：探索与利用"></p><h4 id="UCB"><a href="#UCB" class="headerlink" title="UCB"></a>UCB</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/ucb.png" alt="图：ucb"></p><h4 id="Contextual-Bandit"><a href="#Contextual-Bandit" class="headerlink" title="Contextual Bandit"></a>Contextual Bandit</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/contextual.png" alt="图：contextual bandit"></p><h2 id="搜索广告"><a href="#搜索广告" class="headerlink" title="搜索广告"></a>搜索广告</h2><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul><li>广告网络的特殊形式<ol><li>用户定向标签</li><li>session内的短时间用户搜索行为更重要</li><li>上下文定向标签</li></ol></li><li>典型位置竞价模式<ol><li>分南区、北区、东区等</li></ol></li></ul><h4 id="查询词扩展"><a href="#查询词扩展" class="headerlink" title="查询词扩展"></a>查询词扩展</h4><ul><li>基于推荐的方法<ol><li>挖掘矩阵找到相关query</li></ol></li><li>基于语义的方法<ol><li>topic model</li></ol></li><li>基于收益的方法<ol><li>根据eCPM统计得到表现较好的相关query</li></ol></li></ul><h4 id="用户相关的搜索广告决策"><a href="#用户相关的搜索广告决策" class="headerlink" title="用户相关的搜索广告决策"></a>用户相关的搜索广告决策</h4><ul><li>结果个性化对于搜索广告作用有限</li><li>广告展示条数是可以深度个性化的</li><li>可以根据统一session内的行为调整广告结果</li></ul><h4 id="短时用户行为反馈"><a href="#短时用户行为反馈" class="headerlink" title="短时用户行为反馈"></a>短时用户行为反馈</h4><ul><li>短时用户行为</li><li>短时用户行为反馈<ol><li>短时受众定向</li><li>短时点击反馈</li></ol></li><li>短时用户行为计算<ol><li>需要准实时对用户行为加工，不适合在Hadoop上进行</li></ol></li></ul><h2 id="流式计算平台"><a href="#流式计算平台" class="headerlink" title="流式计算平台"></a>流式计算平台</h2><h4 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h4><ul><li>大规模实时数据处理框架，自动完成数据分发和可靠性管理，开发者只需要关注处理逻辑</li><li>数据流基本在网络和内存进行</li><li>计算逻辑类似mapreduce，区别在于<strong>调度数据而非调度计算</strong>，hadoop却是<strong>调度计算</strong></li></ul><h2 id="广告购买平台（demand端）-trading-desk"><a href="#广告购买平台（demand端）-trading-desk" class="headerlink" title="广告购买平台（demand端）-trading desk"></a>广告购买平台（demand端）-trading desk</h2><h4 id="关键特征"><a href="#关键特征" class="headerlink" title="关键特征"></a>关键特征</h4><ul><li>连接到不同媒体和广告网络，为广告商提供universal marketplace</li><li>非实时竞价campaign的ROI优化能力</li><li>经常由代理公司孵化出来</li></ul><h4 id="非RTB流量的ROI优化"><a href="#非RTB流量的ROI优化" class="headerlink" title="非RTB流量的ROI优化"></a>非RTB流量的ROI优化</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/non_rtb_roi.png" alt="图：non_rtb_roi"></p><h1 id="广告交易市场（ADX）"><a href="#广告交易市场（ADX）" class="headerlink" title="广告交易市场（ADX）"></a>广告交易市场（ADX）</h1><h2 id="广告交易市场（ad-exchange）"><a href="#广告交易市场（ad-exchange）" class="headerlink" title="广告交易市场（ad-exchange）"></a>广告交易市场（ad-exchange）</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/adx.png" alt="图：adx"></p><p><strong>主要流程：</strong></p><ul><li>某个 User 访问某个网站，网站通知 Adx 目前有一次展示机会</li><li>Adx 通过 RTBD 接口向各个 DSP 询价，从而获取 DSP提供的各个广告（相当于图中的 Ad retrieval）</li><li>Adx 通过出价进行排序，并选择出价最高的广告展示给用户（相当于图中的 Ad ranking）</li><li>Adx 在日志中记录该广告的展示记录</li></ul><h4 id="关键特征-1"><a href="#关键特征-1" class="headerlink" title="关键特征"></a>关键特征</h4><ul><li>用实时竞价方式连接广告和上下文、用户</li><li>按照展示上的竞价收取广告主费用</li></ul><h2 id="实时竞价-RTB"><a href="#实时竞价-RTB" class="headerlink" title="实时竞价-RTB"></a>实时竞价-RTB</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/rtb.png" alt="图：rtb"></p><h2 id="cookie-mapping"><a href="#cookie-mapping" class="headerlink" title="cookie mapping"></a>cookie mapping</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/cookie_mapping.png" alt="图：cookie mapping"></p><h2 id="supply-side-platform-供应方平台"><a href="#supply-side-platform-供应方平台" class="headerlink" title="supply side platform-供应方平台"></a>supply side platform-供应方平台</h2><h4 id="媒体流量变现方式"><a href="#媒体流量变现方式" class="headerlink" title="媒体流量变现方式"></a>媒体流量变现方式</h4><ul><li>合约广告，与广告主签订合约进行投放（CPM 结算）</li><li>竞价广告，将广告位托管给广告网络，广告网络根据人群售卖给广告主（CPC 结算）</li><li>实时竞价（按展示结算）</li></ul><h4 id="关键特征-2"><a href="#关键特征-2" class="headerlink" title="关键特征"></a>关键特征</h4><ul><li>提供媒体端的用户划分和售卖能力</li><li>可以灵活接入多种变现方式</li><li>收益管理：统一网络优化和RTB，优化媒体利益</li></ul><h2 id="demand-side-platform-需求方平台（交易市场demand端技术）"><a href="#demand-side-platform-需求方平台（交易市场demand端技术）" class="headerlink" title="demand side platform-需求方平台（交易市场demand端技术）"></a>demand side platform-需求方平台（交易市场demand端技术）</h2><h4 id="关键特征："><a href="#关键特征：" class="headerlink" title="关键特征："></a>关键特征：</h4><ul><li>定制化用户划分</li><li>跨媒体流量采购</li><li>通过ROI估计来支持RTB</li></ul><h4 id="结构图"><a href="#结构图" class="headerlink" title="结构图"></a>结构图</h4><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/dsp.png" alt="图：dsp"></p><p><strong>难点：</strong>要估计 <strong>eCPM = CTR * clickValue</strong>，ADX不是对全部DSP询价，而是会预估每个DSP会不会出价，从而减轻询价成本（带宽成本）。</p><h2 id="DSP流量预测"><a href="#DSP流量预测" class="headerlink" title="DSP流量预测"></a>DSP流量预测</h2><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ul><li>DSP需要预测流量以决定采买策略</li><li>DSP拿到流量就是bid的函数，称为bid landscape</li></ul><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul><li>由于DSP<strong>无法拿到所有流量情况</strong>，因此无法像供给方ssp那样进行流量预测</li><li>如何利用历史投放数据仍然是关键</li></ul><h2 id="DSP点击价值估计"><a href="#DSP点击价值估计" class="headerlink" title="DSP点击价值估计"></a>DSP点击价值估计</h2><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li>DSP的实时出价</li><li>广告网络中的出价工具</li><li>智能定价</li></ul><h4 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h4><ul><li>非常稀疏的训练数据</li><li>与广告主类型强烈相关的行为模式</li></ul><h4 id="点击价值估计若干原则"><a href="#点击价值估计若干原则" class="headerlink" title="点击价值估计若干原则"></a>点击价值估计若干原则</h4><ul><li>模型估计时，用较大的bias换较小的variance，以达到稳健估计的目的</li><li>充分利用广告商类型的层级结构，以及转化流程上的特征</li></ul><h2 id="DSP重定向（retargeting）"><a href="#DSP重定向（retargeting）" class="headerlink" title="DSP重定向（retargeting）"></a>DSP重定向（retargeting）</h2><p><strong>重点：</strong></p><ul><li>当一个用户浏览过我们的产品或官网，但是他没有形成转化，但之后他去浏览其他媒体的时候，我们通过对他投放广告则可以加大其转化可能</li></ul><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><ul><li>网站重定向</li><li>搜索重定向</li><li>个性化重定向<ol><li>根据用户关注或购买，推送商品粒度的广告</li><li>已购买的，推荐相关产品</li><li>站外推荐</li></ol></li></ul><h4 id="推荐算法"><a href="#推荐算法" class="headerlink" title="推荐算法"></a>推荐算法</h4><ul><li>协同过滤算法<ol><li>内存方法或非参数方法：近邻、Item-based/user-based</li><li>模型方法或参数方法：矩阵分解、贝叶斯</li></ol></li><li>基于内容算法</li><li>SVD++算法</li></ul><h4 id="新客推荐-lookalike"><a href="#新客推荐-lookalike" class="headerlink" title="新客推荐-lookalike"></a>新客推荐-lookalike</h4><ul><li>由广告商提供一部分种子用户，DSP通过网络行为的相似性来找到潜在用户</li><li>是一种广告商自定义标签，可以视为扩展的重定向</li><li>在同样reach水平下，效果好于通用标签</li><li>尽量利用非demand数据，注意避免在竞争对手之间倒卖用户</li></ul><h2 id="需求端推荐方法"><a href="#需求端推荐方法" class="headerlink" title="需求端推荐方法"></a>需求端推荐方法</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/recommend.png" alt="图：recommend"></p><h2 id="广告流量交易方式"><a href="#广告流量交易方式" class="headerlink" title="广告流量交易方式"></a>广告流量交易方式</h2><p><img src="/2019/11/22/ji-suan-guang-gao-zhi-li-lun/trade.png" alt="图：trade"></p><p><strong>优先销售：</strong></p><ul><li>CPT结算</li><li>GD（担保式投放）：CPM 结算 + 人群定向</li></ul><p><strong>程序交易：</strong></p><ul><li>竞价广告（Ad network）</li><li><p>实时竞价广告（Adx）</p></li><li><p>DSP：network optimization + RTBD</p></li><li>SSP：portfolio selection + RTBS</li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算广告 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>职场通用技能之职业规划</title>
      <link href="/2019/11/21/zhi-chang-tong-yong-ji-neng-zhi-zhi-ye-gui-hua/"/>
      <url>/2019/11/21/zhi-chang-tong-yong-ji-neng-zhi-zhi-ye-gui-hua/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/21/zhi-chang-tong-yong-ji-neng-zhi-zhi-ye-gui-hua/image-20210621144229801.png" alt="image-20210621144229801"></p><h1 id="一、笔记"><a href="#一、笔记" class="headerlink" title="一、笔记"></a>一、笔记</h1><h2 id="1、职业与职业生涯"><a href="#1、职业与职业生涯" class="headerlink" title="1、职业与职业生涯"></a>1、职业与职业生涯</h2><h3 id="（1）我们如何对待职业，职业也就如何对待我们"><a href="#（1）我们如何对待职业，职业也就如何对待我们" class="headerlink" title="（1）我们如何对待职业，职业也就如何对待我们"></a>（1）我们如何对待职业，职业也就如何对待我们</h3><ul><li>职业它是付出与回报的关系，你付出多少，它就回报多少</li></ul><h3 id="（2）什么是职业生涯，职业生涯分类"><a href="#（2）什么是职业生涯，职业生涯分类" class="headerlink" title="（2）什么是职业生涯，职业生涯分类"></a>（2）什么是职业生涯，职业生涯分类</h3><ul><li>职业生涯就是一个人的职业经历</li><li>职业生涯分为外职业生涯（别人给予和决定的经历）和内职业生涯（自己给自己的经历）</li><li>内职业生涯的发展是外职业生涯发展的前提，外职业生涯促进内职业生涯的发展</li></ul><h3 id="（3）学会为自己工作可以提高内职业生涯"><a href="#（3）学会为自己工作可以提高内职业生涯" class="headerlink" title="（3）学会为自己工作可以提高内职业生涯"></a>（3）学会为自己工作可以提高内职业生涯</h3><ul><li>为自己工作：你的工作热情和程度不为其他条件而所动则是为自己工作，许三多就是为自己当兵</li></ul><h3 id="（4）积极的面对工作可以提高内职业生涯"><a href="#（4）积极的面对工作可以提高内职业生涯" class="headerlink" title="（4）积极的面对工作可以提高内职业生涯"></a>（4）积极的面对工作可以提高内职业生涯</h3><ul><li>积极面对工作：</li></ul><h2 id="2、职业生涯规划"><a href="#2、职业生涯规划" class="headerlink" title="2、职业生涯规划"></a>2、职业生涯规划</h2><h3 id="（1）职业生涯规划定义"><a href="#（1）职业生涯规划定义" class="headerlink" title="（1）职业生涯规划定义"></a>（1）职业生涯规划定义</h3><ul><li>根据主观和客观的方面去规划职业生涯</li></ul><h3 id="（2）误区"><a href="#（2）误区" class="headerlink" title="（2）误区"></a>（2）误区</h3><ul><li>局部思维</li><li>要考虑全局，整个行业，<strong>要想到未来的形势</strong></li><li>静态思维：应该是动态思维，随着时间动态发展的</li><li>从众思维：不要从众，切忌人云亦云</li></ul><h3 id="（3）影响职业生涯的因素"><a href="#（3）影响职业生涯的因素" class="headerlink" title="（3）影响职业生涯的因素"></a>（3）影响职业生涯的因素</h3><ul><li>习惯</li><li>情绪</li><li>兴趣</li><li>特长</li></ul><h2 id="3、制定职业生涯"><a href="#3、制定职业生涯" class="headerlink" title="3、制定职业生涯"></a>3、制定职业生涯</h2><h3 id="（1）明确目标"><a href="#（1）明确目标" class="headerlink" title="（1）明确目标"></a>（1）明确目标</h3><ul><li>目标导向、以始为终</li><li>文字记载、随时提醒</li></ul><h3 id="（2）详细计划"><a href="#（2）详细计划" class="headerlink" title="（2）详细计划"></a>（2）详细计划</h3><ul><li>性格分析：感性还是理性</li><li>职业生涯路径分析：决定自己是走哪条路，创业还是兢兢业业</li><li>自我swot分析：扬长避短</li><li>时间分解和自我反思：自己缺什么赶快补什么</li><li>提高学习力的细分模型：总是忆往昔，说明你还在原地踏步</li><li>付诸行动和做出结果：一定要做，不做一定不会成功</li></ul><h1 id="二、参考课程"><a href="#二、参考课程" class="headerlink" title="二、参考课程"></a>二、参考课程</h1><ul><li><a href="https://study.163.com/course/courseMain.htm?courseId=1003599028" target="_blank" rel="noopener">职业生涯规划</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 职场通用技能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 职业规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>职场通用技能之职业形象</title>
      <link href="/2019/11/20/zhi-chang-tong-yong-ji-neng-zhi-zhi-ye-xing-xiang/"/>
      <url>/2019/11/20/zhi-chang-tong-yong-ji-neng-zhi-zhi-ye-xing-xiang/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/20/zhi-chang-tong-yong-ji-neng-zhi-zhi-ye-xing-xiang/image-20210621144208497.png" alt="image-20210621144208497"></p><h1 id="一、笔记"><a href="#一、笔记" class="headerlink" title="一、笔记"></a>一、笔记</h1><h2 id="1、形象管理概论"><a href="#1、形象管理概论" class="headerlink" title="1、形象管理概论"></a>1、形象管理概论</h2><h3 id="（1）形象的定义"><a href="#（1）形象的定义" class="headerlink" title="（1）形象的定义"></a>（1）形象的定义</h3><ul><li>他人眼中的自己</li></ul><h3 id="（2）形象的形成变数"><a href="#（2）形象的形成变数" class="headerlink" title="（2）形象的形成变数"></a>（2）形象的形成变数</h3><ul><li>性别</li><li>年龄</li><li>教育程度</li><li>职业和工作性质</li><li>工作和居住地缘</li><li>文化背景</li></ul><h3 id="（3）形象管理的三个步骤"><a href="#（3）形象管理的三个步骤" class="headerlink" title="（3）形象管理的三个步骤"></a>（3）形象管理的三个步骤</h3><ul><li>内在</li><li>表达：文字、语言、视觉、行为</li><li>公关：交际</li></ul><h2 id="2、服仪管理"><a href="#2、服仪管理" class="headerlink" title="2、服仪管理"></a>2、服仪管理</h2><h3 id="（1）装扮的三个层面"><a href="#（1）装扮的三个层面" class="headerlink" title="（1）装扮的三个层面"></a>（1）装扮的三个层面</h3><ul><li>心理</li><li>生理</li><li>社会</li></ul><h3 id="（2）主流审美观"><a href="#（2）主流审美观" class="headerlink" title="（2）主流审美观"></a>（2）主流审美观</h3><ul><li>形象被一半以上人认同，就是主流审美观</li></ul><h3 id="（3）社会期望值"><a href="#（3）社会期望值" class="headerlink" title="（3）社会期望值"></a>（3）社会期望值</h3><ul><li>形象风险，什么行业就穿符合该行业的服装</li><li>适合工作的服装，和不适合工作的服装</li></ul><h3 id="（4）穿着的TPO"><a href="#（4）穿着的TPO" class="headerlink" title="（4）穿着的TPO"></a>（4）穿着的TPO</h3><ul><li>时间</li><li>地点</li><li>场合</li></ul><h3 id="（5）男士服装风格分析"><a href="#（5）男士服装风格分析" class="headerlink" title="（5）男士服装风格分析"></a>（5）男士服装风格分析</h3><ul><li>休闲服</li><li>商务装</li><li>商务休闲装</li></ul><h3 id="（6）女士服装风格分析"><a href="#（6）女士服装风格分析" class="headerlink" title="（6）女士服装风格分析"></a>（6）女士服装风格分析</h3><h3 id="（7）身材分析"><a href="#（7）身材分析" class="headerlink" title="（7）身材分析"></a>（7）身材分析</h3><ul><li>比例：头身，趴着睡和斜着睡，躺着睡会让脸横向发展</li><li>曲线（女生）</li></ul><h3 id="（8）以服装修饰身材"><a href="#（8）以服装修饰身材" class="headerlink" title="（8）以服装修饰身材"></a>（8）以服装修饰身材</h3><ul><li><a href="https://www.zhihu.com/question/20872048" target="_blank" rel="noopener">知乎回答</a></li></ul><h3 id="（9）色彩搭配原理"><a href="#（9）色彩搭配原理" class="headerlink" title="（9）色彩搭配原理"></a>（9）色彩搭配原理</h3><ul><li><a href="https://zh.wikihow.com/搭配衣服颜色" target="_blank" rel="noopener">wikihow</a></li></ul><h2 id="3、仪态美学"><a href="#3、仪态美学" class="headerlink" title="3、仪态美学"></a>3、仪态美学</h2><h3 id="（1）基本站姿"><a href="#（1）基本站姿" class="headerlink" title="（1）基本站姿"></a>（1）基本站姿</h3><h3 id="（2）优雅坐姿"><a href="#（2）优雅坐姿" class="headerlink" title="（2）优雅坐姿"></a>（2）优雅坐姿</h3><h2 id="4、行为形象"><a href="#4、行为形象" class="headerlink" title="4、行为形象"></a>4、行为形象</h2><h3 id="（1）礼貌问题"><a href="#（1）礼貌问题" class="headerlink" title="（1）礼貌问题"></a>（1）礼貌问题</h3><ul><li>知识</li><li>常识</li><li>同理心（基础）</li></ul><h3 id="（2）个人隐私"><a href="#（2）个人隐私" class="headerlink" title="（2）个人隐私"></a>（2）个人隐私</h3><ul><li>身高</li><li>年龄</li></ul><h1 id="二、参考课程信息"><a href="#二、参考课程信息" class="headerlink" title="二、参考课程信息"></a>二、参考课程信息</h1><ul><li><a href="https://study.163.com/course/courseMain.htm?courseId=680003" target="_blank" rel="noopener">美丽百分百-个人形象管理</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 职场通用技能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 职业形象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之sql</title>
      <link href="/2019/11/19/shi-yong-gong-ju-zhi-sql/"/>
      <url>/2019/11/19/shi-yong-gong-ju-zhi-sql/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/19/shi-yong-gong-ju-zhi-sql/image-20210621144823221.png" alt="image-20210621144823221"></p><h1 id="一、Sql基础"><a href="#一、Sql基础" class="headerlink" title="一、Sql基础"></a>一、Sql基础</h1><p><a href="llljl.md">git</a></p>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之MySQL</title>
      <link href="/2019/11/18/shi-yong-gong-ju-zhi-mysql/"/>
      <url>/2019/11/18/shi-yong-gong-ju-zhi-mysql/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/18/shi-yong-gong-ju-zhi-mysql/image-20210621145055259.png" alt="image-20210621145055259"></p><h1 id="一、MySql基础"><a href="#一、MySql基础" class="headerlink" title="一、MySql基础"></a>一、MySql基础</h1><h2 id="1、MySql简介"><a href="#1、MySql简介" class="headerlink" title="1、MySql简介"></a>1、MySql简介</h2><h3 id="（1）什么是MySql？"><a href="#（1）什么是MySql？" class="headerlink" title="（1）什么是MySql？"></a>（1）什么是MySql？</h3><ul><li>MySql软件提供了一个非常快速、多线程、多用户和健壮的Sql数据库服务器（结构化查询）。MySql服务器适用于任务关键型、高负载的生产系统以及嵌入到大规模部署的软件中。</li></ul><h3 id="（2）安装"><a href="#（2）安装" class="headerlink" title="（2）安装"></a>（2）安装</h3><ul><li><a href="https://dev.mysql.com/doc/refman/8.0/en/installing.html" target="_blank" rel="noopener">参考官网</a></li></ul><h3 id="（3）启动"><a href="#（3）启动" class="headerlink" title="（3）启动"></a>（3）启动</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">mysql -h host -u user -p# Enter password: ********<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="（4）退出"><a href="#（4）退出" class="headerlink" title="（4）退出"></a>（4）退出</h3><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">QUIT<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2、"><a href="#2、" class="headerlink" title="2、"></a>2、</h2><h1 id="二、MySql进阶"><a href="#二、MySql进阶" class="headerlink" title="二、MySql进阶"></a>二、MySql进阶</h1><h1 id="三、参考书籍"><a href="#三、参考书籍" class="headerlink" title="三、参考书籍"></a>三、参考书籍</h1><ul><li><a href="https://dev.mysql.com/doc/refman/8.0/en/" target="_blank" rel="noopener">官方教程</a></li></ul><h1 id="四、疑难解答"><a href="#四、疑难解答" class="headerlink" title="四、疑难解答"></a>四、疑难解答</h1><h2 id="MySQL写出文件"><a href="#MySQL写出文件" class="headerlink" title="MySQL写出文件"></a>MySQL写出文件</h2><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">mysql -hhost -uuser -ppassword db -A --default-character-set=latin1 -e 'mysql' >> ~/dongtai.xls<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之git</title>
      <link href="/2019/11/12/shi-yong-gong-ju-zhi-git/"/>
      <url>/2019/11/12/shi-yong-gong-ju-zhi-git/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/image-20210621152942100.png" alt="image-20210621152942100"></p><h1 id="Git基础"><a href="#Git基础" class="headerlink" title="Git基础"></a>Git基础</h1><h2 id="1、Git简介"><a href="#1、Git简介" class="headerlink" title="1、Git简介"></a>1、Git简介</h2><h3 id="（1）什么是Git？"><a href="#（1）什么是Git？" class="headerlink" title="（1）什么是Git？"></a>（1）什么是Git？</h3><ul><li>Git是迄今为止最先进的分布式版本控制系统</li></ul><h3 id="（2）Git安装"><a href="#（2）Git安装" class="headerlink" title="（2）Git安装"></a>（2）Git安装</h3><ul><li><a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git" target="_blank" rel="noopener">安装教程</a></li></ul><h3 id="（3）Git设置"><a href="#（3）Git设置" class="headerlink" title="（3）Git设置"></a>（3）Git设置</h3><ol><li>checking your settings</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git config --list  # 查看git的所有配置<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（4）GitLab和GitHub一起使用"><a href="#（4）GitLab和GitHub一起使用" class="headerlink" title="（4）GitLab和GitHub一起使用"></a>（4）GitLab和GitHub一起使用</h3><ul><li><p>通常公司是使用GitLab，而个人是使用GitHub。那么问题来了：在一台电脑上同时使用GitLab和GitHub应该如何配置？</p><p>操作步骤如下：</p></li></ul><ol><li>生成公钥、密钥</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># GitLabssh-keygen -t rsa -C "注册的GitLab邮箱"  # 公钥、密钥名输入gitlab_id_rsa，其他一律回车# GitHubssh-keygen -t rsa -C "注册的GitHub邮箱"  # 公钥、密钥名输入github_id_rsa，其他一律回车<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>备注：</strong></p><ol><li>上述代码完成后会在<code>~/.ssh/</code>目录生成以下文件：github_id_rsa、github_id_rsa.pub、gitlab_id_rsa、gitlab_id_rsa.pub</li><li>将github_id_rsa.pub的内容配置到GitHub网站的sshkey中，将gitlab_id_rsa.pub的内容配置到GitLab网站的sshkey中</li><li>在<code>~/.ssh/</code>目录下创建config文件，告诉git不同平台使用不同key</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">cd ~/.ssh  # cd 到key目录vi config  # 创建并编辑config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># config内容如下# gitlabHost gitlab.yourcompany.comHostName gitlab.yourcompany.comUser gitPort yourportPreferredAuthentications publickeyIdentityFile ~/.ssh/gitlab_id_rsa# githubHost github.comHostName github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/github_id_rsa<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>备注：</strong></p><ul><li>Host是别名，建议与HostName名字一致！</li><li>把工作用的GitLab的<code>git config</code>配置成global</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">cd ~/workspace/gitlab  # gitlab的工作仓库git initgit config --global user.name 'personal'git config --global user.email 'personal@company.com'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>把个人用的GitHub的<code>git config</code>配置成local</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">cd ~/workspace/github  # github的工作仓库git initgit config --local user.name 'yourname'git config --local user.email 'youremail'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>user.eamil</code>建议使用网站提供的<strong>加密邮箱</strong>，例如GitHub的加密邮箱可以从GitHub网站的个人setting中的Emails栏目中找到。如下：</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">*@users.noreply.github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2、创建版本库"><a href="#2、创建版本库" class="headerlink" title="2、创建版本库"></a>2、创建版本库</h2><h3 id="（1）将已有文件夹变为版本库"><a href="#（1）将已有文件夹变为版本库" class="headerlink" title="（1）将已有文件夹变为版本库"></a>（1）将已有文件夹变为版本库</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">cd /home/user/my_project  # 切换到想要初始化的文件夹git init  # 初始化为版本库，文件夹会出现.git的隐藏文件夹git remote add origin ssh://git@github.com:myhaa/How-To-Ask-Questions-The-Smart-Way.gitgit add .  # 添加文件夹中所有文件到暂存区git commit -m "your commit description"  # 提交暂存区所有文件到版本库并保存提交记录<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（2）从远处仓库（GitLab-GitHub）克隆"><a href="#（2）从远处仓库（GitLab-GitHub）克隆" class="headerlink" title="（2）从远处仓库（GitLab\GitHub）克隆"></a>（2）从远处仓库（GitLab\GitHub）克隆</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">cd /home/user/my_project  # 切换到想要存放版本库的文件夹git clone https://github.com/myhaa/How-To-Ask-Questions-The-Smart-Way.git  # clone https地址git clone ssh://git@github.com:myhaa/How-To-Ask-Questions-The-Smart-Way.git  # clone ssh地址cd How-To-Ask-Questions-The-Smart-Way  # 进入clone的版本库文件夹<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、操作版本库"><a href="#3、操作版本库" class="headerlink" title="3、操作版本库"></a>3、操作版本库</h2><h3 id="（1）版本库中文件的两种状态"><a href="#（1）版本库中文件的两种状态" class="headerlink" title="（1）版本库中文件的两种状态"></a>（1）版本库中文件的两种状态</h3><ol><li>未追踪状态（<em>untracked</em>）：从未<strong>add+commit</strong>的文件（Untracked files:）</li><li>追踪状态（<em>tracked</em>）：曾经<strong>add+commit</strong>过的文件<ul><li>未修改（unmodified）：在版本库中</li><li>已修改（modified）：Changes not staged for commit:</li><li>暂存（staged）：Changes to be committed:</li></ul></li><li>详情如下图（图来自：<a href="https://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository" target="_blank" rel="noopener">https://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository</a>）</li></ol><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/lifecycle.png" alt="图1：版本库中的文件状态"></p><h3 id="（2）Tracking-New-Files-untracked"><a href="#（2）Tracking-New-Files-untracked" class="headerlink" title="（2）Tracking New Files(untracked)"></a>（2）Tracking New Files(untracked)</h3><ol><li>查看版本库状态发现README文件是<code>Untracked file</code>。</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Untracked files:  (use "git add <file>..." to include in what will be committed)    READMEnothing added to commit but untracked files present (use "git add" to track)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>将README添加到暂存区并提交到版本库</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git add README  # 添加到暂存区（staged）发现README状态为（Changes to be committed: new file）git commit -m "add README"  # 提交到版本库<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="（3）Staging-Modified-Files-tracked"><a href="#（3）Staging-Modified-Files-tracked" class="headerlink" title="（3）Staging Modified Files(tracked)"></a>（3）Staging Modified Files(tracked)</h3><ol><li>修改刚刚提交到版本库的README文件</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">vi README  # 用vim修改README文件git status  # 查看文件状态发现README的状态为（Changes not staged for commit: modified）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>将README添加到暂存区并提交到版本库</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git add README  # 添加到暂存区（staged）git commit -m "update README"  # 提交到版本库<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="（4）Viewing-Your-Staged-and-Unstaged-Changes"><a href="#（4）Viewing-Your-Staged-and-Unstaged-Changes" class="headerlink" title="（4）Viewing Your Staged and Unstaged Changes"></a>（4）Viewing Your Staged and Unstaged Changes</h3><ol><li>比较<code>Changes not staged for commit:</code>下的文件与本地最新版本库的差别：</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git diff<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>比较<code>Changes to be committed:</code>下的文件与本地最新版本库的差别：</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git diff --stagedgit diff --cached<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>比较<code>Changes not staged for commit:</code>和<code>Changes to be committed:</code>下同名文件的差别</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git diff<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（5）配置忽略文件（Ignoring-Files）"><a href="#（5）配置忽略文件（Ignoring-Files）" class="headerlink" title="（5）配置忽略文件（Ignoring Files）"></a>（5）配置忽略文件（Ignoring Files）</h3><ul><li><p>通常会有一类文件是你不希望Git自动添加或显示为未跟踪的文件，例如日志文件或构建系统生成的文件。 这种情况下可以创建忽略文件<code>.gitignore</code>来避免。 </p></li><li><p>这是一个示例.gitignore文件：</p></li></ul><pre class="line-numbers language-lang-txt"><code class="language-lang-txt"># ignore all .a files*.a# but do track lib.a, even though you're ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in any directory named buildbuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directory and any of its subdirectoriesdoc/**/*.pdf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（6）Removing-and-Moving-files"><a href="#（6）Removing-and-Moving-files" class="headerlink" title="（6）Removing and Moving files"></a>（6）Removing and Moving files</h3><ol><li>short status</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git status -s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>skipping the staging area</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git commit -a -m 'added new benchmarks'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>removing files</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 第一种情况：手动删除或使用rm命令rm PROJECTS.mdgit add PROJECTS.mdgit commit "rm PROJECTS.md"# 第二种情况：使用git rm命令删除git rm PROJECTS.mdgit commit "git rm PROJECTS.md"# 第三种情况：删除Changes to be committed:或者Changes not staged for commit:下显示的文件git rm -f PROJECTS.mdgit commit "git rm -f PROJECTS.md"# 第四种情况：您可能想要做的是将文件保留在工作树中，但将其从暂存区中删除。换句话说，您可能希望将文件保留在硬盘上，但不再需要Git对其进行跟踪。git rm --cached PROJECTS.mdgit commit "git rm --cached PROJECTS.md"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>moving files</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git mv file_from file_to  # 重命名file_from为file_togit commit -m "rename file_from"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="（7）查看版本库提交历史"><a href="#（7）查看版本库提交历史" class="headerlink" title="（7）查看版本库提交历史"></a>（7）查看版本库提交历史</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git log  # 查看历史git log -p -2  # 查看最新2个commit的历史（包含git diff结果）git log --stat  # 查看提交历史的一些简短统计信息git log --pretty=oneline  # 每个commit用一行输出git log --pretty=format:"%h - %an, %ar : %s"  # 按指定格式输出git log --pretty=format:"%h %s" --graph  # 图形化git log --since=2.weeks  # 过去2周的提交历史git log --pretty="%h - %s" --author='Junio C Hamano' --since="2008-10-01" --before="2008-11-01" --no-merges -- t/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>注意：</strong>更多详情请见<a href="https://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-History" target="_blank" rel="noopener">git log</a></li></ul><h3 id="（8）回退操作（undoing-things）"><a href="#（8）回退操作（undoing-things）" class="headerlink" title="（8）回退操作（undoing things）"></a>（8）回退操作（undoing things）</h3><ol><li>当你commit后发现此次commit的message出现错误或者忘记add一些文件时：</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git commit --amend  # 修改commit信息，ctrl+o保存，回车，ctrl+x退出<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>unstaging a staged file(Changes to be committed:)</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git reset HEAD CONTRIBUTING.md  # 将暂存区的某一个文件退回到工作区<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>unmodifying a modified file(Changes not staged for commit:)</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git checkout -- CONTRIBUTING.md  # 撤销对某文件的修改<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><strong>注意：</strong>回退操作异常危险，谨慎使用！！！</li></ul><h3 id="（9）远程仓库操作"><a href="#（9）远程仓库操作" class="headerlink" title="（9）远程仓库操作"></a>（9）远程仓库操作</h3><ol><li>showing your remotes</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git remotegit remote -v  # 详细信息git remote show origin  # 详细信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol><li>添加远程仓库</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git remote add <shortname> <url>  # 通用代码git remote add pb https://github.com/paulboone/ticgit  # pb 是给该远程仓库设定的别名<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>从远程获取最新版本到本地但不自动merge</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git fetch <remote>  # 通用代码git fetch origingit fetch pb<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol><li>从远程获取最新版本到本地并自动merge</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git pull <remote>  # 通用代码git pull origingit pull pb<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol><li>将本地库推送远程仓库</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git push <remote> <branch>  # 通用代码git push origin master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>将远程仓库重命名</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git remote rename pb paul  # 将pb重命名为paul<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>移除某个远程仓库</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git remote remove paul  # 移除paul这个远程仓库<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（10）标签操作"><a href="#（10）标签操作" class="headerlink" title="（10）标签操作"></a>（10）标签操作</h3><ol><li>列出所有标签</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git taggit tag -l "v1.8.5*"  # 列出浅醉是v1.8.5的标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>创建带注释的标签（Annotated Tags）</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git tag -a v1.4 -m "my version 1.4"git show v1.4  # 显示这个标签对应的commit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>创建轻量级标签（Lightweight Tags）</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git tag v1.4-lwgit show v1.4-lw  # 只有commit信息，没有tag信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>补标签</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git log --pretty=oneline  # 想为某次commit补上标签git tag -a v1.2 9fceb02  # 9fceb02 是commit-id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>分享标签到远程</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git push origin v1.5  # push某一个git push origin --tags  # push全部tags<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>删除标签</li></ol><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 第一种方法git tag -d v1.4-lwgit push origin :refs/tags/v1.4-lw# 第二种方法git push origin --delete <tagname><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Git进阶-分支"><a href="#Git进阶-分支" class="headerlink" title="Git进阶-分支"></a>Git进阶-分支</h1><h2 id="1、-分支简介"><a href="#1、-分支简介" class="headerlink" title="1、 分支简介"></a>1、 分支简介</h2><h3 id="（1）什么是分支？"><a href="#（1）什么是分支？" class="headerlink" title="（1）什么是分支？"></a>（1）什么是分支？</h3><ul><li>如下图，master是默认分支，testing是其他分支。</li><li>分支的存在是让你可以把master作为正式环境，把testing作为开发环境，当开发环境的代码需要上线时就将其合并到正式环境master。</li></ul><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/branch.png" alt="图2：分支"></p><h3 id="（2）默认分支"><a href="#（2）默认分支" class="headerlink" title="（2）默认分支"></a>（2）默认分支</h3><ul><li>master</li></ul><h3 id="（3）分支详细说明"><a href="#（3）分支详细说明" class="headerlink" title="（3）分支详细说明"></a>（3）分支详细说明</h3><ul><li><a href="https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell" target="_blank" rel="noopener">参考官网</a></li></ul><h2 id="2、本地分支操作"><a href="#2、本地分支操作" class="headerlink" title="2、本地分支操作"></a>2、本地分支操作</h2><h3 id="（1）创建分支"><a href="#（1）创建分支" class="headerlink" title="（1）创建分支"></a>（1）创建分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git branch test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（2）切换到该分支"><a href="#（2）切换到该分支" class="headerlink" title="（2）切换到该分支"></a>（2）切换到该分支</h3><ul><li>当分支还存在文件没有add+commit的时候，是没法进行分支切换的。</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git checkout test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（3）创建并切换分支一步到位"><a href="#（3）创建并切换分支一步到位" class="headerlink" title="（3）创建并切换分支一步到位"></a>（3）创建并切换分支一步到位</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git checkout -b test<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（4）查看分支"><a href="#（4）查看分支" class="headerlink" title="（4）查看分支"></a>（4）查看分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git branch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（5）合并分支"><a href="#（5）合并分支" class="headerlink" title="（5）合并分支"></a>（5）合并分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git merge test  # 在master分支上将test分支合并过来git merge master  # 在test分支上将master分支合并过来<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="（6）分支冲突解决"><a href="#（6）分支冲突解决" class="headerlink" title="（6）分支冲突解决"></a>（6）分支冲突解决</h3><ul><li>当两个分支的同一文件都被修改提交时，这时合并两个分支就会出现冲突，那么解决冲突的办法就是手动修改两个文件，使其一致。</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git mergetool  # 查看合并冲突<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（7）删除分支"><a href="#（7）删除分支" class="headerlink" title="（7）删除分支"></a>（7）删除分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git branch -d test  # 在master分支上删除test分支<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（8）查看被合并过的分支"><a href="#（8）查看被合并过的分支" class="headerlink" title="（8）查看被合并过的分支"></a>（8）查看被合并过的分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git branch --merged<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（9）查看未被合并过的分支"><a href="#（9）查看未被合并过的分支" class="headerlink" title="（9）查看未被合并过的分支"></a>（9）查看未被合并过的分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git branch --no-merged<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="3、远程分支操作"><a href="#3、远程分支操作" class="headerlink" title="3、远程分支操作"></a>3、远程分支操作</h2><h3 id="（1）克隆远程仓库"><a href="#（1）克隆远程仓库" class="headerlink" title="（1）克隆远程仓库"></a>（1）克隆远程仓库</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/remote_branch.png" alt="图3：克隆后的远程仓库和本地仓库"></p><ul><li>图片来源于<a href="https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches" target="_blank" rel="noopener">官网</a></li></ul><h3 id="（2）当远程仓库被修改后"><a href="#（2）当远程仓库被修改后" class="headerlink" title="（2）当远程仓库被修改后"></a>（2）当远程仓库被修改后</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/remote_branch1.png" alt="图4：被别人修改后的远程仓库和本地仓库"></p><ul><li>图片来源于<a href="https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches" target="_blank" rel="noopener">官网</a></li></ul><h3 id="（3）git-fetch-origin后的远程仓库和本地仓库"><a href="#（3）git-fetch-origin后的远程仓库和本地仓库" class="headerlink" title="（3）git fetch origin后的远程仓库和本地仓库"></a>（3）git fetch origin后的远程仓库和本地仓库</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/remote_branch2.png" alt="图5：fetch过后的仓库"></p><ul><li>图片来源于<a href="https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches" target="_blank" rel="noopener">官网</a></li></ul><h3 id="（4）多个远程仓库"><a href="#（4）多个远程仓库" class="headerlink" title="（4）多个远程仓库"></a>（4）多个远程仓库</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/remote_branch3.png" alt="图6：多个远程仓库"></p><ul><li>图片来源于<a href="https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches" target="_blank" rel="noopener">官网</a></li></ul><h3 id="（5）push分支到远程"><a href="#（5）push分支到远程" class="headerlink" title="（5）push分支到远程"></a>（5）push分支到远程</h3><ul><li>只推送你想跟别人共享的分支deploy</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git push origin deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（6）在本地仓库建立一个新分支并与远程仓库对应"><a href="#（6）在本地仓库建立一个新分支并与远程仓库对应" class="headerlink" title="（6）在本地仓库建立一个新分支并与远程仓库对应"></a>（6）在本地仓库建立一个新分支并与远程仓库对应</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git checkout -b serverfix origin/serverfixgit checkout -b sf origin/serverfixgit checkout --track origin/serverfix<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="（7）查看本地分支跟踪的是哪个远程分支及具体信息"><a href="#（7）查看本地分支跟踪的是哪个远程分支及具体信息" class="headerlink" title="（7）查看本地分支跟踪的是哪个远程分支及具体信息"></a>（7）查看本地分支跟踪的是哪个远程分支及具体信息</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 如果想查看远程最新信息则先用fetch命令# git fetch --all  git branch -vv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="（8）删除远程分支"><a href="#（8）删除远程分支" class="headerlink" title="（8）删除远程分支"></a>（8）删除远程分支</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git push origin --delete serverfix<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>注意：</strong>Basically all this does is remove the pointer from the server. The Git server will generally keep the data there for a while until a garbage collection runs, so if it was accidentally deleted, it’s often easy to recover.</p><h2 id="4、Rebasing"><a href="#4、Rebasing" class="headerlink" title="4、Rebasing"></a>4、Rebasing</h2><h3 id="（1）什么是Rebasing？"><a href="#（1）什么是Rebasing？" class="headerlink" title="（1）什么是Rebasing？"></a>（1）什么是Rebasing？</h3><ul><li>In Git, there are two main ways to integrate changes from one branch into another: the <code>merge</code> and the <code>rebase</code>. </li></ul><h3 id="（2）merge"><a href="#（2）merge" class="headerlink" title="（2）merge"></a>（2）merge</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/merge.png" alt="图7：merge"></p><h3 id="（3）Rebasing"><a href="#（3）Rebasing" class="headerlink" title="（3）Rebasing"></a>（3）Rebasing</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/rebasing.png" alt="图8：rebasing"></p><ul><li>如上图，rebasing相当于是把c4这个commit_id抹去，这样会导致这个提交信息被清除，<strong>有利也有弊</strong>。</li></ul><p><strong>注意：</strong></p><ul><li>Do not rebase commits that exist outside your repository and people may have based work on them.</li><li>即不要rebase别人的提交信息，导致混淆产生。</li></ul><h3 id="（4）merge-vs-rebasing"><a href="#（4）merge-vs-rebasing" class="headerlink" title="（4）merge vs rebasing"></a>（4）merge vs rebasing</h3><ul><li>个人还是觉得merge好，但是如果合并历史太多，可以酌情使用rebasing</li></ul><h1 id="Git进阶-Git-Flow"><a href="#Git进阶-Git-Flow" class="headerlink" title="Git进阶-Git Flow"></a>Git进阶-Git Flow</h1><h2 id="Git-Flow简介"><a href="#Git-Flow简介" class="headerlink" title="Git Flow简介"></a>Git Flow简介</h2><h3 id="Git-Flow是什么？"><a href="#Git-Flow是什么？" class="headerlink" title="Git Flow是什么？"></a>Git Flow是什么？</h3><ul><li>团队合作开发时，主分支、开发分支、功能分支、bug分支等等必不可少。那么问题来了，<strong>多人多分支的情况应该如何制定一个工作流程才不产生冲突呢？</strong>答案是：<ol><li>Git Flow</li><li>GitHub Flow</li><li>GitLab Flow</li></ol></li><li>Git Flow让我们更方便的管理一个项目</li></ul><h3 id="Git-Flow安装"><a href="#Git-Flow安装" class="headerlink" title="Git Flow安装"></a>Git Flow安装</h3><ul><li><p>For Windows users, <a href="https://github.com/petervanderdoes/gitflow-avh/wiki/Installing-on-Windows#git-for-windows" target="_blank" rel="noopener">Git for Windows</a> is the recommended method.</p><h2 id="Git-for-Windows"><a href="#Git-for-Windows" class="headerlink" title="Git for Windows"></a>Git for Windows</h2><p>Follow the instructions on the <a href="https://git-for-windows.github.io/" target="_blank" rel="noopener">Git for Windows homepage</a> to install Git for Windows. As of Git for Windows 2.6.4, GitFlow (AVH edition) is included, so you’re all done.</p></li></ul><h2 id="Git-Flow操作"><a href="#Git-Flow操作" class="headerlink" title="Git Flow操作"></a>Git Flow操作</h2><h3 id="寻求帮助"><a href="#寻求帮助" class="headerlink" title="寻求帮助"></a>寻求帮助</h3><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow help  # 查看命令帮助<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="在仓库中设置git-flow"><a href="#在仓库中设置git-flow" class="headerlink" title="在仓库中设置git-flow"></a>在仓库中设置git-flow</h3><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow init<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>注意：</strong></p><ol><li>把项目切换到git-flow时，就用git-flow命令操作</li><li>普通的git命令和git-flow命令互不冲突</li><li>git-flow不会改变仓库</li><li>使用<code>git flow init</code>命令时，实际上只是在<strong>当前分支配置了一些命名规则</strong></li></ol><h3 id="分支的模式"><a href="#分支的模式" class="headerlink" title="分支的模式"></a>分支的模式</h3><h4 id="长期分支"><a href="#长期分支" class="headerlink" title="长期分支"></a>长期分支</h4><p>git-flow会预设两个主分支在仓库中，也就是说要同时维护两个分支</p><ul><li>master：正式环境代码，不能在这个分支上工作</li><li>develop： 开发分支，以该分支为基础分支来进行功能开发等等</li></ul><h4 id="短期分支"><a href="#短期分支" class="headerlink" title="短期分支"></a>短期分支</h4><p>短期分支只是临时存在，当开发完成则可以删除该分支</p><ul><li>feature：功能开发分支</li><li>hotfix：bug修复分支</li><li>release：版本发布分支</li></ul><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-git/git_flow_branch.png" alt="图：git-flow分支模式"></p><h3 id="功能开发分支-feature"><a href="#功能开发分支-feature" class="headerlink" title="功能开发分支-feature"></a>功能开发分支-feature</h3><p>feature分支基于develop分支</p><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow feature start concat  # 新增留言功能<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><code>git flow feature start new_feature</code>命令会在develop分支上创建一个new_feature分支，然后就可以在该分支上进行功能开发</li><li>当完成功能开发后，合并与删除分支命令如下</li></ul><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow feature finish concat  # 完成功能开发<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><code>git flow feature finsih concat</code>命令会把concat分支开发的内容整合到develop分支中，并把concat分支删除</li></ul><h3 id="版本管理-release"><a href="#版本管理-release" class="headerlink" title="版本管理-release"></a>版本管理-release</h3><p>release分支基于develop分支</p><p>当develop分支的代码已经成熟时，就可以把<strong>这部分工作生成一个要发布的版本</strong>，使用如下命令生成版本号以及版本管理</p><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow release start 1.1.1  # 版本号1.1.1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>上述命令产生release分支后，再对该分支的一些文件记录版本号则可以使用以下命令进行版本发布</li></ul><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow release finish 1.1.1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><code>git flow release finish 1.1.1</code>命令会做如下操作<ol><li>拉取远程仓库，确保本地仓库是最新</li><li>release分支的内容会合并到master和develop分支上</li><li>删除release分支，并回到develop分支</li></ol></li></ul><h3 id="bug修复-hotfix"><a href="#bug修复-hotfix" class="headerlink" title="bug修复-hotfix"></a>bug修复-hotfix</h3><h4 id="创建hotfix"><a href="#创建hotfix" class="headerlink" title="创建hotfix"></a>创建hotfix</h4><p>hotfix分支是基于master分支的，对已发布的代码修复bug</p><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow hotfix start bug1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="完成hotfix分支"><a href="#完成hotfix分支" class="headerlink" title="完成hotfix分支"></a>完成hotfix分支</h4><pre class="line-numbers language-lang-gitbash"><code class="language-lang-gitbash">git flow hotfix finish bug1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><code>git flow hotfix finish bug1</code>内部操作如下<ol><li>完成的改动会合并到master和develop分支上</li><li>这个hotfix程序将被标记</li><li>hotfix分支被删除，然后回到develop分支</li><li><strong>然后利用release分支操作发布版本</strong></li></ol></li></ul><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul><li><a href="https://git-scm.com/book/en/v2" target="_blank" rel="noopener">官方教程</a></li><li><a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener">廖大神</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1><h2 id="清除历史提交中的敏感信息"><a href="#清除历史提交中的敏感信息" class="headerlink" title="清除历史提交中的敏感信息"></a>清除历史提交中的敏感信息</h2><h3 id="问题介绍"><a href="#问题介绍" class="headerlink" title="问题介绍"></a>问题介绍</h3><ul><li>某次提交把私人信息（密码或私钥等）提交到了远程仓库，该怎么办？</li></ul><h3 id="处理方式"><a href="#处理方式" class="headerlink" title="处理方式"></a>处理方式</h3><h4 id="第一种情况"><a href="#第一种情况" class="headerlink" title="第一种情况"></a>第一种情况</h4><ul><li>提交的敏感信息<strong>还没有推送到远程仓库</strong></li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 修改代码git commit --amend<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="第二种情况"><a href="#第二种情况" class="headerlink" title="第二种情况"></a>第二种情况</h4><ul><li>提交的敏感信息<strong>已经推送到远程仓库</strong></li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch your_want_to_rm_file_name' --prune-empty --tag-name-filter cat -- --allgit add .git commit -m "rm some files"git push origin --force --all<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><a href="https://stackoverflow.com/questions/872565/remove-sensitive-files-and-their-commits-from-git-history" target="_blank" rel="noopener">参考链接</a></li></ul><h2 id="2、git-commit-提交规范"><a href="#2、git-commit-提交规范" class="headerlink" title="2、git commit 提交规范"></a>2、git commit 提交规范</h2><p><a href="https://github.com/conventional-changelog/conventional-changelog/blob/v0.5.3/conventions/angular.md" target="_blank" rel="noopener">参考</a></p><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">* 模板如下：\<type>(\<scope>): \<subject>\<BLANK LINE>\<body>\<BLANK LINE>\<footer>* 说明1) typecommit类型，包括以下几种：1. feat: 新功能2. fix: 修复问题3. docs: 修改文档4. style: 修改代码格式，不影响代码逻辑5. refactor: 重构代码，理论上不影响现有功能6. perf: 提升性能7. test: 增加修改测试用例8. chore: 修改工具相关（包括但不限于文档、代码生成等）2) scope修改文件的范围3) subjectcommit的简短描述：用一句话描述这次提交做了什么4) bodysubject的详细说明5 ) footer当有Breaking Change时必须在这里描述清楚关闭issue或是链接到相关文档，如 Closes #1, Closes #2, #36 ) BLANK LINE 空一行7) IntelliJ IDEA 插件推荐使用：Git Commit Template<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之Hadoop</title>
      <link href="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/"/>
      <url>/2019/11/12/shi-yong-gong-ju-zhi-hadoop/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/image-20210621152803717.png" alt="image-20210621152803717"></p><h1 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h1><h2 id="第一部分-简介"><a href="#第一部分-简介" class="headerlink" title="第一部分-简介"></a>第一部分-简介</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>Hadoop是apache下的一个开源分布式计算平台</li><li>基于Java开发，跨平台特性好（其他编程语言也可以使用），可以部署再廉价的计算机集群中</li><li>核心是HDFS（分布式文件系统）和MapReduce</li></ul><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul><li>高可靠性：冗余副本和容错机制</li><li>高效性：把成百上千的服务器集中起来做分布式处理</li><li>高可扩展性：几个到上千个节点</li><li>高容错性：冗余副本和容错机制</li><li>成本低：整个机器集群可以是很多低端机</li><li>运行在Linux平台上：原生是Linux</li><li>支持多种编程语言：基于Java开发，但还可以多种语言开发</li></ul><h2 id="第二部分-结构"><a href="#第二部分-结构" class="headerlink" title="第二部分-结构"></a>第二部分-结构</h2><h3 id="企业应用架构"><a href="#企业应用架构" class="headerlink" title="企业应用架构"></a>企业应用架构</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/hadoop_structure.png" alt="图：Hadoop在企业应用结构"></p><h3 id="各种版本"><a href="#各种版本" class="headerlink" title="各种版本"></a>各种版本</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/hadoop_version.png" alt="图：Hadoop版本"></p><h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/hadoop_project_structure.jpg" alt="图：Hadoop项目结构"></p><ul><li>分布式存储系统-HDFS：怎么用成百上千的机器去存储文件，详情请见</li><li>分布式计算框架-YARN：负责计算资源调度，内存、CPU、带宽等等</li><li>离线计算-MapReduce：分布式并行编程模型，离线计算、批处理，基于磁盘，详情请见</li><li>DAG计算-Tez：运行在YARN之上的下一代Hadoop查询处理框架，将很多的MapReduce作业进行分析优化以后构建成一个有向无环图（DAG），保证获得最好的处理效率，详情请见</li><li>内存计算-Spark：类似MapReduce的通用并行框架，基于内存计算，性能比MapReduce高，详情请见</li><li>Hive：Hadoop上的数据仓库，把大量的历史数据保存到数据仓库中，建立各种维度，专门用于企业决策分析，支持SQL语句，会将SQL语句转化为MapReduce作业去执行，详情请见</li><li>Pig：基于Hadoop的大规模数据分析平台，提供类似SQL的查询语言Pig Latin，流数据处理，轻量级分析，用一两行语句就可以跟复杂的MapReduce语句得到一样的结果，详情请见</li><li>作业流调度系统-Oozie：把很多个工作环节进行调度，详情请见</li><li>分布式协调服务-ZooKeeper：提供分布式协调一致性服务，分布式锁，集群管理等操作，详情请见</li><li>分布式数据库-HBase：Hadoop上的非关系型分布式数据库，基于列的存储，随机读写，支持实时应用，详情请见</li><li>日志收集-Flume：高可用、高可靠、分布式的海量日志采集、聚合和传输的系统，详情请见</li><li>数据库TEL工具-Sqoop：用于在Hadoop与传统数据库之间的数据传递，完成数据导入导出，详情请见</li><li>安装部署工具-Ambari：快速部署工具，支持Hadoop集群的供应、管理和监控，详情请见</li></ul><h2 id="第三部分-本机安装"><a href="#第三部分-本机安装" class="headerlink" title="第三部分-本机安装"></a>第三部分-本机安装</h2><h3 id="apache-Hadoop安装与使用"><a href="#apache-Hadoop安装与使用" class="headerlink" title="apache Hadoop安装与使用"></a>apache Hadoop安装与使用</h3><ul><li><a href="http://dblab.xmu.edu.cn/blog/285/" target="_blank" rel="noopener">大数据技术原理与应用提供的安装教程</a></li></ul><h3 id="Hadoop三种shell命令的区别"><a href="#Hadoop三种shell命令的区别" class="headerlink" title="Hadoop三种shell命令的区别"></a>Hadoop三种shell命令的区别</h3><ul><li>hadoop fs 适用于任何不同的文件系统</li><li>hadoop dfs 只能适用于HDFS文件系统</li><li>hdfs dfs 只能适用于HDFS文件系统</li></ul><h2 id="第四部分-集群部署"><a href="#第四部分-集群部署" class="headerlink" title="第四部分-集群部署"></a>第四部分-集群部署</h2><h3 id="集群节点类型及硬件配置"><a href="#集群节点类型及硬件配置" class="headerlink" title="集群节点类型及硬件配置"></a>集群节点类型及硬件配置</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/hdfs_architecture.png" alt="图：HDFS结构"></p><ul><li>NameNode：负责协调集群中的数据存储，获得数据的地址信息，哪块在哪个机器这样子（HDFS节点）</li><li>DataNode：存储被拆分的数据块（HDFS节点）</li><li>JobTracker：协调数据计算任务（MapReduce节点）</li><li>TaskTracker：负责执行由JobTracker指派的任务（MapReduce节点）</li><li>SecondaryNameNode（冷备份）：帮助NameNode收集文件系统运行的状态信息（HDFS节点）</li></ul><p>集群规模可大可小，可以一步一步往上加机器。</p><p>在集群中，大部分的机器是作为DataNode和TaskTracker工作的，所以DataNode和TaskTracker的硬件规则可以采用以下方案：</p><ul><li>4个磁盘驱动器（单盘1-2T），支持JBOD（just a bunch of disks，磁盘簇）</li><li>2个4核CPU，至少2-2.5GHz</li><li>16-24GB内存</li><li>千兆以太网</li></ul><p>NameNode提供整个HDFS文件系统的命名空间管理、块管理等所有服务，所以需要更多的RAM，并且需要优化RAM的内存通道带宽，可以采用以下方案：</p><ul><li>8-12个磁盘驱动器（单盘1-2T）</li><li>2个4核/8核CPU</li><li>16-72GB内存</li><li>千兆/万兆以太网</li></ul><p>SecondaryNameNode在小型集群可以和NameNode共用一台机器，较大的集群可以采用与NameNode一样的硬件</p><h3 id="集群网络拓扑"><a href="#集群网络拓扑" class="headerlink" title="集群网络拓扑"></a>集群网络拓扑</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hadoop/network_tuo.png" alt="图：网络拓扑"></p><h3 id="集群建立与安装"><a href="#集群建立与安装" class="headerlink" title="集群建立与安装"></a>集群建立与安装</h3><ul><li>自动化部署：Docker等</li></ul><h3 id="集群基准测试"><a href="#集群基准测试" class="headerlink" title="集群基准测试"></a>集群基准测试</h3><ul><li>Hadoop自带一些基准测试程序</li></ul><h3 id="在云计算环境中实用Hadoop"><a href="#在云计算环境中实用Hadoop" class="headerlink" title="在云计算环境中实用Hadoop"></a>在云计算环境中实用Hadoop</h3><p>企业不需要自己部署集群，直接在云上部署</p><ul><li>可以在Amazon EC2中运行Hadoop</li></ul><h1 id="Hadoop进阶"><a href="#Hadoop进阶" class="headerlink" title="Hadoop进阶"></a>Hadoop进阶</h1><h1 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h1><ul><li><a href="https://study.163.com/course/courseMain.htm?courseId=1002887002" target="_blank" rel="noopener">大数据技术原理与应用</a></li></ul><h1 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h1>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之hexo</title>
      <link href="/2019/11/12/shi-yong-gong-ju-zhi-hexo/"/>
      <url>/2019/11/12/shi-yong-gong-ju-zhi-hexo/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/image-20210621151156147.png" alt="image-20210621151156147"></p><h1 id="一、Hexo介绍"><a href="#一、Hexo介绍" class="headerlink" title="一、Hexo介绍"></a>一、Hexo介绍</h1><h2 id="1、官网"><a href="#1、官网" class="headerlink" title="1、官网"></a>1、官网</h2><p><a href="https://hexo.io/zh-cn/index.html" target="_blank" rel="noopener">Hexo</a></p><h2 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h2><h3 id="（1）Node-js（Windows）"><a href="#（1）Node-js（Windows）" class="headerlink" title="（1）Node.js（Windows）"></a>（1）Node.js（Windows）</h3><ol><li><p><a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">官网</a>下载对应版本</p></li><li><p>安装选择全部默认（也可以自己设定安装位置）</p></li><li><p>安装完，键盘上<code>Win+R</code>打开命令行，输入以下命令出现版本号即安装成功</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">node -vnpm -v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h3 id="（2）Git（Windows）"><a href="#（2）Git（Windows）" class="headerlink" title="（2）Git（Windows）"></a>（2）Git（Windows）</h3><ol><li><p><a href="https://git-scm.com/" target="_blank" rel="noopener">官网</a>下载对应版本</p></li><li><p>安装选择全部默认（也可以自己设定安装位置）</p></li><li><p>最后一步也可以选择<code>Use Git from the Windows Command Prompt</code>，这样就可以命令行打开<code>git</code></p></li><li><p>安装完，命令行输入以下命令出现版本号即安装成功</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git --version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><h3 id="（3）Hexo（Windows）"><a href="#（3）Hexo（Windows）" class="headerlink" title="（3）Hexo（Windows）"></a>（3）Hexo（Windows）</h3><ol><li><p>【安装到C盘】在Windows上选定一个目录作为博客目录，在该目录下右键点击<code>Git Bash Here</code>，接下来使用git控制台进行Hexo的安装</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">npm i hexo-cli -ghexo -v  # 验证安装是否成功<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>【安装到指定目录】在npm安装目录右键点击<code>Git Bash Here</code>，然后输入</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">npm install --prefix . hexo-cli -g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><h1 id="二、GitHub"><a href="#二、GitHub" class="headerlink" title="二、GitHub"></a>二、GitHub</h1><h2 id="1、官网-1"><a href="#1、官网-1" class="headerlink" title="1、官网"></a>1、官网</h2><p><a href="https://github.com/" target="_blank" rel="noopener">GitHub</a></p><h2 id="2、注册"><a href="#2、注册" class="headerlink" title="2、注册"></a>2、注册</h2><p>官网跟着指引注册就OK</p><h2 id="3、创建博客仓库"><a href="#3、创建博客仓库" class="headerlink" title="3、创建博客仓库"></a>3、创建博客仓库</h2><h3 id="（1）如图操作"><a href="#（1）如图操作" class="headerlink" title="（1）如图操作"></a>（1）如图操作</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/create1.png" alt="图1"></p><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/create2.png" alt="图2"></p><h3 id="（2）注意事项"><a href="#（2）注意事项" class="headerlink" title="（2）注意事项"></a>（2）注意事项</h3><ol><li>第二张图中的第一步yourname一定要跟Owner的名字一样，且一定要加<code>.github.io</code>后缀（至于为什么，我也不得而知）</li></ol><h2 id="4、给仓库选择主题"><a href="#4、给仓库选择主题" class="headerlink" title="4、给仓库选择主题"></a>4、给仓库选择主题</h2><h3 id="（1）如图操作-1"><a href="#（1）如图操作-1" class="headerlink" title="（1）如图操作"></a>（1）如图操作</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/create3.png" alt="图3"></p><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/create4.png" alt="图4"></p><h1 id="三、写博客并发布到GitHub"><a href="#三、写博客并发布到GitHub" class="headerlink" title="三、写博客并发布到GitHub"></a>三、写博客并发布到GitHub</h1><h2 id="1、本地网站配置"><a href="#1、本地网站配置" class="headerlink" title="1、本地网站配置"></a>1、本地网站配置</h2><h3 id="（1）命令如下（在博客目录下）"><a href="#（1）命令如下（在博客目录下）" class="headerlink" title="（1）命令如下（在博客目录下）"></a>（1）命令如下（在博客目录下）</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">hexo init  # 初始化该目录npm install  # 安装必备的组件hexo g  # 生成静态网页hexo s  # 打开本地服务器并复制地址到chrome打开ctrl c  # 关闭本地服务器<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（2）chrome打开本地网站地址"><a href="#（2）chrome打开本地网站地址" class="headerlink" title="（2）chrome打开本地网站地址"></a>（2）chrome打开本地网站地址</h3><ol><li><a href="http://localhost:4000/" target="_blank" rel="noopener"><a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a></a></li></ol><h2 id="2、-写新文章"><a href="#2、-写新文章" class="headerlink" title="2、 写新文章"></a>2、 写新文章</h2><h3 id="（1）命令如下（在博客目录下）-1"><a href="#（1）命令如下（在博客目录下）-1" class="headerlink" title="（1）命令如下（在博客目录下）"></a>（1）命令如下（在博客目录下）</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">npm i hexo-deployer-git  # 安装拓展hexo new post "new md file"  # 新建一篇文章# 修改./source/_posts下的md文件hexo ghexo sctrl c<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、连接GitHub"><a href="#3、连接GitHub" class="headerlink" title="3、连接GitHub"></a>3、连接GitHub</h2><h3 id="（1）如何配置连接"><a href="#（1）如何配置连接" class="headerlink" title="（1）如何配置连接"></a>（1）如何配置连接</h3><ol><li>详情请见另一篇文章：</li></ol><h3 id="（2）修改配置"><a href="#（2）修改配置" class="headerlink" title="（2）修改配置"></a>（2）修改配置</h3><ol><li><p>命令（在博客目录下）</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">vi _config.ymla  # 修改# 如下图修改最后几行Esc  # 退出修改:wq!  # 保存<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>如图</p></li></ol><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/create5.png" alt="图5"></p><ol><li>注意：修改图中标红的地方就OK，换成你自己的name</li></ol><h3 id="（3）配置全局git-name-and-email"><a href="#（3）配置全局git-name-and-email" class="headerlink" title="（3）配置全局git name and email"></a>（3）配置全局git name and email</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git config --global user.name "your github name"git config --global user.email "your github private email"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>​    <strong>注意：</strong><code>your github private email</code>怎么配置请看</p><h3 id="（4）发布到GitHub"><a href="#（4）发布到GitHub" class="headerlink" title="（4）发布到GitHub"></a>（4）发布到GitHub</h3><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">hexo d<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="（5）同时拥有gitlab-and-github账号时"><a href="#（5）同时拥有gitlab-and-github账号时" class="headerlink" title="（5）同时拥有gitlab and github账号时"></a>（5）同时拥有gitlab and github账号时</h3><ol><li><p>发布完后，将全局git name and email改为gitlab账号</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">git config --global user.name "your gitlab name"git config --global user.email "your gitlab private email"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>进入<code>.deploy_git</code>配置局部账号即github账号</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">cd .deploy_git  # 根目录下进入.deploy_git# 配置git config --local user.name "your github name"git config --local user.email "your github private email"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>这样配置后，hexo d就是用github账号push，对gitlab push就是用gitlab账号</p></li></ol><h2 id="4、新的文章"><a href="#4、新的文章" class="headerlink" title="4、新的文章"></a>4、新的文章</h2><h3 id="（1）添加新md"><a href="#（1）添加新md" class="headerlink" title="（1）添加新md"></a>（1）添加新md</h3><ol><li><p>在./source/_posts下添加新的md文件</p></li><li><p>使用命令push</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">hexo ghexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h3 id="（2）插入图片"><a href="#（2）插入图片" class="headerlink" title="（2）插入图片"></a>（2）插入图片</h3><ol><li><p>安装插件</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">npm install https://github.com/CodeFalling/hexo-asset-image --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><ol><li><p>修改<code>_config_yml</code>配置</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">post_asset_folder: true  # 将false改为true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>在<code>./source/_posts</code>中新建一个md文件时，同时创建一个与md文件同名的文件夹，该文件夹用来存放该md文件所需图片</p></li><li>接着在md文件中以<code>![](/md文件名/1.png)</code>的格式插入图片</li><li>详细教程请见<a href="http://etrd.org/2017/01/23/hexo中完美插入本地图片/" target="_blank" rel="noopener">ETRD博客</a></li></ol><h3 id="（3）插入markdown"><a href="#（3）插入markdown" class="headerlink" title="（3）插入markdown"></a>（3）插入markdown</h3><ol><li><p>Hexo的<a href="https://hexo.io/zh-cn/docs/tag-plugins.html" target="_blank" rel="noopener">标签插件</a></p></li><li><p>引用站内文章</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">{% post_path filename %}{% post_link filename [title] [escape] %}{% post_link hexo-3-8-released %} # 链接使用文章的标题<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>在使用此标签时可以忽略文章文件所在的路径或者文章的永久链接信息、如语言、日期。</p><p>例如，在文章中使用 <code></code> 时，只需有一个名为 <code>how-to-bake-a-cake.md</code> 的文章文件即可。即使这个文件位于站点文件夹的 <code>source/posts/2015-02-my-family-holiday</code> 目录下、或者文章的永久链接是 <code>2018/en/how-to-bake-a-cake</code>，都没有影响。</p><p>默认链接文字是文章的标题，你也可以自定义要显示的文本。此时不应该使用 Markdown 语法 <code>[]()</code>。</p><p>默认对文章的标题和自定义标题里的特殊字符进行转义。可以使用<code>escape</code>选项，禁止对特殊字符进行转义。</p></li><li><p><a href="https://www.jibing57.com/2017/10/30/how-to-use-post-link-on-hexo/" target="_blank" rel="noopener">参考链接</a></p></li></ol><h2 id="5、更换主题"><a href="#5、更换主题" class="headerlink" title="5、更换主题"></a>5、更换主题</h2><ul><li><a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">hexo-theme-matery</a></li></ul><h3 id="（1）设置文章头-一劳永逸"><a href="#（1）设置文章头-一劳永逸" class="headerlink" title="（1）设置文章头-一劳永逸"></a>（1）设置文章头-一劳永逸</h3><ul><li>修改<code>/scaffolds/post.md</code>代码如下：</li></ul><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">---title: {{ title }}date: {{ date }}author: Myhaaimg:top: falsecover: falsecoverImg:password:toc: truemathjax: falsesummary: categories: tags:  - ---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="（2）修改subtitle"><a href="#（2）修改subtitle" class="headerlink" title="（2）修改subtitle"></a>（2）修改subtitle</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/xiugaisubtitle.png" alt="图：修改subtitle"></p><p><strong>注意：</strong>第一步修改应该是在<code>bg-cover-content.ejs</code>中</p><h3 id="（3）修改dream"><a href="#（3）修改dream" class="headerlink" title="（3）修改dream"></a>（3）修改dream</h3><p><img src="/2019/11/12/shi-yong-gong-ju-zhi-hexo/xiugaidream.png" alt="图：修改dream"></p><h2 id="6、让HEXO搭建的博客支持Latex"><a href="#6、让HEXO搭建的博客支持Latex" class="headerlink" title="6、让HEXO搭建的博客支持Latex"></a>6、让HEXO搭建的博客支持Latex</h2><p><a href="https://cps.ninja/2019/03/16/hexo-with-latex/" target="_blank" rel="noopener">参考</a></p><h3 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h3><p>安装 <a href="https://github.com/hexojs/hexo-math" target="_blank" rel="noopener">hexo-math</a> 插件，该插件（plugin）可支持使用 <a href="https://www.mathjax.org/" target="_blank" rel="noopener">MathJax</a> 或 <a href="https://katex.org/" target="_blank" rel="noopener">KaTeX</a> 来实现 LaTeX 排版系统，进而在网页上渲染出数学表达式（本文以 MathJax 为例）。</p><pre class="line-numbers language-lang-bash"><code class="language-lang-bash">## 打开终端，进入 hexo 博客所在文件夹$ cd ~/blog## 安装 hexo ； --save 参数会让 npm 在安装 hexo-math 之后自动将它写入 package.json 文件里，以便之后多电脑同步时使用$ npm install hexo-math --save<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将 Hexo 默认的 markdown 渲染引擎 <a href="https://github.com/hexojs/hexo-renderer-marked" target="_blank" rel="noopener">hexo-renderer-marked</a> 更换为 <a href="https://github.com/sun11/hexo-renderer-kramed" target="_blank" rel="noopener">hexo-renderer-kramed</a> ，引擎是在默认的渲染引擎的基础上修改了一些 bug 而已。此处不更换也没问题，本文以更换为例。</p><pre class="line-numbers language-lang-bash"><code class="language-lang-bash">## 卸载默认 markdown 渲染引擎 hexo-renderer-marked；若不卸载，会和新的引擎发生冲突（conflict）$ npm uninstall hexo-renderer-marked --save## 安装新引擎 hexo-renderer-kramed $ npm install hexo-renderer-kramed --save<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="修改-kramed-配置，解决语义冲突"><a href="#修改-kramed-配置，解决语义冲突" class="headerlink" title="修改 kramed 配置，解决语义冲突"></a>修改 kramed 配置，解决语义冲突</h3><p>由于 LaTeX 与 Markdown 语法存在冲突（例如在 markdown 中，<em>斜体</em>可以用 <code>*</code> 或者 <code>_</code> 表示，而 LaTeX 也会用到 <code>_</code> ），所以我们要对 kramed 默认的语法规则进行修改，否则之后会出现很多奇怪的排版样式。</p><p>打开 <code>~/blog/node_modules\kramed\lib\rules\inline.js</code> 文件（Hexo 博客所在文件夹的根目录下的 <code>node_modules</code> 文件夹），把第 11 行的 <code>escape</code> 变量的值修改为：</p><pre class="line-numbers language-lang-javascript"><code class="language-lang-javascript">escape: /^\\([`*\[\]()#$+\-.!_>])/,<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>同时把第 20 行的 <code>em</code> 变量修改为：</p><pre class="line-numbers language-lang-javascript"><code class="language-lang-javascript">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="设置-config-yml-开启-MathJax-渲染引擎（-重要-）"><a href="#设置-config-yml-开启-MathJax-渲染引擎（-重要-）" class="headerlink" title="设置 _config.yml 开启 MathJax 渲染引擎（==重要==）"></a>设置 _config.yml 开启 MathJax 渲染引擎（==重要==）</h3><p>在 <code>~/blog/_config.yml</code> 文件（注意，是 Hexo 博客文件夹<strong>根目录</strong>中的 <code>/_config.yml</code> 而不是主题目录下的 <code>/themes/next/_config.yml</code>）中增加 MathJax 的支持，并手动设置下面的 src（这一步很重要，使用默认的 src 会导致数学表达式渲染显示失败。这里的关键是 src 中的 <code>?config=TeX-MML-AM_CHTML</code> 这个字段）</p><pre class="line-numbers language-lang-yml"><code class="language-lang-yml">......# MathJaxmath:  engine: 'mathjax'  mathjax:    src: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML   ......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统之算法介绍</title>
      <link href="/2019/10/17/tui-jian-xi-tong-zhi-suan-fa-jie-shao/"/>
      <url>/2019/10/17/tui-jian-xi-tong-zhi-suan-fa-jie-shao/</url>
      
        <content type="html"><![CDATA[<h1 id="一、CF-Collaborative-Filtering"><a href="#一、CF-Collaborative-Filtering" class="headerlink" title="一、CF(Collaborative-Filtering)"></a>一、CF(Collaborative-Filtering)</h1><p>主要思想：</p><p>协同过滤推荐方法的主要思想是：利用已有用户群过去的行为或意见预测当前用户最可能喜欢哪些东西或对哪些东西感兴趣。</p><h2 id="1、基于用户的协同过滤"><a href="#1、基于用户的协同过滤" class="headerlink" title="1、基于用户的协同过滤"></a>1、基于用户的协同过滤</h2><p>潜在假设：</p><ul><li>如果用户过去有相似的偏好，那么他们未来也会有相似的偏好；</li><li>用户偏好不会随时间而变化</li></ul><p>主要思想：</p><ul><li>首先，给定一个评分数据集和当前用户的ID作为输入，找出与当前用户过去有相似偏好的其他用户；</li><li>然后，对当前用户没有评价过的物品，利用最近邻对物品的评分计算预测值。</li></ul><p>（1）协同推荐的评分数据库（例子1）</p><div class="table-container"><table><thead><tr><th></th><th>物品1</th><th>物品2</th><th>物品3</th></tr></thead><tbody><tr><td>用户1</td><td>5</td><td>4</td><td>？</td></tr><tr><td>用户2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>用户3</td><td>4</td><td>5</td><td>3</td></tr></tbody></table></div><p>（2）算法步骤</p><ul><li>利用用户已有的物品评分<code>rate</code>计算用户之间的相似度$sim(a, b)$，相似度计算方法可以是<strong><code>pearson</code>相关系数、改进余弦相似度、<code>spearman</code>秩相关系数、均方差等</strong>。（计算相似度是用两个用户拥有共同行为物品的评分来计算）</li><li>针对要预测的每个用户，选择与其相似度排名最靠前的<code>k</code>个用户对指定物品的评分做加权计算来得到该用户对该物品的评分。</li></ul><p>（3）面临的挑战</p><ul><li>用户量和物品量很大时，算法性能不高</li><li>当需要扫描大量潜在近邻时，很难做到实时计算预测值</li></ul><h2 id="2、基于物品的协同过滤"><a href="#2、基于物品的协同过滤" class="headerlink" title="2、基于物品的协同过滤"></a>2、基于物品的协同过滤</h2><p>主要思想：</p><ul><li>利用物品间的相似度来计算预测值</li></ul><h2 id="3、疑难解答"><a href="#3、疑难解答" class="headerlink" title="3、疑难解答"></a>3、疑难解答</h2><h3 id="1-相似度的改进-热门item或者活跃user带来的问题"><a href="#1-相似度的改进-热门item或者活跃user带来的问题" class="headerlink" title="1. 相似度的改进=热门item或者活跃user带来的问题"></a>1. 相似度的改进=热门item或者活跃user带来的问题</h3><ul><li><a href="chrome-extension://cdonnmffkdaoajfknoeeecmchibpmkmg/assets/pdf/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fftp%2Farxiv%2Fpapers%2F1301%2F1301.7363.pdf" target="_blank" rel="noopener">参考论文: Empirical Analysis of Predictive Algorithm for Collaborative Filtering</a></li><li><a href="https://blog.csdn.net/songbinxu/article/details/79917267" target="_blank" rel="noopener">改进办法参考博客链接</a></li></ul><h1 id="二、LR-Logistic-Regression-FTRL"><a href="#二、LR-Logistic-Regression-FTRL" class="headerlink" title="二、LR(Logistic Regression)+FTRL"></a>二、LR(Logistic Regression)+FTRL</h1><h2 id="1、逻辑回归"><a href="#1、逻辑回归" class="headerlink" title="1、逻辑回归"></a>1、逻辑回归</h2><p>FTRL本质上是一种优化方法，最早由google提出并用于CTR预估。常被用于逻辑回归的优化，因此先简单介绍一下逻辑回归的内容。</p><h3 id="1-1-sigmoid函数"><a href="#1-1-sigmoid函数" class="headerlink" title="1.1 sigmoid函数"></a>1.1 sigmoid函数</h3><p>由于二分类结果是1或者0，这与数学的阶跃函数很类似，但是阶跃函数在x=0的位置会发生突变，这个突变在数学上很难处理。所以一般使用sigmoid函数来拟合：</p><script type="math/tex; mode=display">g(z)={\frac 1{1+e^{-z}}} \qquad(1)</script><p>具体应用到逻辑回归算法中：</p><script type="math/tex; mode=display">z={\omega}_0+{\omega}_1x_1+{\omega}_2x_2+......+{\omega}_nx_n=\sum_{i=0}^n{\omega}_ix_i=\mathbf{\omega^TX}   \qquad(2)</script><p>其中<script type="math/tex">x_i</script>表示样本属性（对于我们而言，就是标签IP）的值， <script type="math/tex">\omega_i</script>表示这个属性对应的系数（也就是算法需要计算的内容）。注意这里将<script type="math/tex">x_0</script>与<script type="math/tex">\omega_0</script>也代入了上述公式，其中前者恒为1。于是问题就变成了在训练样本中，已知属性x与最终分类结果y（1或者0）时，如何求得这些系数 <script type="math/tex">\omega_i</script>，使得损失最小。</p><h3 id="1-2-极大似然估计MLE与损失函数"><a href="#1-2-极大似然估计MLE与损失函数" class="headerlink" title="1.2 极大似然估计MLE与损失函数"></a>1.2 极大似然估计MLE与损失函数</h3><p>在机器学习理论中，损失函数（loss function）是用来衡量模型的预测值<script type="math/tex">f(x)</script>与真实值<script type="math/tex">Y</script>的不一致程度，它是一个非负实值函数，损失函数越小，模型越优（还需考虑过拟合等问题）。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子</p><script type="math/tex; mode=display">\omega^* = \arg \min_\omega \frac{1}{m}{}\sum_{i=1}^{m} L(y_i, f(x_i; \omega)) + \lambda\  \Phi(\omega)  \qquad(3)</script><p>其中m表示样本的数量。对于逻辑回归，其loss function是log损失，这可以通过极大似然估计进行推导得到。</p><p>首先，给定一个样本<script type="math/tex">x</script>，可以使用一个线性函数对自变量进行线性组合，即上述的（2）式子：</p><script type="math/tex; mode=display">z={\omega}_0+{\omega}_1x_1+{\omega}_2x_2+......+{\omega}_nx_n=\sum_{i=0}^n{\omega}_ix_i=\mathbf{\omega^TX} \qquad(4)</script><p>根据sigmoid函数，我们可以得出预测函数的表达式为：</p><script type="math/tex; mode=display">h_{\omega}(x) = g(\omega^Tx) = \frac{1}{1 + e^{-\omega^Tx}}  \qquad(5)</script><p>上式表示<script type="math/tex">y=1</script>的预测函数为<script type="math/tex">h_{\omega}(x)</script>。在这里，假设因变量<script type="math/tex">y</script>服从伯努利分布，那么可以得到下列两个式子：</p><script type="math/tex; mode=display">\begin{aligned}p(y=1 | x) &= h_{\omega} (x)         \quad\qquad(6)\\p(y=0 | x) &= 1 - h_{\omega} (x)      \qquad(7)\end{aligned}</script><p>而对于上面的两个表达式，通过观察，我们发现，可以将其合并为以下表达式：</p><script type="math/tex; mode=display">p(y | x) = h_{\omega} (x)^y (1-h_{\omega} (x))^{1-y} \qquad(8)</script><p>根据上面的式子，给定一定的样本之后，我们可以构造出似然函数，然后可以使用极大似然估计MLE的思想来求解参数。但是，为了满足最小化风险理论，我们可以将MLE的思想转化为最小化风险化理论，最大化似然函数其实就等价于最小化负的似然函数。对于MLE，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说是什么样的参数才能使我们观测到目前这组数据的概率最大。使用MLE推导LR的loss function的过程如下。<br>首先，根据上面的假设，写出相应的极大似然函数（假定有<script type="math/tex">m</script>个样本）：</p><script type="math/tex; mode=display">\begin{aligned}L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\&=  \prod_{i=1}^{m} h_{\omega} (x_i)^{y_i} (1-h_{\omega} (x_i)^{1-y_i} \\\end{aligned}   \qquad(9)</script><p>上述式子中的<script type="math/tex">\omega</script>及<script type="math/tex">x_i</script>均为向量，并未显示其转置。</p><p>直接对上面的式子求导会不方便，因此，为了便于计算，我们可以对似然函数取对数，经过化简可以得到下式的推导结果：</p><script type="math/tex; mode=display">\begin{aligned}\log L(\omega)&= \sum_{i=1}^{m} \log \left [ (h_{\omega} (x_i)^{y_i} (1-h_{\omega} (x_i))^{1-y_i}) \right ] \\&= \sum_{i=1}^{m} \left [ y_i \log h_{\omega} (x_i) +  (1-y_i) \log(1-h_{\omega} (x_i)) \right ]  \\\end{aligned}  \qquad(10)</script><p>因此，损失函数可以通过最小化负的似然函数得到，即下式：</p><script type="math/tex; mode=display">J(\omega) = - \frac{1}{m} \sum_{i=1}^m \left [ y_i \log h_{\omega}(x_i) + (1-y_i) \log(1-h_{\omega}(x_i)  \right ]   \qquad(11)</script><p>在周志华版的机器学习中，将sigmiod函数代入<script type="math/tex">h_{\omega}(x_i)</script>，并使用ln代替log，上述公式表示为：</p><script type="math/tex; mode=display">\begin{aligned}J(\omega) &= - \frac{1}{m} \sum_{i=1}^m \left [ y_i \ln h_{\omega}(x_i) + (1-y_i) \ln(1-h_{\omega}(x_i)  \right ]\\&=- \frac{1}{m} \sum_{i=1}^m \left [ y_i\ln  \frac{1}{1+e^{-\omega x_i}}+(1-y_i)\ln \frac{e^{-\omega x_i}}{1+e^{-\omega x_i}}\right ]\\&=- \frac{1}{m} \sum_{i=1}^m \left [ \ln \frac{1}{1+e^{\omega x_i}} + y_i \ln \frac{1}{e^{-\omega x_i}}\right ]\\&= \frac{1}{m} \sum_{i=1}^m \left [ -y_iwx_i + \ln(1+e^{\omega x_i})\right ]\end{aligned}  \qquad(12)</script><p>在某些资料上，还有另一种损失函数的表达形式，但本质是一样的，如下【推导见下面1.4】：</p><script type="math/tex; mode=display">J(\omega) = \frac{1}{m} \sum_{i=1}^m log(1 + e^{-y_i \omega x}) \qquad(13)</script><h3 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h3><p>我们以梯度下降为例对逻辑回归进行求解，其迭代公式的推导过程如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{ \partial J(\omega)} {\partial \omega_j}&= -\frac{1}{m} \sum_{i}^{m} \left [ y_i(1 - h_{\omega}(x_i)) \cdot (-x_{i,j}) + (1 - y_i) h_{\omega} (x_i)  \cdot (x_{i,j}) \right ]\\& = - \frac{1}{m} \sum_{i}^{m}  (-y_i \cdot x_{i,j} + h_{\omega}(x_i) \cdot x_{i,j})  \\& = -\frac{1}{m} \sum_{i}^{m} (h_{\omega}(x_i) - y_i) x_{i,j}\end{aligned}  \qquad(14)</script><p>上述中<script type="math/tex">x_{i,j}</script>表示第<script type="math/tex">i</script>个样本的第<script type="math/tex">j</script>个属性的取值。<br>于是，<script type="math/tex">\omega</script>的更新方式为：</p><script type="math/tex; mode=display">\omega_{j+1} = \omega_j - \alpha \sum_{i=1}^{m} (h_{\omega}(x_i) - y_i) x_{x,j}</script><p>对于随机梯度下降，每次只取一个样本，则<script type="math/tex">\omega</script>的更新方式为：</p><script type="math/tex; mode=display">\omega_{j+1} = \omega_j - \alpha (h_{\omega}(x)- y) x_{j}</script><p>其中<script type="math/tex">x</script>为这个样本的特征值，<script type="math/tex">y</script>为这个样本的真实值，<script type="math/tex">x_j</script>为这个样本第<script type="math/tex">j</script>个属性的值。</p><p>使用周志华版的损失函数更容易得出这个结论。</p><h3 id="1-4-另一种形式的损失函数及其梯度"><a href="#1-4-另一种形式的损失函数及其梯度" class="headerlink" title="1.4 另一种形式的损失函数及其梯度"></a>1.4 另一种形式的损失函数及其梯度</h3><p>与上面相同，根据sigmoid函数，我们可以得出预测函数的表达式为：</p><script type="math/tex; mode=display">h_{\omega}(x) = g(\omega^Tx) = \frac{1}{1 + e^{-\omega^Tx}}</script><p>上式表示<script type="math/tex">y=1</script>的预测函数为<script type="math/tex">h_{\omega}(x)</script>。<br>但与上面不同，我们假设样本的分布为{-1,1}，则</p><script type="math/tex; mode=display">p(y=1 | x) = h_{\omega} (x)</script><script type="math/tex; mode=display">p(y=-1 | x) = 1 - h_{\omega} (x)</script><p>对于sigmoid函数，有以下特性（简单推导一下就可以得到）：</p><script type="math/tex; mode=display">h(-x) = 1 - h(x)</script><p>于是(14)(15)式可以表示为：</p><script type="math/tex; mode=display">p(y|x) = h_\omega(yx)</script><p>同样，我们使用MLE作估计，</p><script type="math/tex; mode=display">\begin{aligned}L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\&=  \prod_{i=1}^{m} h_\omega(y_i x_i)\\&= \prod_{i=1}^{m} \frac{1}{1+e^{-y_iwx_i}}\end{aligned}</script><p>对上式取对数及负值，得到损失为：</p><script type="math/tex; mode=display">\begin{aligned}-\log L(\omega)&= -\log \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\&=  -\sum_{i=1}^{m} \log p(y_i | x_i; \omega)  \\&=  -\sum_{i=1}^{m} \log \frac{1}{1+e^{-y_iwx_i}}\\&=  \sum_{i=1}^{m} \log(1+e^{-y_iwx_i})\\\end{aligned}</script><p>即对于每一个样本，损失函数为：</p><script type="math/tex; mode=display">L(\omega)=\log(1+e^{-y_iwx_i})</script><p>对上式求梯度，容易得到：</p><script type="math/tex; mode=display">\begin{aligned}\frac{ \partial J(\omega)} {\partial \omega_j}&= \frac{-y_i x_i}{1+e^{y_i \omega x_i}}\end{aligned}</script><h2 id="2、FOBOS与RDA"><a href="#2、FOBOS与RDA" class="headerlink" title="2、FOBOS与RDA"></a>2、FOBOS与RDA</h2><h3 id="2-1-FOBOS基本原理"><a href="#2-1-FOBOS基本原理" class="headerlink" title="2.1 FOBOS基本原理"></a>2.1 FOBOS基本原理</h3><p>FOBOS算法由John Duchi和Yoram Singer提出，是梯度下降的一个变种。<br>与梯度下降不同，它将权重的更新分为2个步骤：</p><script type="math/tex; mode=display">\begin{aligned}W_{t+\frac{1}{2}}&=W_t-\eta_tG_t   \\nW_{t+1}&=\arg\min\{\frac{1}{2}\|W-W_{t+\frac{1}{2}}\|^2+\eta_{(t+\frac{1}{2})}\psi(W)\}\end{aligned}</script><p>从上面的2个步骤可以看出：<br>第一个步骤是一个标准的梯度下降。<br>第二个步骤是对梯度下降的结果进行微调。这里可以分为2部分：（1）前半部分保证了微调发生在第一个步骤结果（取梯度下降结果）的附近。（2）后半部分用于处理正则化，产生稀疏性。</p><p>根据次梯度理论的推断，可以得出<script type="math/tex">W_{(t+1)}</script>不仅仅与迭代前的状态<script type="math/tex">W_t</script>有关，而且与迭代后的相关</p><h3 id="2-2-L1-FOBOS"><a href="#2-2-L1-FOBOS" class="headerlink" title="2.2 L1-FOBOS"></a>2.2 L1-FOBOS</h3><p>FOBOS在L1正则化条件下，特征权重的更新方式为：（推导过程暂略）</p><script type="math/tex; mode=display">\omega_{t+1,i}=sgn(\omega_{t,i}-\eta_tg_{t,i})max\{0,|\omega_{t,i}-\eta_tg_{t,i}|-\eta_{t+\frac{1}{2}}\lambda\}</script><p>其中<script type="math/tex">g_{t,i}</script>为梯度在维度i上的取值</p><h3 id="2-3-RDA基本原理"><a href="#2-3-RDA基本原理" class="headerlink" title="2.3 RDA基本原理"></a>2.3 RDA基本原理</h3><p>简单截断、TG、FOBOS都是建立在SGD基础上的一个变种，属于梯度下降类型的方法，这类方法的精度比较高，同时也能得到一定的稀疏性。而RDA是从另一个方面来求解，它的精度比FOBOS等略差，但它更有效的提升了稀疏性。</p><p>在RDA中，特征权重的更新策略为：</p><script type="math/tex; mode=display">W_{t+1}=\arg\min\{\frac{1}{t}\sum_{r=1}^{t}<G^{r},W>+\psi(W)+\frac{\beta_t}{t}h(W)\}</script><p>其中<script type="math/tex"><G^{r},W></script>表示梯度<script type="math/tex">G^r</script>对W的积分平均值（积分中值）；<script type="math/tex">\psi(W)</script>为正则项；<script type="math/tex">h(W)</script>为一个辅助的严格的凸函数；<script type="math/tex">{\beta_t|t\ge1}</script>是一个非负且非自减序列。</p><h3 id="2-4-L1-RDA"><a href="#2-4-L1-RDA" class="headerlink" title="2.4 L1-RDA"></a>2.4 L1-RDA</h3><p>在L1正则化条件下，RDA的各个维度的权重更新方式为：</p><p><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.17.20.png" alt></p><p>亦即当某个维度上的累积梯度平均值的绝对值 <script type="math/tex">|\overline g_{t,i}|</script>小于阈值<script type="math/tex">\lambda</script>时，该维度权重被置为0。</p><h2 id="3、FTRL"><a href="#3、FTRL" class="headerlink" title="3、FTRL"></a>3、FTRL</h2><p>理论及实验均证明，L1-FOBOS这类基于梯度下降的算法有比较高的精度，但L1-RDA却能在损失一定精度的情况下产生更好的稀疏性。<br>把这二者的优点结合成一个算法，这就是FTRL算法的来源。</p><h3 id="3-1-从L1-FOBOS和L1-RDA推导FTRL"><a href="#3-1-从L1-FOBOS和L1-RDA推导FTRL" class="headerlink" title="3.1 从L1-FOBOS和L1-RDA推导FTRL"></a>3.1 从L1-FOBOS和L1-RDA推导FTRL</h3><p>我们令<script type="math/tex">\eta_{t+\frac{1}{2}}=\eta_t=\Theta(\frac{1}{\sqrt{t}})</script>是一个非增正序列，同时代入L1正则项，得到L1-FOBOS的形式如下：</p><script type="math/tex; mode=display">\begin{aligned}W_{t+\frac{1}{2}}&=W_t-\eta_tG_t \\W_{t+1}&=\arg\min\{\frac{1}{2}\|W-W_{t+\frac{1}{2}}\|^2+\eta_{t}\lambda\|W\|_1\}\end{aligned}</script><p>将这2个公式合并到一起，得到L1-FOBOS的形式如下：</p><script type="math/tex; mode=display">W_{t+1}=\arg\min\{\frac{1}{2}\|W-W_t+\eta_tG_t\|^2+\eta_{t}\lambda\|W\|_1\}</script><p>将上式分解为N个独立的维度进行最优化求解：</p><script type="math/tex; mode=display">\begin{aligned}w_i&=\arg\min\{\frac{1}{2}\|w_i-w_{t,i}+\eta_tg_{t,i}\|^2+\eta_{t}\lambda|w_{t,i}|_1\} \\&=\arg\min\{\frac{1}{2}(w_i-w_{t,i})^2+\frac{1}{2}(\eta_t g_{t,i})^2+w_i\eta_tg_{t,i}-w_{t,i}\eta_tg_{t,i}+\eta_t\lambda|w_i|\}\\&=\arg\min\{w_ig_{t,i}+\lambda|w_i|+\frac{1}{2\eta_t}(w_i-w_{t,i})^2+|\frac{\eta_t}{2}g_{t,i}^2-w_{t,i}g_{t,i}|\}\end{aligned}</script><ul><li>上述推导的最后一步是通过除以<script type="math/tex">\eta_t</script>得到的。<br>由于上式中的最后一项<script type="math/tex">|\frac{\eta_t}{2}g_{t,i}^2-w_{t,i}g_{t,i}|</script>是一个与<script type="math/tex">w_i</script>无关的量，因此上式可简化为：<script type="math/tex; mode=display">w_i=\arg\min\{w_ig_{t,i}+\lambda|w_i|+\frac{1}{2\eta_t}(w_i-w_{t,i})^2\}</script>把N个独立优化的维度重新合并，L1-FOBOS可写成以下形式：<script type="math/tex; mode=display">W_{t+1}=\arg\min\{G_tW+\lambda\|W\|_1+\frac{1}{2\eta_t}\|W-W_t\|_2^2\}</script></li></ul><p>另一方面，L1-RDA可以表达为：</p><script type="math/tex; mode=display">W_{t+1}=\arg\min\{G_{(1:t)}W+t\lambda\|W\|_1+\frac{1}{2\eta_t}\|W-0\|_2^2\}</script><p>其中<script type="math/tex">G_{(1:t)}=\sum_{s=1}{t}G_s</script>。<br>我们令<script type="math/tex">\sigma_s=\frac{1}{\eta_s}-\frac{1}{\eta_{s-1}}</script>，可以得到<script type="math/tex">\sigma_{(1:t)}=\frac{1}{\eta_t}</script>。那么L1-FOBOS与L1-RDA可以写为以下格式：</p><script type="math/tex; mode=display">\begin{aligned}W_{t+1}&=\arg\min\{G_tW+\lambda\|W\|_1+\frac{1}{2}\sigma_{(1:t)}\|W-W_t\|_2^2\}\\W_{t+1}&=\arg\min\{G_{(1:t)}W+t\lambda\|W\|_1+\frac{1}{2}\sigma_{(1:t)}\|W-0\|_2^2\}\end{aligned}</script><p>比较以上2式的区别：</p><ul><li>（1）L1-FOBOS考虑的是当前梯度的影响，L1-RDA则考虑了累积影响。</li><li>（2）L1-FOBOS限制<script type="math/tex">W_{t+1}</script>不能离<script type="math/tex">W_t</script>太远，而L1-RDA的W则不能离0太远，因此后者更容易产生稀疏性。</li></ul><h3 id="3-2-FTRL权重更新的最终形式"><a href="#3-2-FTRL权重更新的最终形式" class="headerlink" title="3.2 FTRL权重更新的最终形式"></a>3.2 FTRL权重更新的最终形式</h3><p>在google2010公布的理论文章中并没有使用L2正则项，但在2013年公布工程实施方案时引入了L2。<br>因此，综合上述的L1-FOBOS及L1-RDA，FTRL算法的权重更新方式为：</p><script type="math/tex; mode=display">W_{t+1}=\arg\min\{G_{(1:t)}W+\lambda_1\|W\|_1+\lambda_2\|W\|_2^2+\frac{1}{2}\sum_{s=1}^t(\sigma_s\|W-W_s\|_2^2)\}</script><p>将上式展开，得到</p><script type="math/tex; mode=display">W_{t+1}=\arg\min\{(G_{(1:t)}-\sum_{s=1}^t\sigma_sW_s)W+\lambda_1\|W\|_1+\frac{1}{2}(\lambda_2+\sum_{s=1}^t\sigma_s)\|W\|^2+\frac{1}{2}\sum_{s=1}^{t}\sigma_s\|W_s\|_2^2\}</script><p>由于上式的最后一项相对于W来说是一个常数，同时令<script type="math/tex">Z_t=G_{(1:t)}-\sum_{(s=1)}^t\sigma_sW_s</script>，上式可表示为：</p><script type="math/tex; mode=display">W_{t+1}=\arg\min\{Z_tW+\lambda_1\|W\|_1+\frac{1}{2}(\lambda_2+\sum_{s=1}^t\sigma_s)\|W\|^2\}</script><p>各个维度可以独立表示为：</p><script type="math/tex; mode=display">w_{i+1}=\arg\min\{z_{t,i}w_i+\lambda_1|w_i|+\frac{1}{2}(\lambda_2+\sum_{s=1}^{t}\sigma_s)w_i^2\}</script><p>使用与L1-FOBOS相同的分析方法可以得到：<br><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.18.58.png" alt></p><p>根据上面的定义:</p><script type="math/tex; mode=display">\sigma_{(1:t)}=\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}</script><p>我们使用下面介绍的学习率，以及令<script type="math/tex">n_i=\sum g_i^2</script>，则可以得到FTRL的最终形式：<br><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.19.50.png" alt></p><h3 id="3-3-学习率"><a href="#3-3-学习率" class="headerlink" title="3.3 学习率"></a>3.3 学习率</h3><p>1、per-coordinate learning rate<br>在FTRL中，学习率的定义如下：</p><script type="math/tex; mode=display">\eta_{t,i}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^tg_{s,i}^2}}</script><p>其中<script type="math/tex">\alpha\beta</script>是自定义的参数。</p><p>在一般梯度下降算法中，使用的是一个全局的学习率策略：<script type="math/tex">\eta_t=\frac{1}{\sqrt{t}}</script>。这个策略保证了学习率是一个正的非增序列，这个值对于每一个特征都是一样的。</p><p>考虑一个极端的情况，我们有多个稀疏特征，其中一个特征<script type="math/tex">x_1</script>出现非常频繁，而另一个特征<script type="math/tex">x_2</script>很少出现。比如第一个样本，<script type="math/tex">x_1</script>和<script type="math/tex">x_2</script>同时出现了，但接下来的99个样本都只出现了<script type="math/tex">x_1</script>，然后第100个样本<script type="math/tex">x_2</script>又出来了，由于此时的学习率为<script type="math/tex">\frac{1}{\sqrt{100}}</script>，远小于第一个样的影响了。也就是说假如第一个样本是正样本，第10个样本是负样本，此时由于学习率的不同，模型会认为<script type="math/tex">x_2</script>会是一个有用的正相关的特征，即<script type="math/tex">\omega>0</script>。但事实上，这个特征只出现了2次，而且一正一负，因此这应该是一个无用的特征，即<script type="math/tex">\omega=0</script>。</p><p>在FTRL中，我们会使用一个特征的累积梯度，由于中间的99个数据没有这个特征，因此其对应的梯度为0，因此第二次的负样本对应的学习率只是略小于第一个正样本。</p><h3 id="3-4-工程实现计算过程"><a href="#3-4-工程实现计算过程" class="headerlink" title="3.4 工程实现计算过程"></a>3.4 工程实现计算过程</h3><h4 id="3-4-1-一些定义"><a href="#3-4-1-一些定义" class="headerlink" title="3.4.1 一些定义"></a>3.4.1 一些定义</h4><p>对于每一个样本，我们计算以下数值。</p><p>（1）<script type="math/tex">p_t</script></p><p>使用当前的<script type="math/tex">\omega</script>代入sigmoid函数得出的预测值，即：</p><script type="math/tex; mode=display">p=\frac{1}{1+e^{-(\sum_{i=1}^n\omega_ix_i)}}</script><p>（2）<script type="math/tex">g_i</script></p><p>损失函数在某一个维度上的梯度，对于逻辑回归而言：</p><script type="math/tex; mode=display">g_i=(p-y)x_i</script><p>其中y为当前样本的实际值，即0或者1。</p><p>（3）<script type="math/tex">n_i</script></p><p>这是该维度梯度<script type="math/tex">g_i^2</script>的累积值，即：</p><script type="math/tex; mode=display">n_i=n_i+g_i^2</script><p>（4）<script type="math/tex">\eta_i</script></p><p>这是该维度的学习率，它与累积梯度有关。定义为：</p><script type="math/tex; mode=display">\eta_i=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^t(g_i^s)^2}}=\frac{\alpha}{\beta+\sqrt{n_i}}</script><p>其中<script type="math/tex">\alpha\beta</script>为用户定义的2个参数。</p><p>（5）<script type="math/tex">\sigma_i</script></p><script type="math/tex; mode=display">\sigma_i$$是一个中间计算值，没有实际含义，其定义为：</script><p>\sigma_i=\frac{1}{\eta_{t,i}}-\frac{1}{\eta_{t-1,i}}=\frac{1}{\alpha}(\sqrt{n_i+g_i^2}-\sqrt{n_i})</p><script type="math/tex; mode=display">（6）$$z_i</script><script type="math/tex; mode=display">z_i$$也是一个辅助计算的中间值，它的定义为：</script><p>z_{t,i}=g^{(1:t)}-\sum_{s=1}^t{\sigma_{s,i}\omega_{s,i}}</p><script type="math/tex; mode=display">于是$$z_i$$的更新为：</script><p>z_t-z_{t-1}=g_t-\sigma_{t}\omega_{t}</p><script type="math/tex; mode=display">即：</script><p>z_i=z_i+g_t-\sigma_i\omega_i</p><script type="math/tex; mode=display">#### 3.4.2 FTRL算法（1）设定以下4个输入参数，这些参数根据经验而定，可以参考以下数据：</script><p>\alpha=0.1,\beta=1,\lambda_1=1,\lambda_2=1</p><script type="math/tex; mode=display">（2）初始化以下数值：</script><p>z_i=0,n_i=0</p><script type="math/tex; mode=display">（3）对于**每一个样本**的所带的**每一个维度**,更新$$\omega</script><p><img src="https://lujinhong-markdown.oss-cn-beijing.aliyuncs.com/md/%E6%88%AA%E5%B1%8F2020-09-04%20%E4%B8%8A%E5%8D%8810.20.36.png" alt></p><p>（4）使用上面更新后的<script type="math/tex">\omega</script>，预测<strong>这个样本</strong>的值，即代入sigmoid函数计算<script type="math/tex">p_t</script></p><script type="math/tex; mode=display">p=\frac{1}{1+e^{-(\sum_{i=1}^n\omega_ix_i)}}</script><p>（5）对于<strong>每一个样本</strong>的<strong>每一个维度</strong>，更新<script type="math/tex">g_i,\sigma_i,z_i,n_i</script>，即上面所说的：</p><script type="math/tex; mode=display">\begin{aligned}g_i&=(p-y)x_i \\\sigma_i&=\frac{1}{\alpha}(\sqrt{n_i+g_i^2}-\sqrt{n_i})\\z_i&=z_i+g_t-\sigma_i\omega_i\\n_i&=n_i+g_i^2\end{aligned}</script><h4 id="3-4-3-特征的独立性"><a href="#3-4-3-特征的独立性" class="headerlink" title="3.4.3 特征的独立性"></a>3.4.3 特征的独立性</h4><p>如果特征是非独立的，会导致特征的权重不符合预期。<br>比如，有2个特征A和B，然后加了一个组合标签A&amp;B，假如A和B都是和label非常正相关的，A&amp;B效果更好。但结果会发现A&amp;B这个标签的权重值可能还不如A或者B的权重高。<br>这是因为，带了A&amp;B标签的，必然带A或者B，大概率被预测成高概率值，而LR梯度为：</p><script type="math/tex; mode=display">(p-y)x_i</script><p>对于正样本，<script type="math/tex">y=1</script>，则梯度并不大，所以A&amp;B提升并不大；而对于负样本，<script type="math/tex">y=0</script>，则梯度非常大，导致惩罚很大，权重降低很多。</p><p>所以最终结果A和B的权重都可能比A&amp;B要大，但对于带A&amp;B标签的用户来说，他由于同时带3个特征，所以预测概率值还是比较高的。</p><p>当然最好的办法是尽量避免这种关联性太强的特征。</p><h2 id="4、FTRL的工程应用"><a href="#4、FTRL的工程应用" class="headerlink" title="4、FTRL的工程应用"></a>4、FTRL的工程应用</h2><p>这里只列出了google提供的一些建议，我们自身的工程应用并不包含在内。</p><p>（1）减少样本的数量</p><ul><li>Poisson Inclusion：以p的概率接受样本</li><li>Bloom Filter Inclusion：只有当特征出现次数达到一定数量后，特征才会被加入模型</li></ul><p>（2）使用更少的位来进行浮点数编码<br>一般而言，特征对应的<script type="math/tex">\omega</script>在(-2,2)之间，所以我们可以将32，或者64位的浮点数编码规则改为q2.13编码方式，即2位整数，1位小数点，13位小数。</p><p>（3）多个类似的模型<br>把多个同类型/相似的模型放在一起保存，可以不需要记录多个key，只需要记录一次Key，以及各个模型中的<script type="math/tex">\omega</script>。这可以节省内存、带宽、CPU、存储。</p><p>（4）只保存一个模型<br> 在上面的基础上更进一步，所有模型共用一份<script type="math/tex">\omega</script>，以及以一个bit来记录本模型有哪些特征。当模型更新某个<script type="math/tex">\omega</script>后，取它与之前那个值的平均值。<br>与共用一个模型对比，好处是记录了这个模型有哪些有效特征，共用模型就无法区分了。</p><p>（5）近似计算学习率<br>只使用正样本P与负样本数据N来近似估计梯度的平方和，前提是假设特征有类似的分布。</p><script type="math/tex; mode=display">\sum{g_i^2}=\frac{PN}{P+N}</script><p>（6）减少负样本数量<br>减少负样本数量，然后在训练时弥补其损失。</p><h1 id="三、FM-Factorization-Machine"><a href="#三、FM-Factorization-Machine" class="headerlink" title="三、FM(Factorization Machine)"></a>三、FM(Factorization Machine)</h1><h2 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h2><h3 id="1-1-1-线性模型"><a href="#1-1-1-线性模型" class="headerlink" title="1.1.1 线性模型"></a>1.1.1 线性模型</h3><p>常见的线性模型，比如线性回归、逻辑回归等，它只考虑了每个特征对结果的单独影响，而没有考虑特征间的组合对结果的影响。</p><p>对于一个有n维特征的模型，线性回归的形式如下：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= \omega_0 + \omega_1x_1+\omega_2x_2+...+\omega_nx_n \\ &=\omega_0+\sum_{i=1}^n{\omega_ix_i} \end{aligned}       \qquad (1)</script><p>其中<script type="math/tex">(\omega_0,\omega_1...\omega_n)</script>为模型参数，<script type="math/tex">(x_1,x_2...x_n)</script>为特征。<br>从(1)式可以看出来，模型的最终计算结果是各个特征的独立计算结果，并没有考虑特征之间的相互关系。</p><p>举个例子，我们认为“USA”与”Thanksgiving”，”China”与“Chinese new year”这样的组合特征是很有意义的，在这样的组合特征下，会对某些商品表现出更强的购买意愿，而单独考虑国家及节日都是没有意义的。</p><h2 id="1-1-2-二项式模型"><a href="#1-1-2-二项式模型" class="headerlink" title="1.1.2 二项式模型"></a>1.1.2 二项式模型</h2><p>我们在（1）式的基础上，考虑任意2个特征分量之间的关系，得出以下模型：</p><script type="math/tex; mode=display">f(x)=\omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\omega_{ij}x_ix_j               \qquad (2)</script><p>这个模型考虑了任意2个特征分量之间的关系，但并未考虑更高阶的关系。<br>模型涉及的参数数量为：</p><script type="math/tex; mode=display">1+n+\frac{n(n-1)}{2}=\frac{1}{2}(n^2+n+2)  \qquad (3)</script><p>对于参数<script type="math/tex">\omega_i</script>的训练，只要这个样本中对应的<script type="math/tex">x_i</script>不为0，则可以完成一次训练。<br>但对于参数<script type="math/tex">\omega_{ij}</script>的训练，需要这个样本中的<script type="math/tex">x_i</script>和<script type="math/tex">x_j</script>同时不为0，才可以完成一次训练。<br>在数据稀疏的实际应用场景中，二次项<script type="math/tex">\omega_{ij}</script>的训练是非常困难的。因为每个<script type="math/tex">\omega_{ij}</script>都需要大量<script type="math/tex">x_i</script>和<script type="math/tex">x_j</script>都不为0的样本。但在数据稀疏性比较明显的样本中，<script type="math/tex">x_i</script>和<script type="math/tex">x_j</script>都不为0的样本会非常稀少，这会导致<script type="math/tex">\omega_{ij}</script>不能得到足够的训练，从而不准确。</p><h2 id="1-2-FM"><a href="#1-2-FM" class="headerlink" title="1.2 FM"></a>1.2 FM</h2><h3 id="1-2-1-FM基本原理"><a href="#1-2-1-FM基本原理" class="headerlink" title="1.2.1 FM基本原理"></a>1.2.1 FM基本原理</h3><p>为了解决二项式模型中由于数据稀疏引起的训练不足的问题，我们为每个特征维度<script type="math/tex">x_i</script>引入一个辅助向量：</p><script type="math/tex; mode=display">V_i = (v_{i1},v_{i2},v_{i3},...,v_{ik})^T\in \mathbb R^k, i=1,2,3,...,n  \qquad(4)</script><p>其中<script type="math/tex">k</script>为辅助变量的维度，依经验而定，一般而言，对于特征维度足够多的样本，<script type="math/tex">k<<n</script>。<br>将<script type="math/tex">\omega_{ij}</script>表示为：</p><script type="math/tex; mode=display">\omega_{ij}=V_i^TV_j=\sum_{l=1}^kv_{il}v_{jl} \qquad(5)</script><p>简单的说，我们不再简单的使用样本训练具体的<script type="math/tex">\omega_{ij}</script>，而是先训练2个隐变量<script type="math/tex">V_i</script>以及<script type="math/tex">V_j</script>，然后使用式（5）求出最终的<script type="math/tex">\omega_{ij}</script>。</p><p>具体而言，<script type="math/tex">\omega_{ij}=V_i^TV_j</script>与<script type="math/tex">\omega_{hi}=V_h^TV_i</script>有相同的项<script type="math/tex">V_i</script>，也就是只要样本中的<script type="math/tex">x_i</script>不为0，且最少具有一个其它特征，则这个样本则可用于训练<script type="math/tex">V_i</script>，这就解决了数据稀疏性导致的问题。</p><p>于是，在FM中，模型可以表达为：</p><script type="math/tex; mode=display">f(x) = \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j \qquad(6)</script><h3 id="1-2-2-数据分析"><a href="#1-2-2-数据分析" class="headerlink" title="1.2.2 数据分析"></a>1.2.2 数据分析</h3><p>我们的目标是要求得以下交互矩阵W：</p><script type="math/tex; mode=display">W=        \begin{pmatrix}        \omega_{11} & \omega_{12}& ... &\omega_{1n} \\        \omega_{21} & \omega_{22}& ... &\omega_{2n} \\        \vdots &\vdots &\ddots &\vdots\\       \omega_{n1} & \omega_{n2}& ... &\omega_{nn} \\        \end{pmatrix}_{n\times n} \qquad(7)</script><p>由于直接求解W不方便，因此我们引入隐变量V：</p><script type="math/tex; mode=display">V=        \begin{pmatrix}        v_{11} & v_{12}& ... &v_{1k} \\        v_{21} & v_{22}& ... &v_{2k} \\        \vdots &\vdots &\ddots &\vdots\\       v_{n1} & v_{n2}& ... &v_{nk} \\        \end{pmatrix}_{n\times k}=\begin{pmatrix}V_1^T\\V_2^T\\\cdots \\V_n^T\\\end{pmatrix} \qquad(8)</script><p>令</p><script type="math/tex; mode=display">VV^T = W \qquad(9)</script><p>如果我们先得到V，则可以得到W了。<br>现在只剩下一个问题了，是否一个存在V，使得上述式（9）成立。<br>理论研究表明：当k足够大时，对于任意对称正定的实矩阵<script type="math/tex">W\in \mathbb R^{n \times  n}</script>，均存在实矩阵<script type="math/tex">V\in \mathbb R^{n \times  k}</script>，使得<script type="math/tex">W=VV^T</script>。</p><p>理论分析中要求参数k足够的大，但在高度稀疏数据的场景中，由于 没有足够的样本，因此k通常取较小的值。事实上，对参数<script type="math/tex">k</script>的限制，在一定程度上可以提高模型的泛化能力。</p><h3 id="1-2-3参数个数"><a href="#1-2-3参数个数" class="headerlink" title="1.2.3参数个数"></a>1.2.3参数个数</h3><p>假设样本中有n个特征，每个特征对应的隐变量维度为k，则参数个数为<script type="math/tex">1+n+nk</script>。<br>正如上面所言，对于特征维度足够多的样本，<script type="math/tex">k<<n</script>。</p><h3 id="1-2-4-计算时间复杂度"><a href="#1-2-4-计算时间复杂度" class="headerlink" title="1.2.4 计算时间复杂度"></a>1.2.4 计算时间复杂度</h3><p>下面我们分析一下已经知道所有参数，代入式（6）计算预测值时的时间复杂度。从式（6）中一看，</p><script type="math/tex; mode=display">f(x) = \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j \qquad(6)</script><p>可以看出时间复杂度是<script type="math/tex">O(kn^2)</script>。但我们对上述式子的最后一项作变换后，可以得出一个<script type="math/tex">O(kn)</script>的时间复杂度表达式。</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j&= \frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n(V_i^TV_j)x_ix_j-\sum_{i=1}^n(V_i^TV_i)x_ix_i\right) \\&=\frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n\sum_{l=1}^kv_{il}v_{jl}x_ix_j-\sum_{i=1}^n\sum_{l=1}^k v_{il}^2x_i^2\right)\\&=\frac{1}{2}\sum_{l=1}^k\left(\sum_{i=1}^n(v_{il}x_i)\sum_{j=1}^n(v_{jl}x_j)-\sum_{i=1}^nv_{il}^2x_i^2\right)\\&=\frac{1}{2}\sum_{l=1}^k\left(\left(\sum_{i=1}^n(v_{il}x_i)\right)^2-\sum_{i=1}^nv_{il}^2x_i^2\right)\end{aligned} \qquad(10)</script><p>上述式子中的<script type="math/tex">\sum_{i=1}^n(v_{il}x_i)</script>只需要计算一次就好，因此，可以看出上述模型的复杂度为<script type="math/tex">O(kn)</script>。<br>也就是说我们不要直接使用式（6）来计算预测结果，而应该使用式（10），这样的计算效率更高。</p><h3 id="1-2-5-梯度"><a href="#1-2-5-梯度" class="headerlink" title="1.2.5 梯度"></a>1.2.5 梯度</h3><p>FM有一个重要的性质：multilinearity：若记<script type="math/tex">\Theta=(\omega_0,\omega_1,\omega_2,...,\omega_n,v_{11},v_{12},...,v_{nk})</script>表示FM模型的所有参数，则对于任意的<script type="math/tex">\theta \in \Theta</script>，存在与<script type="math/tex">\theta</script>无关的<script type="math/tex">g(x)</script>与<script type="math/tex">h(x)</script>，使得式（6）可以表示为：</p><script type="math/tex; mode=display">f(x) = g(x) + \theta h(x) \qquad(11)</script><p>从式（11）中可以看出，如果我们得到了<script type="math/tex">g(x)</script>与<script type="math/tex">h(x)</script>，则对于参数<script type="math/tex">\theta</script>的梯度为<script type="math/tex">h(x)</script>。下面我们分情况讨论。</p><ul><li><p>当 <script type="math/tex">\theta=\omega_0</script>时，式（6）可以表示为：</p><script type="math/tex; mode=display">f(x) = \color{blue}{\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j} +\omega_0 \times \color{red}{1} \qquad(12)</script><p>上述中的蓝色表示<script type="math/tex">g(x)</script>，红色表示<script type="math/tex">h(x)</script>。下同。<br>从上述式子可以看出此时的梯度为1.</p></li><li><p>当<script type="math/tex">\theta=\omega_l, l \in (1,2,...,n)</script>时，</p><script type="math/tex; mode=display">f(x) = \color{blue}{\omega_0+\sum_{i=1,\ i \ne l}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(V_i^TV_j)x_ix_j}+\omega_l \times \color{red}{x_l} \qquad(13)</script><p>此时梯度为<script type="math/tex">x_l</script>。</p></li><li><p>当<script type="math/tex">\theta=v_{lm}</script>时</p><script type="math/tex; mode=display">f(x) =\color{blue}{ \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\sum_{s=1 , is \ne lm ,js \ne lm}^k v_{is}v_{js})x_ix_j }+v_{lm}\times \color{red}{x_l\sum_{i=1,i \ne l }^n v_{im}x_i} \qquad(14)</script><p>此时梯度为<script type="math/tex">x_l\sum_{i \ne l } v_{im}x_i</script>。</p></li></ul><p>综合上述结论，<script type="math/tex">f(x)</script>关于<script type="math/tex">\theta</script>的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial f(x)}{\partial \theta}  =\begin{cases}1,  & \theta=\omega_0 \\x_l, & \theta=\omega_l, l \in (1,2,...,n) \\x_l\sum_{i=1,i \ne l }^n v_{im}x_i & \theta=v_{lm}\end{cases}   \qquad(15)</script><h3 id="1-2-6-训练时间复杂度"><a href="#1-2-6-训练时间复杂度" class="headerlink" title="1.2.6 训练时间复杂度"></a>1.2.6 训练时间复杂度</h3><p>由上述式（15）可以得到：</p><script type="math/tex; mode=display">x_l\sum_{i=1,i \ne l }^n v_{im}x_i = x_l\sum_{i=1}^n v_{im}x_i-v_{lm}x_l^2 \qquad(16)</script><p>对于上式中的前半部分<script type="math/tex">\sum_{i=1}^n v_{im}x_i</script>，对于每个样本只需要计算一次，所以时间复杂度为<script type="math/tex">O(n)</script>，对于k个隐变量的维度分别计算一次，则复杂度为<script type="math/tex">O(kn)</script>。其它项的时间复杂度都小于这一项，因此，模型训练的时间复杂度为<script type="math/tex">O(kn)</script>。</p><p>详细一点解释：<br>（1）我们首先计算<script type="math/tex">\sum_{i=1}^n v_{im}x_i</script>，时间复杂度为n，这个值对于所有特征对应的隐变量的某一个维度是相同的。我们设这值为C。</p><p>（2）计算每一个特征对应的<script type="math/tex">x_l\sum_{i=1}^n v_{im}x_i-v_{lm}x_l^2 =Cx_l-v_{lm}x_l^2</script>，由于总共有n个特征，因此时间复杂度为n，至此，总的时间复杂度为n+n。</p><p>（3）上述只是计算了隐变量的其中一个维度，我们总共有k个维度，因此总的时间复杂度为<script type="math/tex">k(n+n)=O(kn)</script>.</p><h1 id="四、FFM-Field-Factorization-Machine"><a href="#四、FFM-Field-Factorization-Machine" class="headerlink" title="四、FFM(Field Factorization Machine)"></a>四、FFM(Field Factorization Machine)</h1><h2 id="2-1-背景及基本原理"><a href="#2-1-背景及基本原理" class="headerlink" title="2.1 背景及基本原理"></a>2.1 背景及基本原理</h2><p>在FM模型中，每一个特征会对应一个隐变量，但在FFM模型中，认为应该将特征分为多个field，每个特征对应每个field分别有一个隐变量。</p><p>举个例子，我们的样本有3种类型的字段：publisher, advertiser, gender，分别可以代表媒体，广告主或者是具体的商品，性别。其中publisher有5种数据，advertiser有10种数据，gender有男女2种，经过one-hot编码以后，每个样本有17个特征，其中只有3个特征非空。</p><p>如果使用FM模型，则17个特征，每个特征对应一个隐变量。<br>如果使用FFM模型，则17个特征，每个特征对应3个隐变量，即每个类型对应一个隐变量，具体而言，就是对应publisher, advertiser, gender三个field各有一个隐变量。</p><h2 id="2-2模型与最优化问题"><a href="#2-2模型与最优化问题" class="headerlink" title="2.2模型与最优化问题"></a>2.2模型与最优化问题</h2><h3 id="2-2-1-模型"><a href="#2-2-1-模型" class="headerlink" title="2.2.1 模型"></a>2.2.1 模型</h3><p>根据上面的描述，可以得出FFM的模型为：</p><script type="math/tex; mode=display">f(x) = \omega_0+\sum_{i=1}^n\omega_ix_i+\sum_{j1=1}^{n-1}\sum_{j2=j1+1}^n(V_{j1,f2}^TV_{j2,f1})x_{j1}x_{j2} \qquad(17)</script><p>其中<script type="math/tex">j1,j2</script>表示特征的索引。我们假设<script type="math/tex">j1</script>特征属于<script type="math/tex">f1</script>这个field，<script type="math/tex">j2</script>特征属于<script type="math/tex">f2</script>这个field，则<script type="math/tex">V_{j1,f2}</script>表示<script type="math/tex">j1</script>这个特征对应<script type="math/tex">f2</script>(<script type="math/tex">j2</script>所属的field)的隐变量，同时<script type="math/tex">V_{j2,f1}</script>表示<script type="math/tex">j2</script>这个特征对应<script type="math/tex">f1</script>(<script type="math/tex">j1</script>所属的field)的隐变量。</p><p>事实上，在大多数情况下，FFM模型只保留了二次项，即：</p><script type="math/tex; mode=display">\phi(V,x) =  \sum_{j1=1}^{n-1}\sum_{j2=j1+1}^n(V_{j1,f2}^TV_{j2,f1})x_{j1}x_{j2} \qquad(18)</script><h3 id="2-2-2-最优化问题"><a href="#2-2-2-最优化问题" class="headerlink" title="2.2.2 最优化问题"></a>2.2.2 最优化问题</h3><p>根据逻辑回归的损失函数及分析，可以得出FFM的最优化问题为：</p><script type="math/tex; mode=display">\min \frac{\lambda}{2}||V||_2^2+\sum_{i=1}^{m}\log(1+exp(-y_i\phi(V,x))) \qquad(19)</script><p>上面加号的前面部分使用了L2范式，后面部分是逻辑回归的损失函数。m表示样本的数量，<script type="math/tex">y_i</script>表示训练样本的真实值（如是否点击的-1/1），<script type="math/tex">\phi(V,x)</script>表示使用当前的V代入式（18）计算得到的值。</p><p><strong>注意，以上的损失函数适用于样本分布为{-1,1}的情况。</strong></p><h3 id="2-2-3-自适应学习率"><a href="#2-2-3-自适应学习率" class="headerlink" title="2.2.3 自适应学习率"></a>2.2.3 自适应学习率</h3><p>与FTRL一样，FFM也使用了累积梯度作为学习率的一部分，即：</p><script type="math/tex; mode=display">V_{j1,f2} = V_{j1,f2} - \frac{\eta}{\sqrt{1+\sum_t(g_{v_{j1,f2}}^t)^2}}g_{v_{j1,f2}} \qquad(20)</script><p>其中<script type="math/tex">g_{v_{j1,f2}}</script>表示对于<script type="math/tex">V_{j1,f2}</script>这个变量的梯度向量，因为<script type="math/tex">V_{j1,f2}</script>是一个向量，因此<script type="math/tex">g_{v_{j1,f2}}</script>也是一个向量，尺寸为隐变量的维度大小，即k。<br>而<script type="math/tex">\sum_t(g_{v_{j1,f2}}^t)^2</script>表示从第一个样本到当前样本一直以来的累积梯度平方和。</p><h3 id="2-2-4-FFM算法的最终形式"><a href="#2-2-4-FFM算法的最终形式" class="headerlink" title="2.2.4 FFM算法的最终形式"></a>2.2.4 FFM算法的最终形式</h3><script type="math/tex; mode=display">\begin{aligned}(V_{j1,f2})_d &=(V_{j1,f2})_{d-1}-\frac{\eta}{\sqrt{(G_{j1,f2})_d}} \cdot (g_{j1,f2})_d \\(V_{j2,f1})_d &=(V_{j2,f1})_{d-1}-\frac{\eta}{\sqrt{(G_{j2,f1})_d}} \cdot (g_{j2,f1})_d\end{aligned}</script><p>其中G为累积梯度平方：</p><script type="math/tex; mode=display">(G_{j1,f2})_d=(G_{j1,f2})_{d-1}+(g_{j1,f2})_d^2(G_{j2,f1})_d=(G_{j2,f1})_{d-1}+(g_{j2,f1})_d^2</script><p>g为梯度，比如<script type="math/tex">g_{j1,f2}</script>为<script type="math/tex">j1</script>这个特征对应<script type="math/tex">f2</script>这个field的梯度向量：</p><script type="math/tex; mode=display">g_{ji,f2}=\lambda \cdot V_{ji,f2} + \kappa \cdot V_{j2,f1}g_{j2,f1}=\lambda \cdot V_{j2,f1} + \kappa \cdot V_{j1,f2}</script><p>其中<script type="math/tex">\kappa</script>为：</p><script type="math/tex; mode=display">\kappa=\frac{\partial\log(1+exp(-y_i\phi(V,x))) }{\partial \phi(V,x) }=\frac{-y}{1+\exp(y\phi(V,x) )}</script><h2 id="2-3完整算法流程"><a href="#2-3完整算法流程" class="headerlink" title="2.3完整算法流程"></a>2.3完整算法流程</h2><p>使用随机梯度下降（SGD）训练FFM模型的完整过程如下：</p><h3 id="2-3-1-计算梯度"><a href="#2-3-1-计算梯度" class="headerlink" title="2.3.1 计算梯度"></a>2.3.1 计算梯度</h3><p>对于每一个样本的每一对特征组合都要计算以下梯度向量。</p><script type="math/tex; mode=display">\begin{aligned}g_{ji,f2}=\lambda \cdot V_{ji,f2} + \kappa \cdot V_{j2,f1} \\g_{j2,f1}=\lambda \cdot V_{j2,f1} + \kappa \cdot V_{j1,f2}\end{aligned}\qquad(21)</script><p>其中<script type="math/tex">\kappa</script>为式(19)后半部分对应的梯度，即：</p><script type="math/tex; mode=display">\kappa=\frac{\partial\log(1+exp(-y_i\phi(V,x))) }{\partial \phi(V,x) }=\frac{-y}{1+\exp(y\phi(V,x) )} \qquad(22)</script><p>再重申一次，<script type="math/tex">g</script>与<script type="math/tex">V</script>都是k维的向量，在python中可以作为一个向量计算，在java/c++等需要通过一个循环进行计算。</p><p>详细推导（21）式如下：<br>（1）在SGD中，式（19）可以转化为：</p><script type="math/tex; mode=display">\min \frac{\lambda}{2}||V||_2^2+\log(1+exp(-y_i\phi(V,x))) \qquad(23)</script><p>（2）上式对<script type="math/tex">V_{j1,f2}</script>求偏导，可得：</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\partial \frac{\lambda}{2}||V||_2^2+\log(1+exp(-y_i\phi(V,x)))}{\partial V_{j1,f2}} \\&=\lambda \cdot V_{j1,f2} + \frac{\partial \log(1+exp(-y_i\phi(V,x)))}{\partial V_{j1,f2}}\\&=\lambda \cdot V_{j1,f2} + \frac{\partial \log(1+exp(-y_i\phi(V,x)))}{\partial \phi} \cdot  \frac{\partial \phi}{V_{j1,f2}}\\&=\lambda \cdot V_{j1,f2} + \frac{-y}{1+\exp(y\phi(V,x) )}  \cdot V_{j2,f1}\end{aligned}\qquad(24)</script><h3 id="2-3-2-计算累积梯度平方和"><a href="#2-3-2-计算累积梯度平方和" class="headerlink" title="2.3.2 计算累积梯度平方和"></a>2.3.2 计算累积梯度平方和</h3><p>计算从第一个样本，到当前样本（第d个）以来的累积梯度平方和：</p><script type="math/tex; mode=display">\begin{aligned}(G_{j1,f2})_d=(G_{j1,f2})_{d-1}+(g_{j1,f2})_d^2 \\(G_{j2,f1})_d=(G_{j2,f1})_{d-1}+(g_{j2,f1})_d^2\end{aligned}\qquad(26)</script><h3 id="2-3-3-更新隐变量"><a href="#2-3-3-更新隐变量" class="headerlink" title="2.3.3 更新隐变量"></a>2.3.3 更新隐变量</h3><script type="math/tex; mode=display">\begin{aligned}(V_{j1,f2})_d=(V_{j1,f2})_{d-1}-\frac{\eta}{\sqrt{(G_{j1,f2})_d}} \cdot (g_{j1,f2})_d\\(V_{j2,f1})_d=(V_{j2,f1})_{d-1}-\frac{\eta}{\sqrt{(G_{j2,f1})_d}} \cdot (g_{j2,f1})_d\end{aligned}\qquad(26)</script><h3 id="2-3-4-关于初始参数的设定"><a href="#2-3-4-关于初始参数的设定" class="headerlink" title="2.3.4 关于初始参数的设定"></a>2.3.4 关于初始参数的设定</h3><p>文献1中如此建议：<br>（1）<script type="math/tex">\eta</script>：没有具体的建议，用户根据经验指定即可，一般会取0.1，0.01，0.001。</p><p>（2）<script type="math/tex">V</script>：在区间<script type="math/tex">[0,1/\sqrt{k}]</script>间的随机值，均匀分布即可。</p><p>（3）<script type="math/tex">G</script>：设置为1，以避免<script type="math/tex">(G_{j1,f2})_d^{-\frac{1}{2}}</script>出现很大的值。</p><h2 id="2-4-时间复杂度"><a href="#2-4-时间复杂度" class="headerlink" title="2.4 时间复杂度"></a>2.4 时间复杂度</h2><h3 id="2-4-1-计算时间复杂度"><a href="#2-4-1-计算时间复杂度" class="headerlink" title="2.4.1 计算时间复杂度"></a>2.4.1 计算时间复杂度</h3><p>由于式(18)无法做类似于式（10）的简化，因此FFM的计算时间复杂度为<script type="math/tex">O(kn^2)</script>。</p><h3 id="2-4-2-训练时间复杂度"><a href="#2-4-2-训练时间复杂度" class="headerlink" title="2.4.2 训练时间复杂度"></a>2.4.2 训练时间复杂度</h3><p>由于训练时，需要先根据式（18）计算<script type="math/tex">\phi</script>，复杂度为<script type="math/tex">O(kn^2)</script>，计算得到<script type="math/tex">\phi</script>后，还需要按照式（22）计算1次，按照式（21）计算2k次，按照式（23）计算2k次，按照式（24）计算2k次，也就是说，总的训练时间复杂度为：</p><script type="math/tex; mode=display">O(kn^2) + 1 + 2k + 2k + 2k = O(kn^2)</script><p>因此，训练时间复杂度为<script type="math/tex">O(kn^2)</script>。</p><h2 id="2-5-计算速度优化"><a href="#2-5-计算速度优化" class="headerlink" title="2.5 计算速度优化"></a>2.5 计算速度优化</h2><h3 id="2-5-1-openMP"><a href="#2-5-1-openMP" class="headerlink" title="2.5.1 openMP"></a>2.5.1 openMP</h3><p>OpenMP提供的这种对于并行描述的高层抽象降低了并行编程的难度和复杂度，这样程序员可以把更多的精力投入到并行算法本身，而非其具体实现细节。对基于数据分集的多线程程序设计，OpenMP是一个很好的选择。同时，使用OpenMP也提供了更强的灵活性，可以较容易的适应不同的并行系统配置。线程粒度和负载平衡等是传统多线程程序设计中的难题，但在OpenMP中，OpenMP库从程序员手中接管了部分这两方面的工作。</p><p>openPM原生支持C/C++/Fortran，但java可以通过jomp等引入，未测试。</p><h3 id="2-5-2-SSE3"><a href="#2-5-2-SSE3" class="headerlink" title="2.5.2 SSE3"></a>2.5.2 SSE3</h3><p>SSE3 中13个新指令的主要目的是改进线程同步和特定应用程序领域，例如媒体和游戏。这些新增指令强化了处理器在浮点转换至整数、复杂算法、视频编码、SIMD浮点寄存器操作以及线程同步等五个方面的表现，最终达到提升多媒体和游戏性能的目的。Intel是从Prescott核心的Pentium 4开始支持SSE3指令集的，而AMD则是从2005年下半年Troy核心的Opteron开始才支持SSE3的。但是需要注意的是，AMD所支持的SSE3与Intel的SSE3并不完全相同，主要是删除了针对Intel超线程技术优化的部分指令。<br>SSE3指令采用128位的寄存器，可以同时操作4个单精度浮点数或者整数，因此非常类似于向量运算。这对于有大量向量计算的的FFM模型是有用的。<br>但事实上，计算<script type="math/tex">\phi</script>是几乎无用，而这是最耗时间的部分。</p><h3 id="2-5-3-ParameterServer"><a href="#2-5-3-ParameterServer" class="headerlink" title="2.5.3 ParameterServer"></a>2.5.3 ParameterServer</h3><p><a href="https://www.zybuluo.com/Dounm/note/517675" target="_blank" rel="noopener">https://www.zybuluo.com/Dounm/note/517675</a><br>Paraeter Server框架中，每个server都只负责分到的部分参数（server共同维持一个全局共享参数）。server节点可以和其他server节点通信，每个server负责自己分到的参数，server group共同维持所有参数的更新。server manage node负责维护一些元数据的一致性，例如各个节点的状态，参数的分配情况。</p><h2 id="2-6模型优化"><a href="#2-6模型优化" class="headerlink" title="2.6模型优化"></a>2.6模型优化</h2><h3 id="2-6-1-特征编码连续"><a href="#2-6-1-特征编码连续" class="headerlink" title="2.6.1 特征编码连续"></a>2.6.1 特征编码连续</h3><p>如果特征的编码不连续，比如编码是有意义的，或者预留空间给之后的编码。如果直接使用最大编码值的作为参数数据尺寸，则会导致大量内存空间的浪费，因此有2种解决方案：<br>（1）使用hashmap，而非数组。<br>（2）将有意义的编码映射到一个连续的编码空间。<br>目前我们使用方式（1），理论上方式（2）的计算速度会更快。</p><h3 id="2-6-2-一次项缺失的影响"><a href="#2-6-2-一次项缺失的影响" class="headerlink" title="2.6.2 一次项缺失的影响"></a>2.6.2 一次项缺失的影响</h3><p>正如上面式（18）所言，我们经常会忽略了一次项的影响，因此我们可以为每个样本加上一个辅助特征，这项特征的值恒为1，这相当于引入了一次项。</p><h3 id="2-6-3-样本归一化"><a href="#2-6-3-样本归一化" class="headerlink" title="2.6.3 样本归一化"></a>2.6.3 样本归一化</h3><p>文献1还建议，将样本向量的长度归一化后，性能有少量的提升。</p><script type="math/tex; mode=display">R[i]=\frac{1}{||X||}</script><h3 id="2-6-4-特征归一化"><a href="#2-6-4-特征归一化" class="headerlink" title="2.6.4 特征归一化"></a>2.6.4 特征归一化</h3><p>某些特征（如购买个数）有可能很大，而一些类别参数则恒为1，这将导致不同特征最终对模型的影响相关很大，这很不合理。</p><h1 id="五、DEEP-amp-WIDE"><a href="#五、DEEP-amp-WIDE" class="headerlink" title="五、DEEP &amp; WIDE"></a>五、DEEP &amp; WIDE</h1>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CTR预估 </tag>
            
            <tag> 算法介绍 </tag>
            
            <tag> 协同过滤 </tag>
            
            <tag> LR </tag>
            
            <tag> FM </tag>
            
            <tag> FFM </tag>
            
            <tag> DEEP &amp; WIDE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之python</title>
      <link href="/2019/10/15/shi-yong-gong-ju-zhi-python/"/>
      <url>/2019/10/15/shi-yong-gong-ju-zhi-python/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/15/shi-yong-gong-ju-zhi-python/image-20210621144956956.png" alt="image-20210621144956956"></p><h1 id="一、Python基础"><a href="#一、Python基础" class="headerlink" title="一、Python基础"></a>一、Python基础</h1><h1 id="二、Python进阶"><a href="#二、Python进阶" class="headerlink" title="二、Python进阶"></a>二、Python进阶</h1><h1 id="三、参考书籍"><a href="#三、参考书籍" class="headerlink" title="三、参考书籍"></a>三、参考书籍</h1><h1 id="四、疑难解答"><a href="#四、疑难解答" class="headerlink" title="四、疑难解答"></a>四、疑难解答</h1><h2 id="1、编码问题"><a href="#1、编码问题" class="headerlink" title="1、编码问题"></a>1、编码问题</h2><h3 id="Python2编码问题"><a href="#Python2编码问题" class="headerlink" title="Python2编码问题"></a><code>Python2</code>编码问题</h3><ul><li><a href="https://foofish.net/why-Python-encoding-is-tricky.html" target="_blank" rel="noopener">参考</a></li></ul><h3 id="Python3编码问题"><a href="#Python3编码问题" class="headerlink" title="Python3编码问题"></a><code>Python3</code>编码问题</h3><ul><li>在<code>Python3</code>版本中，把<code>&#39;xxx&#39;</code>和<code>u&#39;xxx&#39;</code>统一成<code>Unicode</code>编码，即写不写前缀<code>u</code>都是一样的。</li><li>在<code>Python3</code>版本中，所有的字符串都是使用<code>Unicode</code>编码的字符串序列。</li><li><a href="https://foofish.net/how-Python3-handle-charset-encoding.html" target="_blank" rel="noopener">参考</a></li></ul><h2 id="2、日期操作"><a href="#2、日期操作" class="headerlink" title="2、日期操作"></a>2、日期操作</h2><ul><li><a href="https://www.runoob.com/python/python-date-time.html" target="_blank" rel="noopener">参考菜鸟教程</a></li></ul><h3 id="产生一段时间的日期"><a href="#产生一段时间的日期" class="headerlink" title="产生一段时间的日期"></a>产生一段时间的日期</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">from datetime import datetime, date, timedeltaimport pandas as pddate_id_list = [datetime.strftime(x, '%Y%m%d') for x in list(pd.date_range(start='20190701', end='20190928'))]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="获取指定日期的前-后N天"><a href="#获取指定日期的前-后N天" class="headerlink" title="获取指定日期的前-后N天"></a>获取指定日期的前-后N天</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">import datetimen = 1tomorrow = datetime.datetime(2015, 10, 28) + datetime.timedelta(days=1)    # 2015-10-29 00:00:00tomorrow_format = tomorrow.strftime('%Y%m%d')    # '20151029'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、mrjob运行参数详情"><a href="#3、mrjob运行参数详情" class="headerlink" title="3、mrjob运行参数详情"></a>3、mrjob运行参数详情</h2><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">python xxx.py -r hadoop --local-tmp-dir 'xxx' --hadoop-tmp-dir 'hdfs:xxx' --file 'xxx.txt' --jobconf mapred.map.tasks=20 --jobconf mapred.reduce.tasks=2 input.txt -o output_dir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>参考官方文档：</strong><a href="https://mrjob.readthedocs.io/en/latest/" target="_blank" rel="noopener">mrjob</a></p><h2 id="4、list中排列组合"><a href="#4、list中排列组合" class="headerlink" title="4、list中排列组合"></a>4、list中排列组合</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">from itertools import combinationscombine_2 = list(combinations([1,2,3,4], 2))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="5、文件操作"><a href="#5、文件操作" class="headerlink" title="5、文件操作"></a>5、文件操作</h2><h3 id="获取指定目录下指定文件"><a href="#获取指定目录下指定文件" class="headerlink" title="获取指定目录下指定文件"></a>获取指定目录下指定文件</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">import osL = []for root, dirs, files in os.walk(os.getcwd()):    for x in files:        if os.path.splitext(x)[1] == '.txt':            L.append(os.path.join(root, x))file_path = L[0]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="pandas读取excel文件"><a href="#pandas读取excel文件" class="headerlink" title="pandas读取excel文件"></a>pandas读取excel文件</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">roc_data = pd.read_excel(file_path, sheet_name='20200323_10000')roc_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="6、命令行参数sys-argv-1-解析"><a href="#6、命令行参数sys-argv-1-解析" class="headerlink" title="6、命令行参数sys.argv[1:]解析"></a>6、命令行参数<code>sys.argv[1:]</code>解析</h2><ul><li><a href="https://www.cnblogs.com/itech/archive/2010/12/31/1919017.html" target="_blank" rel="noopener">python类库31—命令行解析</a></li></ul><h3 id="手动解析"><a href="#手动解析" class="headerlink" title="手动解析"></a>手动解析</h3><h3 id="getopt解析"><a href="#getopt解析" class="headerlink" title="getopt解析"></a>getopt解析</h3><h3 id="optionparser解析【推荐】"><a href="#optionparser解析【推荐】" class="headerlink" title="optionparser解析【推荐】"></a>optionparser解析【推荐】</h3><h2 id="7、字典排序"><a href="#7、字典排序" class="headerlink" title="7、字典排序"></a>7、字典排序</h2><pre class="line-numbers language-lang-python"><code class="language-lang-python">result = {}sorted(result.items(), key=lambda x: x[1], reverse=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="8、python自动登录Linux等服务"><a href="#8、python自动登录Linux等服务" class="headerlink" title="8、python自动登录Linux等服务"></a>8、python自动登录Linux等服务</h2><p><a href="https://pexpect.readthedocs.io/en/stable/overview.html" target="_blank" rel="noopener">参考</a></p><pre class="line-numbers language-lang-python"><code class="language-lang-python">import pexpectchild = pexpect.spawn('ftp ftp.openbsd.org')child.expect('Name .*: ')child.sendline('anonymous')child.expect('Password:')child.sendline('noah@example.com')child.expect('ftp> ')child.sendline('lcd /tmp')child.expect('ftp> ')child.sendline('cd pub/OpenBSD')child.expect('ftp> ')child.sendline('get README')child.expect('ftp> ')child.sendline('bye')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="9、-生成requirements-txt"><a href="#9、-生成requirements-txt" class="headerlink" title="9、 生成requirements.txt"></a>9、 生成requirements.txt</h2><h3 id="第一种方法：太多太杂，对整个虚拟环境的"><a href="#第一种方法：太多太杂，对整个虚拟环境的" class="headerlink" title="第一种方法：太多太杂，对整个虚拟环境的"></a>第一种方法：太多太杂，对整个虚拟环境的</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python"># 生成pip freeze > requirements.txt# 安装pip install -r requirements.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="第二种方法：需要pip安装模块，但是可以对指定目录进行生成"><a href="#第二种方法：需要pip安装模块，但是可以对指定目录进行生成" class="headerlink" title="第二种方法：需要pip安装模块，但是可以对指定目录进行生成"></a>第二种方法：需要pip安装模块，但是可以对指定目录进行生成</h3><pre><code># pip 安装模块pip3 install pipreqs# 对指定目录进行生成requirements.txtcd 到指定目录pipreqs ./ --encoding=utf8# 这样在指定目录就会有requirements.txt的依赖文件</code></pre><h2 id="10、python取mysql中文乱码"><a href="#10、python取mysql中文乱码" class="headerlink" title="10、python取mysql中文乱码"></a>10、python取mysql中文乱码</h2><ol><li>mysql代码中将中文字段用<code>hex</code>函数转换</li><li>python代码中用<code>bytes.fromhex(取出的字段).decode(&#39;utf-8&#39;)</code>来转换</li></ol><h2 id="11、更新所有模块"><a href="#11、更新所有模块" class="headerlink" title="11、更新所有模块"></a>11、更新所有模块</h2><ul><li><a href="https://pypi.org/project/pip-review/" target="_blank" rel="noopener">参考</a></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">pip3 install pip-reviewpip-review --interactive<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="12、多进程与多线程"><a href="#12、多进程与多线程" class="headerlink" title="12、多进程与多线程"></a>12、多进程与多线程</h2><ul><li><a href="https://github.com/jackfrued/Python-100-Days/blob/master/Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md" target="_blank" rel="noopener">参考</a></li><li>例子1：多进程</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">    def processTask(self, role_data, index):        print('启动计算子进程，进程号[%d].' % os.getpid())        print(role_data.shape)        # post请求参数        url = ''        headers = {            'game_code': '',  # 游戏代号            'secret': '',  # secret key，线下提供            'Content-Type': 'application/json'        }        # 推荐结果写入结果文件        outputPath = 'g_role_item_test_result_multi_%s.txt' % str(index)        with open(outputPath, mode='w', encoding='utf-8') as f:            role_data.reset_index(drop=True, inplace=True)            for i in range(role_data.shape[0]):                role_id = role_data.role_id[i]                gender = role_data.gender[i]                purchased_items = role_data.item_set[i].replace('[', '').replace(']', '').replace('"', '').split(',')                current_amount = role_data.left_yuanbao[i]                argsData = {                    "role_id": role_id,  # 角色id                    "gender": int(gender) + 1,  # 角色性别：1-男，2-女                    "purchased_items": purchased_items,  # 角色已购买的道具id列表, e.g. ["123", "321"],                    "current_amount": int(current_amount)  # 角色当前账户剩余元宝                }                # print(i, argsData)                t1 = time.time()                try:                    result = self.getRecommendResultInit(url, headers, argsData)                except:                    result = []                t_diff = time.time() - t1                # 输出结果                result1 = {}                for i, x in enumerate(result):                    result1['R' + str(i)] = x                output1 = [role_id, json.dumps(argsData), str(round(t_diff, 5))] + [json.dumps(result1)]                f.write('\t'.join(output1) + '\n')    def getRecommendResultMulti(self):        """        给定数据，从接口获取推荐结果，多进程        :return:        """        # data参数获取        inputPath = "g_role_item_test_data.csv"        role_data = pd.read_csv(inputPath, sep=',', encoding='utf-8')        role_data.fillna('[]', inplace=True)        print(role_data.shape)        # 多进程        print('启动计算母进程，进程号[%d].' % os.getpid())        index = 0        processes = []        for _ in range(4):            p = Process(target=self.processTask,                        args=(role_data.loc[index:index+70000], index))            index += 70000            processes.append(p)            p.start()            # 开始记录所有进程执行完成花费的时间        start = time.time()        for p in processes:            p.join()        end = time.time()        print('Execution time: ', (end - start), 's', sep='')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>例子2：多线程</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">import timeimport tkinterimport tkinter.messageboxfrom threading import Threaddef main():    class DownloadTaskHandler(Thread):        def run(self):            time.sleep(10)            tkinter.messagebox.showinfo('提示', '下载完成!')            # 启用下载按钮            button1.config(state=tkinter.NORMAL)    def download():        # 禁用下载按钮        button1.config(state=tkinter.DISABLED)        # 通过daemon参数将线程设置为守护线程(主程序退出就不再保留执行)        # 在线程中处理耗时间的下载任务        DownloadTaskHandler(daemon=True).start()    def show_about():        tkinter.messagebox.showinfo('关于', '作者: 骆昊(v1.0)')    top = tkinter.Tk()    top.title('单线程')    top.geometry('200x150')    top.wm_attributes('-topmost', 1)    panel = tkinter.Frame(top)    button1 = tkinter.Button(panel, text='下载', command=download)    button1.pack(side='left')    button2 = tkinter.Button(panel, text='关于', command=show_about)    button2.pack(side='right')    panel.pack(side='bottom')    tkinter.mainloop()if __name__ == '__main__':    main()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="13、文本相似度"><a href="#13、文本相似度" class="headerlink" title="13、文本相似度"></a>13、文本相似度</h2><ol><li>$similarity=\frac{|A\cap B|}{|A\cup B|}$</li><li>修正，对长度做出惩罚：$similarity=\frac{|A\cap B|}{|A\cup B|+\alpha \times abs{(len(A)-len(B)})}$</li></ol><h2 id="14、one-hot-编码-multiple-values"><a href="#14、one-hot-编码-multiple-values" class="headerlink" title="14、one hot 编码=multiple values"></a>14、one hot 编码=multiple values</h2><ul><li><a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.str.get_dummies.html#pandas.Series.str.get_dummies" target="_blank" rel="noopener">pandas-Series.str.get_dummiers</a></li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">pd.Series(['80001,800002', '150001,150002', '80001,80002,150001,150002']).str.get_dummies(sep=',')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer" target="_blank" rel="noopener">sklearn-MultiLabelBinarizer</a></li><li><a href="https://stackoverflow.com/questions/63544536/convert-pd-get-dummies-result-to-df-str-get-dummies" target="_blank" rel="noopener">one-hot过后压缩矩阵（稀疏矩阵）</a></li></ul><h2 id="15、pandas使用apply返回多列"><a href="#15、pandas使用apply返回多列" class="headerlink" title="15、pandas使用apply返回多列"></a>15、pandas使用apply返回多列</h2><ul><li><a href="https://stackoverflow.com/questions/23586510/return-multiple-columns-from-pandas-apply" target="_blank" rel="noopener">参考</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程语言 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之Linux</title>
      <link href="/2019/10/14/shi-yong-gong-ju-zhi-linux/"/>
      <url>/2019/10/14/shi-yong-gong-ju-zhi-linux/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Linux基础"><a href="#一、Linux基础" class="headerlink" title="一、Linux基础"></a>一、Linux基础</h1><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h3><ul><li><a href="https://man.linuxde.net/rsync" target="_blank" rel="noopener">参考</a></li></ul><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">rsync [OPTION]... SRC DESTrsync [OPTION]... SRC [USER@]host:DESTrsync [OPTION]... [USER@]HOST:SRC DESTrsync [OPTION]... [USER@]HOST::SRC DESTrsync [OPTION]... SRC [USER@]HOST::DESTrsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对应于以上六种命令格式，rsync有六种不同的工作模式：</p><ol><li>拷贝本地文件。当SRC和DES路径信息都不包含有单个冒号”:”分隔符时就启动这种工作模式。如：<code>rsync -a /data /backup</code></li><li>使用一个远程shell程序(如<a href="http://man.linuxde.net/rsh" target="_blank" rel="noopener">rsh</a>、<a href="http://man.linuxde.net/ssh" target="_blank" rel="noopener">ssh</a>)来实现将本地机器的内容拷贝到远程机器。当DST路径地址包含单个冒号”:”分隔符时启动该模式。如：<code>rsync -avz *.c foo:src</code></li><li>使用一个远程shell程序(如rsh、ssh)来实现将远程机器的内容拷贝到本地机器。当SRC地址路径包含单个冒号”:”分隔符时启动该模式。如：<code>rsync -avz foo:src/bar /data</code></li><li>从远程rsync服务器中拷贝文件到本地机。当SRC路径信息包含”::”分隔符时启动该模式。如：<code>rsync -av root@192.168.78.192::www /databack</code></li><li>从本地机器拷贝文件到远程rsync服务器中。当DST路径信息包含”::”分隔符时启动该模式。如：<code>rsync -av /databack root@192.168.78.192::www</code></li><li>列远程机的文件列表。这类似于rsync传输，不过只要在命令中省略掉本地机信息即可。如：<code>rsync -v rsync://192.168.78.192/www</code></li></ol><h4 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。-l, --links 保留软链结。-L, --copy-links 想对待常规文件一样处理软链结。--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。--safe-links 忽略指向SRC路径目录树以外的链结。-H, --hard-links 保留硬链结。-p, --perms 保持文件权限。-o, --owner 保持文件属主信息。-g, --group 保持文件属组信息。-D, --devices 保持设备文件信息。-t, --times 保持文件时间信息。-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, --dry-run现实哪些文件将被传输。-w, --whole-file 拷贝文件，不进行增量检测。-x, --one-file-system 不要跨越文件系统边界。-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。--delete 删除那些DST中SRC没有的文件。--delete-excluded 同样删除接收端那些被该选项指定排除的文件。--delete-after 传输结束以后再删除。--ignore-errors 及时出现IO错误也进行删除。--max-delete=NUM 最多删除NUM个文件。--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。--force 强制删除目录，即使不为空。--numeric-ids 不将数字的用户和组id匹配为用户名和组名。--timeout=time ip超时时间，单位为秒。-I, --ignore-times 不跳过那些有同样的时间和长度的文件。--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T --temp-dir=DIR 在DIR中创建临时文件。--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 --partial。--progress 显示备份过程。-z, --compress 对备份的文件在传输时进行压缩处理。--exclude=PATTERN 指定排除不需要传输的文件模式。--include=PATTERN 指定不排除而需要传输的文件模式。--exclude-from=FILE 排除FILE中指定模式的文件。--include-from=FILE 不排除FILE指定模式匹配的文件。--version 打印版本信息。--address 绑定到特定的地址。--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。--port=PORT 指定其他的rsync服务端口。--blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。--progress 在传输时现实传输过程。--log-format=formAT 指定日志文件格式。--password-file=FILE 从FILE中得到密码。--bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, --help 显示帮助信息。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="二、Linux进阶"><a href="#二、Linux进阶" class="headerlink" title="二、Linux进阶"></a>二、Linux进阶</h1><h2 id="linux配置conda环境及搭建深度学习环境"><a href="#linux配置conda环境及搭建深度学习环境" class="headerlink" title="linux配置conda环境及搭建深度学习环境"></a>linux配置conda环境及搭建深度学习环境</h2><h3 id="安装minconda3"><a href="#安装minconda3" class="headerlink" title="安装minconda3"></a>安装minconda3</h3><ul><li><a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">参考</a></li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 安装miniconda3wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh# 校验hash值sha256sum Miniconda3-latest-Linux-x86_64.sh# 更改执行权限chmod +x Miniconda3-latest-Linux-x86_64.sh# 安装bash Miniconda3-latest-Linux-x86_64.sh# 安装完成后配置任意位置启动condasource <path to conda>/bin/activateconda initconda deactivatesource ~/.bashrcconda config --set auto_activate_base False# 验证conda --version# 装好conda之后，确定一个你放软件的地方mkdir ~/conda_softwareconda create -p ~/conda_software# 配置环境变量，可以加到.bashrc或者.zshrc文件，这样每次登录不用再设置export PATH=$HOME/conda_software/bin${PATH:+:${PATH}}# 安装软件conda activate ~/softwares/conda_softwareconda config --add channels conda-forge# 以下举几个例子：# conda install zsh# conda install htop# conda install tmux# conda install openssl# 查看安装了哪些包conda list# 查看当前存在哪些虚拟环境conda env list# conda info -e# 检查更新conda update conda# 创建python虚拟环境conda create -n python38 python=3.8# 激活conda activate python38# 安装包# 指定环境安装包conda install -n python38 numpy# 在环境内安装包conda install numpy# 关闭虚拟环境conda deactivate# 删除虚拟环境conda remove -n python38 --all# 删除环境中的包conda remove -n python38 numpy# 设置国内镜像conda config --add channels https://mirrors.tuna.tsinghua.edu.cn# 设置搜索时显示通道地址conda config --set show_channel_urls yes# 恢复默认镜像conda config --remove-key channels<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="配置服务器jupyter环境"><a href="#配置服务器jupyter环境" class="headerlink" title="配置服务器jupyter环境"></a>配置服务器jupyter环境</h3><ul><li><a href="https://blog.lihj.me/post/conda-jupyter-installation.html" target="_blank" rel="noopener">参考</a></li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 安装jupyter labconda install jupyterlab# jupyter 配置# 生成配置文件jupyter notebook --generate-configipython# ipythonfrom notebook.auth import passwdpasswd()# 复制生成的密文# 修改默认配置文件vim ~/.jupyter/jupyter_notebook_config.py# 修改以下内容c.NotebookApp.allow_remote_access = Truec.NotebookApp.ip = '*'c.NotebookApp.open_browser = Falsec.NotebookApp.password = u'sha1:...刚才复制的密文'c.NotebookApp.port = 8888 # 指定一个访问端口# 启动jupyter# jupyter notebookjupyter lab# 远程访问# 在本地浏览器输入 address_of_remote:8888 进入jupyter 的登陆界面# 安装 jupyter 使用的 python 可直接被 jupyter 调用，不需额外配置。# 为了使用其他语言或者新环境中的某种语言，需要单独安装该语言的 jupyter kernel 供 jupyter 调用# 进入环境conda activate python38# 安装依赖包conda install notebook ipykernel# 安装python kernelwhich ipython # should show ~/miniconda3/envs/python38/bin/ipythonipython kernel install --user --name "python38" --display-name "Python38"# kernel 文件保存在 ~/.local/share/python38/kernels/python37。# 此时安装的是新环境中 ipython 所属的 python 3.8 (~/miniconda3/envs/python38/bin/python) 为 jupyter kernel。# 其他位置或环境的 python 可用相同方法安装为 jupyter kernel<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="升级cuda"><a href="#升级cuda" class="headerlink" title="升级cuda"></a>升级cuda</h3><ul><li><a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">各显卡驱动下载地址</a></li><li><a href="https://developer.nvidia.com/zh-cn/cuda-toolkit" target="_blank" rel="noopener">CUDA下载地址</a></li><li><a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">cudnn下载地址</a>：会让登录和比较慢，耐心等待</li><li><a href="https://www.tensorflow.org/install/source#linux" target="_blank" rel="noopener">tensorflow各版本对比</a></li></ul><h4 id="删除历史cuda版本信息"><a href="#删除历史cuda版本信息" class="headerlink" title="删除历史cuda版本信息"></a>删除历史cuda版本信息</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># sudo rm /etc/apt/sources.list.d/cuda*# sudo apt-get --purge remove "*cublas*" "cuda*" "nsight*" # sudo apt-get --purge remove "*nvidia*"# sudo apt-get autoremove# sudo apt-get autoclean# sudo rm -rf /usr/local/cuda*# 卸载所有N卡驱动sudo apt-get remove --purge nvidia-\*sudo apt-get remove --purge cuda-\*sudo apt-get remove --purge *cudnn*sudo apt autoremovesudo apt-get autoclean# 查看已安装的东西sudo dpkg --list | grep nvidia*ubuntu1604_1.0.0-1_amd64.deb  # 可安装的显卡驱动lspci | grep -i nvidia  # 查看显卡nvidia-smi  # 查看显卡<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Linux系统信息查看"><a href="#Linux系统信息查看" class="headerlink" title="Linux系统信息查看"></a><a href="https://blog.csdn.net/weixin_41010198/article/details/109166131" target="_blank" rel="noopener">Linux系统信息查看</a></h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 查看内核版本cat /proc/versionuname -auname -r# 查看linux版本信息lsb_release -acat /etc/issue# 查看linux是64为还是32位getconf LONG_BITfile /bin/ls# 直接查看系统的架构dpkg --print-architecturearchfile /lib/systemd/systemd# 查看Mint系统对应的Ubuntu系统cat /etc/os-releasecat /etc/upstream-release/lsb-releasegcc --version  # 查看gcc版本<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="知乎教程"><a href="#知乎教程" class="headerlink" title="知乎教程"></a><a href="https://zhuanlan.zhihu.com/p/143429249" target="_blank" rel="noopener">知乎教程</a></h4><ul><li>安装教程来几乎没问题</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 安装显卡驱动sudo add-apt-repository ppa:graphics-drivers  #添加NVIDA显卡驱动库sudo apt updateubuntu-drivers devices  #显示可安装驱动#sudo ubuntu-drivers autoinstall  #让Ubuntu自动帮你选择版本并安装sudo apt install nvidia-driver-450  #安装450驱动sudo rebootnvidia-smi  #查看GPU信息,需先重启# 安装cudawget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pinsudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubsudo add-apt-repository "deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /"sudo apt-get updatesudo apt list cuda* # 查看所有，名字以cuda开头的，可以用apt install安装的packagessudo apt install cuda-toolkit-10-2  #只安装CUDA 10.2#sudo apt install cuda-10-2  #安装CUDA 10.2。包含驱动，版本自动选择。# 安装cudnn# 官网下载符合版本的三个debcd ~/Downloads  #进入下载好的三个文件的路径sudo dpkg -i libcudnn*  #同时安装#sudo dpkg -i libcudnn7_7.6.5.32-1+cuda10.2_amd64.deb  #逐个安装#sudo dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb#sudo dpkg -i libcudnn7-doc_7.6.5.32-1+cuda10.2_amd64.deb# 测试cp -r /usr/src/cudnn_samples_v7/ $HOME  #复制样本文件到$HOME文件夹下cd  $HOME/cudnn_samples_v7/mnistCUDNN  #进入样本目录make clean && make  #编译./mnistCUDNN  #执行cuDNN测试# 输出“Test Passed”说明cuDNN安装成功。# 安装tensorflowpip install tensorflow-gpu==2.4.0# 测试import tensorflow as tftf.test.is_gpu_avaiable()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="tensorflow官网教程：GPU支持"><a href="#tensorflow官网教程：GPU支持" class="headerlink" title="tensorflow官网教程：GPU支持"></a><a href="https://www.tensorflow.org/install/gpu" target="_blank" rel="noopener">tensorflow官网教程：GPU支持</a></h4><ul><li>Ubuntu 16.04(CUDA 11.0)</li></ul><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># Add NVIDIA package repositories# Add HTTPS support for apt-keysudo apt-get install gnupg-curlwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-ubuntu1604.pinsudo mv cuda-ubuntu1604.pin /etc/apt/preferences.d/cuda-repository-pin-600sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub# apt-get install software-properties-commonsudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/ /"# apt-get install apt-transport-https ca-certificatessudo apt-get updatewget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.debsudo apt install ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.debsudo apt-get updatewget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/libnvinfer7_7.1.3-1+cuda11.0_amd64.debsudo apt install ./libnvinfer7_7.1.3-1+cuda11.0_amd64.debsudo apt-get update# Install NVIDIA driver# Issue with driver install requires creating /usr/lib/nvidiasudo mkdir /usr/lib/nvidiaubuntu-drivers devices  # 查看显卡和推荐驱动sudo apt-get install --no-install-recommends nvidia-450# sudo apt-get install --no-install-recommends nvidia-driver-450# Reboot. Check that GPUs are visible using the command: nvidia-smi# Install development and runtime libraries (~4GB)# sudo apt install cuda-toolkit-11-0  #只安装CUDA 10.2# sudo apt install cuda-10-2  #安装CUDA 10.2。包含驱动，版本自动选择。sudo apt-get install --no-install-recommends \    cuda-11-0 \    libcudnn8=8.0.4.30-1+cuda11.0  \    libcudnn8-dev=8.0.4.30-1+cuda11.0# Install TensorRT. Requires that libcudnn7 is installed above.sudo apt-get install -y --no-install-recommends \    libnvinfer7=7.1.3-1+cuda11.0 \    libnvinfer-dev=7.1.3-1+cuda11.0 \    libnvinfer-plugin7=7.1.3-1+cuda11.0 \    libnvinfer-plugin-dev=7.1.3-1+cuda11.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="导师教程"><a href="#导师教程" class="headerlink" title="导师教程"></a>导师教程</h4><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">1、安装CUDA及cuDNNTENSORFLOW对CUDA的要求：https://www.tensorflow.org/install/install_linux?hl=zh-cnnvdia的官方文档：http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#handle-uninstallation（1）tensorflow1.8的版本要求为：* CUDA9.0，不支持8.0* cuDNN7.0.x，不支持7.1.x（2）卸载 CUDA 8.0：如果原来没有安装CUDA8.0，则可以忽略这一步/usr/local/cuda-X.Y/bin/uninstall_cuda_X.Y.pl比如：/usr/local/cuda-8.0/bin/uninstall_cuda_8.0.pl【可选】驱动也一并卸掉，因为新版 CUDA 通常需要装一个新驱动。$ sudo /usr/bin/nvidia-uninstall（3）下载CUDA9.0 与 cuDNN7.0下载 CUDA Toolkit 现在需要注册一个 NVIDIA 官方账号。注册完成后在 https://developer.nvidia.com/cuda-release-candidate-download 按照系统、版本选择要下载的包。cuDNN 的安装类似，地址在 https://developer.nvidia.com/rdp/cudnn-download 。不过官方文档表示 cuDNN 的升级不会冲突，直接安装就好。 （4）安装CUDA9.0sudo dpkg -i cuda-repo-ubuntu1704-9-0-local-rc_9.0.103-1_amd64.debsudo apt-key add /var/cuda-repo-9-0-local-rc/7fa2af80.pubsudo apt-get updatesudo apt-get install cuda在装新 CUDA 的时候系统会安装新版驱动。（5）安装完后运行 nvidia-smi 试一下，如果提示 mismatch 就重启。 （6）配置环境变量$ export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}$ export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}（7）安装cuDNN7.0.xdpkg -i libcudnn7_7.0.5.15-1+cuda9.0_amd64.deb（8）安装cuda-command-line-toolsapt-get install cuda-command-line-tools2、安装tensorflowpip3 install tensorflow-gpu<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="安装时遇到的问题"><a href="#安装时遇到的问题" class="headerlink" title="安装时遇到的问题"></a>安装时遇到的问题</h4><ol><li><a href="https://blog.csdn.net/missyoudaisy/article/details/104432746" target="_blank" rel="noopener">nvidia-smi报错：NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver._missyoudaisy的博客-CSDN博客</a></li><li><a href="https://blog.csdn.net/wangyezi19930928/article/details/54928201" target="_blank" rel="noopener">apt-get update ，upgarde 和dist-upgrade 的区别_wangyezi19930928的专栏-CSDN博客</a></li><li><a href="https://askubuntu.com/questions/227258/error-could-not-locate-dkms-conf-file" target="_blank" rel="noopener"><code>Error! Could not locate dkms.conf file</code> - Ask Ubuntu</a></li><li><a href="https://blog.csdn.net/xiaoaid01/article/details/41862487" target="_blank" rel="noopener">更新Linux内核头文件(linux headers)_xiaoaide01的专栏-CSDN博客</a></li><li><a href="https://www.google.com.hk/search?q=Error!+Your+kernel+headers+for+kernel+4.4.0-210-generic+cannot+be+found&amp;rlz=1C1GCEU_zh-CNCN866CN866&amp;oq=Error!+Your+kernel+headers+for+kernel+4.4.0-210-generic+cannot+be+found&amp;aqs=chrome..69i57.681j0j4&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">Error! Your kernel headers for kernel 4.4.0-210-generic cannot be found - Google 搜索</a></li><li><a href="https://askubuntu.com/questions/951046/unable-to-install-nvidia-drivers-unable-to-locate-package" target="_blank" rel="noopener">17.04 - Unable to install nvidia drivers - unable to locate package - Ask Ubuntu</a></li><li><a href="https://blog.csdn.net/qq_34168515/article/details/107410732" target="_blank" rel="noopener">Ubuntu报错software-properties-common : Depends: python3-software-properties</a></li><li><a href="https://askubuntu.com/questions/361862/nvidia-drivers-synaptic" target="_blank" rel="noopener">ubuntu-drivers: command not found</a></li></ol><h1 id="三、参考书籍"><a href="#三、参考书籍" class="headerlink" title="三、参考书籍"></a>三、参考书籍</h1><h1 id="四、疑难解答"><a href="#四、疑难解答" class="headerlink" title="四、疑难解答"></a>四、疑难解答</h1><h2 id="1、修改文件或目录的权限"><a href="#1、修改文件或目录的权限" class="headerlink" title="1、修改文件或目录的权限"></a>1、修改文件或目录的权限</h2><p>（1）语法</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">chmod [-cfvR] [--help] [--version] mode file...<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）参数说明</p><p><strong>mode :</strong> 权限设定字串，格式如下 :</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">[ugoa...][[+-=][rwxX]...][,...]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>其中：</strong></p><ul><li>u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。</li><li>+ 表示增加权限、- 表示取消权限、= 表示唯一设定权限。</li><li>r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。</li></ul><p>（3）其他参数说明：</p><ul><li>-c : 若该文件权限确实已经更改，才显示其更改动作</li><li>-f : 若该文件权限无法被更改也不要显示错误讯息</li><li>-v : 显示权限变更的详细资料</li><li>-R : 对目前目录下的所有文件与子目录进行相同的权限变更(即以递回的方式逐个变更)</li><li>—help : 显示辅助说明</li><li>—version : 显示版本</li></ul><p>（4）实例</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 给个人目录的其他用户删除写权限 hadoop fs -chmod -R o-w /user/name/dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2、vim-文本搜索"><a href="#2、vim-文本搜索" class="headerlink" title="2、vim 文本搜索"></a>2、vim 文本搜索</h2><h3 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h3><ul><li>在Linux环境中，一个大文本中搜索指定字符串应该怎么操作？</li></ul><h3 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h3><pre class="line-numbers language-lang-linux"><code class="language-lang-linux">vi my.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>键盘按<code>Esc</code></li><li>输入<code>/search_string</code></li><li>键盘按<code>n</code>或者<code>N</code>来进行向前或向后搜索</li></ul><h2 id="3、日期循环"><a href="#3、日期循环" class="headerlink" title="3、日期循环"></a>3、日期循环</h2><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">#! /bin/bashstart=20200312end=20200322while [ ${start} -le ${end} ]do  echo ${start}  start=`date -d "1 day ${start}" +%Y%m%d`    # 日期自增done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>参考<a href="https://sjq597.github.io/2015/11/03/Shell-按日期循环执行/" target="_blank" rel="noopener">日期循环</a></li></ul><h2 id="4、将代码输出重定向到log文件-不覆盖的形式"><a href="#4、将代码输出重定向到log文件-不覆盖的形式" class="headerlink" title="4、将代码输出重定向到log文件-不覆盖的形式"></a>4、将代码输出重定向到log文件-不覆盖的形式</h2><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">0 11 * * * /usr/bin/python3 /home/user/adsp_new/orientation.py >> /home/user/adsp_new/logs1_ori.txt 2>&1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5、查看系统版本"><a href="#5、查看系统版本" class="headerlink" title="5、查看系统版本"></a>5、查看系统版本</h2><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">lsb_release -a<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="6、查看显卡信息"><a href="#6、查看显卡信息" class="headerlink" title="6、查看显卡信息"></a>6、查看显卡信息</h2><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">lspci | grep -i nvidia<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之Java</title>
      <link href="/2019/10/13/shi-yong-gong-ju-zhi-java/"/>
      <url>/2019/10/13/shi-yong-gong-ju-zhi-java/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Java基础"><a href="#一、Java基础" class="headerlink" title="一、Java基础"></a>一、Java基础</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul><li><p><code>JDK</code>：Java Development Kit</p><p>如果只有Java源码，要编译成Java字节码，就需要JDK，因为JDK除了包含JRE，还提供了编译器JavaC、调试器JDB等开发工具</p></li><li><p><code>JRE</code>：Java Runtime Environment</p><p>JRE就是运行Java字节码的虚拟机</p></li></ul><ul><li><p><code>IDE</code>：Integrated Development Environment </p><p>流行的有：Eclipse、IntelliJ Idea</p></li></ul><h3 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h3><ul><li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1280507291631649" target="_blank" rel="noopener">安装链接</a></li></ul><h3 id="安装IDE"><a href="#安装IDE" class="headerlink" title="安装IDE"></a>安装IDE</h3><ul><li><a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener">IntelliJ Idea</a></li></ul><h2 id="程序基础"><a href="#程序基础" class="headerlink" title="程序基础"></a>程序基础</h2><h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p>（1）一个完整的Java程序。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">/** * 可以用来自动创建文档的注释 */public class Hello {    public static void main(String[] args) {        // 向屏幕输出文本:        final double PI = 3.14; // PI是一个常量        System.out.println("Hello, world!");        /* 多行注释开始        注释内容        注释结束 */    }} // class定义结束<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）结构解释</p><ul><li>一个程序的基本单位是<code>class</code>。类名要求必须以英文字母开头，后接字母，数字和下划线的组合，习惯以大写字母开头。</li><li><code>public</code>是访问修饰符，表示该class是公开的。</li><li><code>main</code>是方法名，返回值是<code>void</code>。</li><li>每一行语句后面都要加分号<code>;</code>。</li><li>一个文件顶多一个public类，且类名与文件名一致。</li><li><code>final</code>是常量修饰符。</li></ul><h3 id="变量类型"><a href="#变量类型" class="headerlink" title="变量类型"></a>变量类型</h3><p>（1）变量定义、赋值示例程序。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        int n = 100; // 定义变量n，同时赋值为100        System.out.println("n = " + n); // 打印n的值        n = 200; // 变量n赋值为200        System.out.println("n = " + n); // 打印n的值        int x = n; // 变量x赋值为n（n的值为200，因此赋值后x的值也是200）        System.out.println("x = " + x); // 打印x的值        x = x + 100; // 变量x赋值为x+100（x的值为200，因此赋值后x的值是200+100=300）        System.out.println("x = " + x); // 打印x的值        System.out.println("n = " + n); // 再次打印n的值，n应该是200还是300？200   }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）基本类型：变量是“持有”某个数值</p><ul><li><p>整数</p><p>byte(8bit)、short(16bit)、int(32bit)、long(64bit)</p></li><li><p>浮点数</p><p>float(32bit)、double(64bit)</p></li><li><p>字符</p><p>char(16bit)</p></li><li><p>布尔</p></li></ul><p>（3）引用类型：变量是“指向”某个对象</p><ol><li><p>字符串</p><p>String</p></li></ol><h3 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h3><p><strong>注意</strong>：由于存在范围限制，如果计算结果超出了范围，就会产生溢出，而溢出<em>不会出错</em>，却会得到一个奇怪的结果</p><p>（1）整数运算</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        int i = (100 + 200) * (99 - 88); // 3300        int n = 7 * (5 + (i - 9)); // 23072        n ++;        n --;        System.out.println(i);        System.out.println(n);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：<code>++n</code>表示先加1再引用n，<code>n++</code>表示先引用n再加1。</p><ul><li>整数运算的结果永远是精确的；</li><li>运算结果会自动提升；例如，<code>short</code>和<code>int</code>计算，结果总是<code>int</code>，原因是<code>short</code>首先自动被转型为<code>int</code>；</li><li>可以强制转型，但超出范围的强制转型会得到错误的结果；例如，将<code>int</code>强制转型为<code>short</code>；</li><li><p>应该选择合适范围的整型（<code>int</code>或<code>long</code>），没有必要为了节省内存而使用<code>byte</code>和<code>short</code>进行整数运算。</p></li><li><p>运算优先级从高到低：</p><ul><li><code>()</code></li><li><code>!</code> <code>~</code> <code>++</code> <code>--</code></li><li><code>*</code> <code>/</code> <code>%</code></li><li><code>+</code> <code>-</code></li><li><code>&lt;&lt;</code> <code>&gt;&gt;</code> <code>&gt;&gt;&gt;</code></li><li><code>&amp;</code> ：与运算，是位运算</li><li><code>|</code> ：或运算</li><li><code>+=</code> <code>-=</code> <code>*=</code> <code>/=</code></li></ul></li></ul><p>（2）浮点数运算</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">// 浮点数误差public class Main {    public static void main(String[] args) {        double x = 1.0 / 10;        double y = 1 - 9.0 / 10;        // 观察x和y是否相等:        System.out.println(x);        System.out.println(y);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>浮点数常常无法精确表示，并且浮点数的运算结果可能有误差；</li><li>比较两个浮点数通常比较它们的绝对值之差是否小于一个特定值；</li><li>整型和浮点型运算时，整型会自动提升为浮点型；</li><li>可以将浮点型强制转为整型，但超出范围后将始终返回整型的最大值。</li><li>浮点数运算和整数运算相比，只能进行加减乘除这些数值计算，不能做位运算和移位运算。</li></ul><p>（3）布尔运算</p><ul><li><p>关系运算符优先级从高到低：</p><ul><li><code>!</code></li><li><code>&gt;</code>，<code>&gt;=</code>，<code>&lt;</code>，<code>&lt;=</code></li><li><code>==</code>，<code>!=</code></li><li><code>&amp;&amp;</code></li><li><code>||</code></li></ul></li><li><p>短路运算：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        boolean b = 5 < 3;        boolean result = b && (5 / 0 > 0);        System.out.println(result);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>因为<code>false &amp;&amp; x</code>的结果总是<code>false</code>，无论<code>x</code>是<code>true</code>还是<code>false</code>，因此，与运算在确定第一个值为<code>false</code>后，不再继续计算，而是直接返回<code>false</code>。</li><li>如果没有短路运算，<code>&amp;&amp;</code>后面的表达式会由于除数为<code>0</code>而报错，但实际上该语句并未报错，原因在于与运算是短路运算符，提前计算出了结果<code>false</code>。</li></ul></li><li><p>三元运算符</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        int n = -100;        int x = n >= 0 ? n : -n;        System.out.println(x);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="字符和字符串"><a href="#字符和字符串" class="headerlink" title="字符和字符串"></a>字符和字符串</h3><ul><li><p>Java的字符类型<code>char</code>是基本类型，字符串类型<code>String</code>是引用类型；</p></li><li><p>基本类型的变量是“持有”某个数值，引用类型的变量是“指向”某个对象；</p></li><li><p>引用类型的变量可以是空值<code>null</code>；</p></li><li><p>要区分空值<code>null</code>和空字符串<code>&quot;&quot;</code>。</p></li><li><p>可以使用<code>+</code>连接任意字符串和其他数据类型：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        int age = 25;        String s = "age is " + age;        System.out.println(s);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>字符串不可变：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        String s = "hello";        String t = s;        s = "world";        System.out.println(t); // t是"hello"还是"world"? "hello"    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="数组类型和操作"><a href="#数组类型和操作" class="headerlink" title="数组类型和操作"></a>数组类型和操作</h3><ul><li><p>数组类型，数组是引用类型</p><p><strong>注意</strong>：这里的<code>s</code>指向了<code>name[1]</code>之后，即将<code>s</code>指向了<code>&quot;XYZ&quot;</code>，这个是不变的字符串常量</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        String[] names = {"ABC", "XYZ", "zoo"};        String s = names[1];        names[1] = "cat";        System.out.println(s); // s是"XYZ"还是"cat"?  "XYZ"     }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>数组元素可以是值类型（如int）或引用类型（如String），但数组本身是引用类型；</li><li>数组是同一数据类型的集合，数组一旦创建后，大小就不可变；</li><li>可以通过索引访问数组元素，但索引超出范围将报错；</li></ul></li><li><p>数组操作</p><ul><li><p>遍历数组</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">for (int n : ns)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-java"><code class="language-lang-java">for (int i=0; i<ns.length; i++)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-lang-java"><code class="language-lang-java">import Java.util.Arrays;int[] ns = { 1, 1, 2, 3, 5, 8 };System.out.println(Arrays.toString(ns));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>数组排序</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">import Java.util.Arrays;int[] ns = { 28, 12, 89, 73, 65};Arrays.sort(ns); //正序<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>多维数组</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">import Java.util.Arrays;int[][] ns = {            { 1, 2, 3, 4 },            { 5, 6, 7, 8 },            { 9, 10, 11, 12 }        };System.out.println(Arrays.deepToString(ns));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h2 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h2><h3 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h3><ul><li><p>输出：<code>System.out.println(&quot;END&quot;);</code></p></li><li><p>格式化输出：<code>System.out.printf(&quot;%.4f\n&quot;, d);</code></p></li><li><p>占位符及说明</p><p>| 占位符 | 说明                             |<br>| :——- | :———————————————- |<br>| %d     | 格式化输出整数                   |<br>| %x     | 格式化输出十六进制整数           |<br>| %f     | 格式化输出浮点数                 |<br>| %e     | 格式化输出科学计数法表示的浮点数 |<br>| %s     | 格式化字符串                     |</p></li><li><p>输入：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">import Java.util.Scanner;public class Main {    public static void main(String[] args) {        Scanner scanner = new Scanner(System.in); // 创建Scanner对象        System.out.print("Input your name: "); // 打印提示        String name = scanner.nextLine(); // 读取一行输入并获取字符串        System.out.print("Input your age: "); // 打印提示        int age = scanner.nextInt(); // 读取一行输入并获取整数        System.out.printf("Hi, %s, you are %d\n", name, age); // 格式化输出    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="IF判断"><a href="#IF判断" class="headerlink" title="IF判断"></a>IF判断</h3><ul><li><p><code>if</code>基本语法：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">if (条件) {    // 条件满足时执行} else if (条件) {    // } else {    //}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>判断浮点数相等时利用差值小于某个临界值来判断：<code>Math.abs(a-b)&lt;0.00001</code></li><li>判断引用类型的变量内容是否相等，必须使用<code>equals()</code>方法：<code>s1.equals(s2)</code></li></ul></li></ul><h3 id="SWITCH多重选择"><a href="#SWITCH多重选择" class="headerlink" title="SWITCH多重选择"></a>SWITCH多重选择</h3><ul><li><p><code>switch</code>语句可以做多重选择，然后执行匹配的<code>case</code>语句后续代码；<code>switch</code>的计算结果必须是整型、字符串或枚举类型；注意千万不要漏写<code>break</code>，建议打开<code>fall-through</code>警告；总是写上<code>default</code>，建议打开<code>missing default</code>警告；<strong>从Java 13开始，<code>switch</code>语句升级为表达式，不再需要<code>break</code>，并且允许使用<code>yield</code>返回值。</strong></p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        int option = 1;        switch (option) {        case 1:            System.out.println("Selected 1");            break;        case 2:            System.out.println("Selected 2");            break;        default:            System.out.println("Selected 3");            break;        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="WHILE循环"><a href="#WHILE循环" class="headerlink" title="WHILE循环"></a>WHILE循环</h3><ul><li><p><code>while</code>基本语法：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">while (条件表达式) {    循环语句}// 继续执行后续代码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><code>do while</code>基本语法：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">do {    执行循环语句} while (条件表达式);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="FOR循环"><a href="#FOR循环" class="headerlink" title="FOR循环"></a>FOR循环</h3><ul><li><p><code>for</code>循环基本语法：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">for (初始条件; 循环检测条件; 循环后更新计数器) {    // 执行语句}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="BREAK-AND-CONTINUE"><a href="#BREAK-AND-CONTINUE" class="headerlink" title="BREAK AND CONTINUE"></a>BREAK AND CONTINUE</h3><ul><li><code>break</code>：在循环过程中，可以使用<code>break</code>语句跳出当前循环</li><li><code>continue</code>：前结束本次循环，直接继续执行下次循环。</li></ul><h1 id="二、Java进阶"><a href="#二、Java进阶" class="headerlink" title="二、Java进阶"></a>二、Java进阶</h1><h2 id="面向对象编程基础"><a href="#面向对象编程基础" class="headerlink" title="面向对象编程基础"></a>面向对象编程基础</h2><h3 id="什么是面向对象？"><a href="#什么是面向对象？" class="headerlink" title="什么是面向对象？"></a>什么是面向对象？</h3><ul><li><p>简单来说，人就是一个<strong>对象</strong>，将人身高、年龄等特征封装到人这个对象里，定义人这个类。具体到小明、小芳这些人就被称为<strong>实例</strong>。</p></li><li><p>定义对象</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Person {    public String name;    public int age;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>创建实例</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">Person ming = new Person();ming.name = "Xiao Ming";<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>指向实例的变量<code>ming</code>是引用变量</li></ul></li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul><li><p>方法（<code>method</code>）可以让外部代码安全地访问实例字段（<code>filed</code>）；</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        Person ming = new Person();        ming.setName("Xiao Ming"); // 设置name        System.out.println(ming.getName());    }}class Person {    private String name;    public String getName() {        return this.name;    }    public void setName(String name) {        this.name = name;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>方法是一组执行语句，并且可以执行任意逻辑；</p></li><li><p>方法内部遇到return时返回，void表示不返回任何值（注意和返回null不同）；</p></li><li><p>外部代码通过public方法操作实例，内部代码可以调用private方法；其中<code>this</code>变量是一个隐含变量，通过<code>this.field</code>就可以访问当前实例的字段。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Person {    private String name;    private int birth;    public void setBirth(int birth) {        this.birth = birth;    }    public int getAge() {        return calcAge(2019); // 调用private方法    }    // private方法:    private int calcAge(int currentYear) {        return currentYear - this.birth;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>理解方法的参数绑定。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">// 基本类型参数绑定public class Main {    public static void main(String[] args) {        Person p = new Person();        int n = 15; // n的值为15        p.setAge(n); // 传入n的值        System.out.println(p.getAge()); // 15        n = 20; // n的值改为20        System.out.println(p.getAge()); // 15还是20?  15    }}class Person {    private int age;    public int getAge() {        return this.age;    }    public void setAge(int age) {        this.age = age;    }}// 引用类型参数绑定public class Main {    public static void main(String[] args) {        Person p = new Person();        String[] fullname = new String[] { "Homer", "Simpson" };        p.setName(fullname); // 传入fullname数组        System.out.println(p.getName()); // "Homer Simpson"        fullname[0] = "Bart"; // fullname数组的第一个元素修改为"Bart"        System.out.println(p.getName()); // "Homer Simpson"还是"Bart Simpson"?  Bart Simpson    }}class Person {    private String[] name;    public String getName() {        return this.name[0] + " " + this.name[1];    }    public void setName(String[] name) {        this.name = name;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>基本类型参数的传递，是调用方值的复制。双方各自的后续修改，互不影响。</li><li>引用类型参数的传递，调用方的变量，和接收方的参数变量，指向的是同一个对象。双方任意一方对这个对象的修改，都会影响对方（因为指向同一个对象嘛）。</li></ul></li></ul><h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><ul><li><p>由于构造方法是如此特殊，所以构造方法的名称就是类名。构造方法的参数没有限制，在方法内部，也可以编写任意语句。但是，和普通方法相比，构造方法没有返回值（也没有<code>void</code>），调用构造方法，必须用<code>new</code>操作符。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        Person p = new Person("Xiao Ming", 15);        System.out.println(p.getName());        System.out.println(p.getAge());    }}class Person {    private String name;    private int age;    // 构造方法    public Person(String name, int age) {        this.name = name;        this.age = age;    }    public String getName() {        return this.name;    }    public int getAge() {        return this.age;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>如果一个类没有定义构造方法，编译器会自动为我们生成一个默认构造方法，它没有参数，也没有执行语句</p></li><li><p>如果既要能使用带参数的构造方法，又想保留不带参数的构造方法，那么只能把两个构造方法都定义出来：</p></li><li><p>既对字段进行初始化，又在构造方法中对字段进行初始化：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Person {    private String name = "Unamed";    private int age = 10;    public Person(String name, int age) {        this.name = name;        this.age = age;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>多构造方法</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Person {    private String name;    private int age;    public Person(String name, int age) {        this.name = name;        this.age = age;    }    public Person(String name) {        this.name = name;        this.age = 12;    }    public Person() {    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>在通过<code>new</code>操作符调用的时候，编译器通过构造方法的参数数量、位置和类型自动区分</p></li><li><p>一个构造方法可以调用其他构造方法，这样做的目的是便于代码复用。调用其他构造方法的语法是<code>this(…)</code></p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Person {    private String name;    private int age;    public Person(String name, int age) {        this.name = name;        this.age = age;    }    public Person(String name) {        this(name, 18); // 调用另一个构造方法Person(String, int)    }    public Person() {        this("Unnamed"); // 调用另一个构造方法Person(String)    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h3 id="方法重载"><a href="#方法重载" class="headerlink" title="方法重载"></a>方法重载</h3><ul><li><p>这种方法名相同，但各自的参数不同，称为方法重载（<code>Overload</code>）。</p><p><strong>注意</strong>：方法重载的返回值类型通常都是相同的。</p></li><li><p>重载方法应该完成类似的功能，参考<code>String</code>的<code>indexOf()</code>；</p></li></ul><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><ul><li><p>继承是面向对象编程中非常强大的一种机制，它首先可以复用代码。当我们让<code>Student</code>从<code>Person</code>继承时，<code>Student</code>就获得了<code>Person</code>的所有功能，我们只需要为<code>Student</code>编写新增的功能。</p><p>Java使用<code>extends</code>关键字来实现继承：</p><p><strong>注意</strong>：一定要加<code>super</code>调用父类的构造方法</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class Main {    public static void main(String[] args) {        Student s = new Student("Xiao Ming", 12, 89);    }}class Person {    protected String name;    protected int age;    public Person(String name, int age) {        this.name = name;        this.age = age;    }}// 继承Personclass Student extends Person {    protected int score;    public Student(String name, int age, int score) {        super(name, age); // 调用父类的构造方法Person(String, int)        this.score = score;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="Java核心类"><a href="#Java核心类" class="headerlink" title="Java核心类"></a>Java核心类</h2><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><h2 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h2><h1 id="三、参考书籍"><a href="#三、参考书籍" class="headerlink" title="三、参考书籍"></a>三、参考书籍</h1><ul><li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744" target="_blank" rel="noopener">廖大神</a></li></ul><h1 id="四、疑难解答"><a href="#四、疑难解答" class="headerlink" title="四、疑难解答"></a>四、疑难解答</h1><h2 id="1、使用IntelliJ-IDEA-配置Maven"><a href="#1、使用IntelliJ-IDEA-配置Maven" class="headerlink" title="1、使用IntelliJ IDEA 配置Maven"></a>1、使用IntelliJ IDEA 配置Maven</h2><ul><li><a href="https://blog.csdn.net/qq_32588349/article/details/51461182" target="_blank" rel="noopener">参考</a></li></ul><h2 id="2、Intellij-IDEA-打包jar的多种方式"><a href="#2、Intellij-IDEA-打包jar的多种方式" class="headerlink" title="2、Intellij IDEA 打包jar的多种方式"></a>2、Intellij IDEA 打包jar的多种方式</h2><ul><li><a href="https://blog.csdn.net/Thousa_Ho/article/details/72799871" target="_blank" rel="noopener">参考</a></li></ul><h2 id="3、final关键字解释"><a href="#3、final关键字解释" class="headerlink" title="3、final关键字解释"></a>3、final关键字解释</h2><ul><li><p><a href="https://www.cnblogs.com/dolphin0520/p/3736238.html" target="_blank" rel="noopener">参考</a></p></li><li><p>类：当用final修饰一个类时，表明这个类不能被继承。</p></li><li><p>方法：使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升。在最近的Java版本中，不需要使用final方法进行这些优化了。</p></li><li><p>变量：对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象</p></li></ul><h2 id="4、一行代码判断String字符串是否为纯符号"><a href="#4、一行代码判断String字符串是否为纯符号" class="headerlink" title="4、一行代码判断String字符串是否为纯符号"></a>4、一行代码判断String字符串是否为纯符号</h2><ul><li><a href="https://blog.csdn.net/qq_39731011/article/details/85159476" target="_blank" rel="noopener">参考</a></li></ul><pre class="line-numbers language-lang-java"><code class="language-lang-java">if (term.trim().replaceAll("\\p{P}", "").length() == 0){                    continue;                }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 编程语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具之hive</title>
      <link href="/2019/10/12/shi-yong-gong-ju-zhi-hive/"/>
      <url>/2019/10/12/shi-yong-gong-ju-zhi-hive/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/12/shi-yong-gong-ju-zhi-hive/image-20210621150214191.png" alt="image-20210621150214191"></p><h1 id="一、Hive基础"><a href="#一、Hive基础" class="headerlink" title="一、Hive基础"></a>一、Hive基础</h1><h2 id="1、Hive数据类型"><a href="#1、Hive数据类型" class="headerlink" title="1、Hive数据类型"></a>1、Hive数据类型</h2><p>（1）原始类型</p><div class="table-container"><table><thead><tr><th style="text-align:center">类型</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th></tr></thead><tbody><tr><td style="text-align:center"><code>Boolean</code></td><td style="text-align:center"><code>true</code>、<code>false</code></td><td style="text-align:center"><code>TRUE</code></td></tr><tr><td style="text-align:center"><code>TINYINT</code></td><td style="text-align:center">-128~127</td><td style="text-align:center"><code>1Y</code></td></tr><tr><td style="text-align:center"><code>SMALLINT</code></td><td style="text-align:center">-32768~32767</td><td style="text-align:center"><code>1S</code></td></tr><tr><td style="text-align:center"><code>INT</code></td><td style="text-align:center">4个字节的带符号整数</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center"><code>BIGINT</code></td><td style="text-align:center">8字节带符号整数</td><td style="text-align:center"><code>1L</code></td></tr><tr><td style="text-align:center"><code>FLOAT</code></td><td style="text-align:center">4字节单精度浮点数</td><td style="text-align:center">1.0</td></tr><tr><td style="text-align:center"><code>DOUBLE</code></td><td style="text-align:center">8字节双精度浮点数</td><td style="text-align:center">1.0</td></tr><tr><td style="text-align:center"><code>DECIMAL</code></td><td style="text-align:center">任意精度的带符号小数</td><td style="text-align:center">1.0</td></tr><tr><td style="text-align:center"><code>STRING</code></td><td style="text-align:center">字符串</td><td style="text-align:center"><code>‘ABC’</code></td></tr><tr><td style="text-align:center"><code>VARCHAR</code></td><td style="text-align:center">长字符串</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>CHAR</code></td><td style="text-align:center">固定长度字符串</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>BINARY</code></td><td style="text-align:center">字节数组</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>TIMESTAMP</code></td><td style="text-align:center">时间戳，纳秒精度</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>DATE</code></td><td style="text-align:center">日期</td><td style="text-align:center"><code>2019-10-08</code></td></tr></tbody></table></div><p>（2）复杂类型</p><div class="table-container"><table><thead><tr><th style="text-align:center">类型</th><th style="text-align:center">描述</th><th style="text-align:center">示例</th></tr></thead><tbody><tr><td style="text-align:center"><code>ARRAY</code></td><td style="text-align:center">有序的的同类型的集合</td><td style="text-align:center"><code>array(1,2)</code></td></tr><tr><td style="text-align:center"><code>MAP</code></td><td style="text-align:center">key-value<br>key必须为原始类型，value可以任意类型</td><td style="text-align:center"><code>map(‘a’,1,’b’,2)</code></td></tr><tr><td style="text-align:center"><code>UNION</code></td><td style="text-align:center">在有限取值范围内的一个值</td><td style="text-align:center"><code>create_union(1,’a’,63)</code></td></tr><tr><td style="text-align:center"><code>STRUCT</code></td><td style="text-align:center">字段集合,类型可以不同</td><td style="text-align:center"><code>struct(‘1’,1,1.0)</code></td></tr></tbody></table></div><h2 id="2、Hive数据库操作"><a href="#2、Hive数据库操作" class="headerlink" title="2、Hive数据库操作"></a>2、Hive数据库操作</h2><p>（1）创建数据库</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令CREATE DATABASE|SCHEMA [IF NOT EXISTS] <database name>;-- 例子CREATE DATABASE IF NOT EXISTS database_name;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）删除数据库</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令DROP DATABASE StatementDROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];-- 例子DROP DATABASE IF EXISTS userdb;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3、Hive表操作"><a href="#3、Hive表操作" class="headerlink" title="3、Hive表操作"></a>3、Hive表操作</h2><p>（1）创建表</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][ROW FORMAT row_format][STORED AS file_format]-- 例子CREATE TABLE IF NOT EXISTS employee(eid int,  name String, salary String,  destination String)COMMENT ‘Employee details’ROW FORMAT DELIMITEDFIELDS TERMINATED BY ‘\t’LINES TERMINATED BY ‘\n’STORED AS TEXTFILE;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）修改表</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 重命名表ALTER TABLE name RENAME TO new_nameALTER TABLE employee RENAME TO emp;-- 添加列ALTER TABLE name ADD COLUMNS (col_spec[, col_spec ...])ALTER TABLE employee ADD COLUMNS (    dept STRING COMMENT 'Department name');-- 删除列ALTER TABLE name DROP [COLUMN] column_name-- 更改列属性ALTER TABLE name CHANGE column_name new_name new_typeALTER TABLE employee CHANGE salary salary Double;-- 替换列ALTER TABLE name REPLACE COLUMNS (col_spec[, col_spec ...])ALTER TABLE employee REPLACE COLUMNS (    eid INT empid Int,    ename STRING name String);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）删除表</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">DROP TABLE [IF EXISTS] table_name;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="4、Hive表数据操作"><a href="#4、Hive表数据操作" class="headerlink" title="4、Hive表数据操作"></a>4、Hive表数据操作</h2><p>（1）插入数据</p><ul><li><code>LOAD DATA</code>：从<code>HDFS</code>中加载数据会直接<code>move</code>！</li></ul><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">--命令LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]--例子LOAD DATA LOCAL INPATH '/home/user/sample.txt' OVERWRITE INTO TABLE employee;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>INSERT INTO</code></li></ul><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">insert into table account select id, age, name from account_tmp;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li><code>INSERT OVERWRITE</code></li></ul><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">insert overwrite table account2 select id, age, name from account_tmp;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong><em>注意</em></strong>：</p><p><code>insert overwrite</code>会覆盖已经存在的数据，假如原始表使用<code>overwrite</code>上述的数据，先现将原始表的数据<code>remove</code>，再插入新数据。</p><p><code>insert into</code>只是简单的插入，不考虑原始表的数据，直接追加到表中。</p><p>（2）修改数据</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">UPDATE tablename SET column = value [, column = value ...] [WHERE expression]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）删除数据</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">DELETE FROM tablename [WHERE expression]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5、Hive分区操作"><a href="#5、Hive分区操作" class="headerlink" title="5、Hive分区操作"></a>5、Hive分区操作</h2><p>（1）添加分区</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION (p_column = p_col_value, p_column = p_col_value, ...)[LOCATION 'location1'] (p2_column = p2_col_value, p2_column = p2_col_value, ...) [LOCATION 'location2'] ...;-- 例子ALTER TABLE employee ADD PARTITION (year=’2013’) location '/2012/part2012';<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）重命名分区</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec;-- 例子ALTER TABLE employee PARTITION (year=’1203’) RENAME TO PARTITION (Yoj=’1203’);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）删除分区</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec, PARTITION partition_spec,...;-- 例子ALTER TABLE employee DROP [IF EXISTS] PARTITION (year=’1203’);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="6、Hive内置运算符"><a href="#6、Hive内置运算符" class="headerlink" title="6、Hive内置运算符"></a>6、Hive内置运算符</h2><p>（1）关系运算符</p><div class="table-container"><table><thead><tr><th>运算符</th><th>操作</th><th>描述</th></tr></thead><tbody><tr><td>A = B</td><td>所有基本类型</td><td></td></tr><tr><td>A != B</td><td>所有基本类型</td><td></td></tr><tr><td>A &lt; B</td><td>所有基本类型</td><td></td></tr><tr><td>A &lt;= B</td><td>所有基本类型</td><td></td></tr><tr><td>A &gt; B</td><td>所有基本类型</td><td></td></tr><tr><td>A &gt;= B</td><td>所有基本类型</td><td></td></tr><tr><td>A IS NULL</td><td>所有类型</td><td></td></tr><tr><td>A IS NOT NULL</td><td>所有类型</td><td></td></tr><tr><td>A LIKE B</td><td>字符串</td><td>TRUE，如果字符串模式A匹配到B，<br>否则FALSE。</td></tr><tr><td>A RLIKE B</td><td>字符串</td><td>TRUE：A任何子字符串匹配Java正则表达式B；</td></tr><tr><td>A REGEXP B</td><td>字符串</td><td>等同于RLIKE.</td></tr></tbody></table></div><p>（2）算术运算符</p><div class="table-container"><table><thead><tr><th>运算符</th><th>操作</th><th>描述</th></tr></thead><tbody><tr><td>A + B</td><td>所有数字类型</td><td></td></tr><tr><td>A - B</td><td>所有数字类型</td><td></td></tr><tr><td>A * B</td><td>所有数字类型</td><td></td></tr><tr><td>A / B</td><td>所有数字类型</td><td></td></tr><tr><td>A % B</td><td>所有数字类型</td><td></td></tr><tr><td>A &amp; B</td><td>所有数字类型</td><td>A和B的按位与结果</td></tr><tr><td>A \</td><td>B</td><td>所有数字类型</td><td>A和B的按位或结果</td></tr><tr><td>A ^ B</td><td>所有数字类型</td><td>A和B的按位异或结果</td></tr><tr><td>~ A</td><td>所有数字类型</td><td>A按位非的结果</td></tr></tbody></table></div><p>（3）逻辑运算符</p><div class="table-container"><table><thead><tr><th>运算符</th><th>操作</th><th>描述</th></tr></thead><tbody><tr><td>A AND B</td><td>Boolean</td><td>与</td></tr><tr><td>A &amp;&amp; B</td><td>Boolean</td><td></td></tr><tr><td>A OR B</td><td>Boolean</td><td>或</td></tr><tr><td>A \</td><td>\</td><td>B</td><td>Boolean</td><td></td></tr><tr><td>NOT A</td><td>Boolean</td><td>非</td></tr><tr><td>! A</td><td>Boolean</td></tr></tbody></table></div><p>（4）复杂运算符</p><div class="table-container"><table><thead><tr><th>运算符</th><th>操作</th><th>描述</th></tr></thead><tbody><tr><td>A[n]</td><td>A是一个数组，n是一个int</td><td>它返回数组A的第n+1个元素，第一个元素的索引0。</td></tr><tr><td>M[Key]</td><td>M 是一个 Map<K, v> 并 key 的类型为K</K,></td><td>它返回对应于映射中关键字的值。</td></tr><tr><td>S.x</td><td>S 是一个结构</td><td>它返回S的s字段</td></tr></tbody></table></div><h2 id="7、Hive内置函数"><a href="#7、Hive内置函数" class="headerlink" title="7、Hive内置函数"></a>7、Hive内置函数</h2><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 返回BIGINT最近的double值。round(double a)-- 返回最大BIGINT值等于或小于double。floor(double a)-- 它返回最小BIGINT值等于或大于double。ceil(double a)-- 它返回一个随机数，从行改变到行。rand()rand(int seed)-- 它返回从A后串联B产生的字符串concat(string A, string B, ...)-- 它返回一个起始位置start到A结束的子字符串substr(string A, int start)-- 返回从给定长度length的从起始位置start开始的字符串。substr(string A, int start, int length)-- 它返回转换所有字符为大写的字符串。upper(string A)ucase(string A)-- 它返回转换所有字符为小写的字符串。lower(string A)lcase(string A)-- 它返回字符串从A两端修剪空格的结果trim(string A)-- 它返回从A左边开始修整空格产生的字符串(左手侧)ltrim(string A)-- 它返回从A右边开始修整空格产生的字符串(右侧)rtrim(string A)-- 它返回将A中的子字符串B替换为C的全新字符串regexp_replace(string A, string B, string C)-- 它返回在映射类型或数组类型的元素的数量。size(Map<K.V>)size(Array<T>)-- 将字段expr的数据类型转换为type。如果转换不成功，返回的是NULL。cast(<expr> as <type>)-- 将10位的时间戳值unixtime转为日期函数from_unixtime(int unixtime, 'yyyy-MM-dd HH:mm:ss')-- 返回一个字符串时间戳的日期部分：to_date("1970-01-01 00:00:00") = "1970-01-01"to_date(string timestamp, 'yyyy-MM-dd')-- 返回指定日期的unix时间戳unix_timestamp(string date, 'yyyy-MM-dd HH:mm:ss')  -- date的形式必须为’yyyy-MM-dd HH:mm:ss’的形式unix_timestamp()  -- 返回当前时间的unix时间戳-- 返回时间字段中的年月日year(string date, 'yyMMdd')  -- 年month(string date, 'yyMMdd')  -- 月day(string date, 'yyMMdd')  -- 日-- 返回时间字段是本年的第多少周weekofyear(string date, 'yyMMdd')-- 返回enddate与begindate之间的时间差的天数datediff(string enddate,string begindate)select datediff(‘2016-06-01’,’2016-05-01’) from Hive_sum limit 1;-- 返回date增加days天后的日期date_add(string date,int days)-- 返回date减少days天后的日期date_sub(string date,int days)-- 提取从基于指定的JSON路径的JSON字符串JSON对象，并返回提取的JSON字符串的JSON对象。get_json_object(string json_string, string path)-- 返回检索行的总数。count(*)count(expr)-- 返回该组或该组中的列的不同值的分组和所有元素的总和。sum(col)sum(DISTINCT col)-- 返回上述组或该组中的列的不同值的元素的平均值。avg(col)avg(DISTINCT col)-- 返回该组中的列的最大最小值。min(col)max(col)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="8、Hive视图和索引"><a href="#8、Hive视图和索引" class="headerlink" title="8、Hive视图和索引"></a>8、Hive视图和索引</h2><p>（1）创建视图</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ][COMMENT table_comment]AS SELECT ...-- 例子CREATE VIEW emp_30000 ASSELECT * FROM employeeWHERE salary>30000;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）删除视图</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">DROP VIEW view_name<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）创建索引</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 命令CREATE INDEX index_nameON TABLE base_table_name (col_name, ...)AS 'index.handler.class.name'[WITH DEFERRED REBUILD][IDXPROPERTIES (property_name=property_value, ...)][IN TABLE index_table_name][PARTITIONED BY (col_name, ...)][   [ ROW FORMAT ...] STORED AS ...   | STORED BY ...][LOCATION hdfs_path][TBLPROPERTIES (...)]-- 例子：使用字段 Id, Name, Salary, Designation, 和 Dept创建一个名为index_salary的索引，对employee 表的salary列索引。CREATE INDEX inedx_salary ON TABLE employee(salary)AS 'org.apache.hadoop.Hive.ql.index.compact.CompactIndexHandler';<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）删除索引</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">DROP INDEX <index_name> ON <table_name><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h1 id="二、Hive进阶"><a href="#二、Hive进阶" class="headerlink" title="二、Hive进阶"></a>二、Hive进阶</h1><h2 id="1、Hive-SELECT-数据"><a href="#1、Hive-SELECT-数据" class="headerlink" title="1、Hive SELECT 数据"></a>1、Hive SELECT 数据</h2><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [HAVING having_condition] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]] [ORDER BY col_list][LIMIT number];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2、命令执行顺序"><a href="#2、命令执行顺序" class="headerlink" title="2、命令执行顺序"></a>2、命令执行顺序</h2><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 大致顺序from... where.... select... group by... having ... order by...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>备注：</strong>Hive语句和mysql都可以通过explain查看执行计划，使用explain + Hive语句</p><h1 id="三、Hive原理"><a href="#三、Hive原理" class="headerlink" title="三、Hive原理"></a>三、Hive原理</h1><h2 id="数据仓库概念"><a href="#数据仓库概念" class="headerlink" title="数据仓库概念"></a>数据仓库概念</h2><ul><li>Hive是数据仓库</li><li><strong>数据仓库跟数据库的区别：</strong><ol><li>数据库：传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。</li><li>数据仓库：数据仓库系统的应用主要是OLAP（On-Line Analytical Processing），支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。</li></ol></li></ul><p><img src="/2019/10/12/shi-yong-gong-ju-zhi-hive/hive_data_warehouse.png" alt="图：数据仓库结构"></p><h2 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h2><ul><li>Hadoop顶层的数据仓库工具</li><li>支持大数据存储与分析</li><li>依赖HDFS存储数据</li><li>依赖MapReduce处理数据</li><li>有类似SQL的查询语言-HiveQL</li><li>用户通过HiveQL运行MapReduce任务</li><li>Hive适用于<strong>BI报表</strong>，Pig适合ETL过程，HBase提供数据的实时访问</li></ul><h2 id="Hive与传统Sql对比"><a href="#Hive与传统Sql对比" class="headerlink" title="Hive与传统Sql对比"></a>Hive与传统Sql对比</h2><div class="table-container"><table><thead><tr><th>对比项目</th><th>Hive</th><th>传统Sql</th></tr></thead><tbody><tr><td>数据插入</td><td>支持批量导入</td><td>支持单条和批量</td></tr><tr><td>数据更新</td><td>不支持</td><td>支持</td></tr><tr><td>索引</td><td>支持</td><td>支持</td></tr><tr><td>分区</td><td>支持</td><td>支持</td></tr><tr><td>执行延迟</td><td>高</td><td>低</td></tr><tr><td>扩展性</td><td>好</td><td>有限</td></tr></tbody></table></div><h2 id="Hive系统架构"><a href="#Hive系统架构" class="headerlink" title="Hive系统架构"></a>Hive系统架构</h2><h3 id="用户接口模块"><a href="#用户接口模块" class="headerlink" title="用户接口模块"></a>用户接口模块</h3><h4 id="CLI"><a href="#CLI" class="headerlink" title="CLI"></a>CLI</h4><ul><li>Shell命令行</li><li>Karmasphere</li></ul><h4 id="HWI"><a href="#HWI" class="headerlink" title="HWI"></a>HWI</h4><ul><li>web接口</li><li>Hue</li></ul><h4 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h4><ul><li>Qubole</li><li>Hive 的Java，与使用传统数据库JDBC的方式类似</li></ul><h4 id="ODBC"><a href="#ODBC" class="headerlink" title="ODBC"></a>ODBC</h4><ul><li>Hive 的Java，与使用传统数据库JDBC的方式类似</li></ul><h4 id="Thrift-Server"><a href="#Thrift-Server" class="headerlink" title="Thrift Server"></a>Thrift Server</h4><ul><li>RPC调用</li></ul><h3 id="驱动模块"><a href="#驱动模块" class="headerlink" title="驱动模块"></a>驱动模块</h3><ul><li>编译器</li><li>优化器</li><li>执行器</li><li>解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。</li><li>生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行</li></ul><h3 id="元数据存储"><a href="#元数据存储" class="headerlink" title="元数据存储"></a>元数据存储</h3><ul><li>元数据存储模块（metastore）是一个独立的关系型数据库，目前只支持 mysql、derby。</li><li>Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等</li></ul><h2 id="Hive工作原理"><a href="#Hive工作原理" class="headerlink" title="Hive工作原理"></a>Hive工作原理</h2><h3 id="HQL转换成MapReduce作业的过程"><a href="#HQL转换成MapReduce作业的过程" class="headerlink" title="HQL转换成MapReduce作业的过程"></a>HQL转换成MapReduce作业的过程</h3><ul><li><a href="http://www.aboutyun.com/thread-20461-1-1.html" target="_blank" rel="noopener">HQL解析原理</a></li></ul><p><img src="/2019/10/12/shi-yong-gong-ju-zhi-hive/hive_to_mapreduce.png" alt="图：hive_to_mapreduce"></p><p><strong>注意：</strong></p><ul><li>当启动MR程序时，Hive本身不会生成MR算法程序，需要通过一个“Job执行计划”的XML文件驱动来执行内置的Mapper和Reducer模块</li><li>Hive通过和JobTracker通信来初始化MR任务</li></ul><h3 id="Hive-HA基本原理"><a href="#Hive-HA基本原理" class="headerlink" title="Hive HA基本原理"></a>Hive HA基本原理</h3><p><img src="/2019/10/12/shi-yong-gong-ju-zhi-hive/hiveHA.png" alt="图：hiveHA"></p><h1 id="四、impala"><a href="#四、impala" class="headerlink" title="四、impala"></a>四、impala</h1><ul><li>impala是由Cloudera公司开发的新型查询系统，提供SQL语义。</li><li>它的运行<strong>需要依赖Hive的元数据</strong>，能够查询PB级数据，在性能上比Hive高出3~30倍</li><li>impala不需要将查询语句转为MR程序运行，<strong>而是使用传统的分布式查询引擎直接到HDFS上查询数据</strong></li><li>详情请见</li></ul><h1 id="五、参考书籍"><a href="#五、参考书籍" class="headerlink" title="五、参考书籍"></a>五、参考书籍</h1><ul><li><a href="https://www.yiibai.com/Hive/" target="_blank" rel="noopener">易百教程</a></li><li><a href="https://study.163.com/course/courseMain.htm?courseId=1002887002" target="_blank" rel="noopener">大数据技术原理与应用</a></li></ul><h1 id="六、疑难解答"><a href="#六、疑难解答" class="headerlink" title="六、疑难解答"></a>六、疑难解答</h1><h2 id="1、Hive-在指定位置添加字段"><a href="#1、Hive-在指定位置添加字段" class="headerlink" title="1、Hive 在指定位置添加字段"></a>1、Hive 在指定位置添加字段</h2><ul><li><p>首先添加字段</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">alter table table_name add columns (c_time string comment '当前时间');<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>其次更改字段的顺序</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">alter table table_name change c_time c_time string after address;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>添加字段后，重写表数据后，查询新字段为null的情况</p><ul><li><a href="https://blog.csdn.net/lzxlfly/article/details/116788104" target="_blank" rel="noopener">参考</a></li></ul><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">alter table table_name drop partition(date_id='20210101')alter table table_name add partition(date_id='20210101')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><h2 id="2、Hive创建表时指定文件格式"><a href="#2、Hive创建表时指定文件格式" class="headerlink" title="2、Hive创建表时指定文件格式"></a>2、Hive创建表时指定文件格式</h2><ul><li><p><strong>TEXTFIEL</strong></p><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。<br>可结合Gzip、Bzip2使用（系统自动检查，执行查询时自动解压），但使用这种方式，Hive不会对数据进行切分，从而无法对数据进行并行操作。</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 固定格式create table test1(str STRING)  STORED AS TEXTFILE;-- 自定义格式create table test1(str STRING)  STORED ASINPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'OUTPUTFORMAT 'org.apache.hadoop.Hive.ql.io.HiveIgnoreKeyTextOutputFormat'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>SEQUENCEFILE</strong></p><p>SequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。<br>SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 固定格式create table test1(str STRING)  STORED AS SEQUENCEFILE;-- 自定义格式create table test1(str STRING)  STORED ASINPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'OUTPUTFORMAT 'org.apache.hadoop.Hive.ql.io.HiveSequenceFileOutputFormat'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><strong>RCFILE</strong></p><p>RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">-- 固定格式create table test1(str STRING)  STORED AS RCFILE;-- 自定义格式create table test1(str STRING)  STORED ASINPUTFORMAT 'org.apache.hadoop.Hive.ql.io.RCFileInputFormat'OUTPUTFORMAT 'org.apache.hadoop.Hive.ql.io.RCFileOutputFormat'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="3、Hive同时拆分多列为多行"><a href="#3、Hive同时拆分多列为多行" class="headerlink" title="3、Hive同时拆分多列为多行"></a>3、Hive同时拆分多列为多行</h2><ul><li><a href="https://stackoverflow.com/questions/37585638/Hive-split-delimited-columns-over-multiple-rows-select-based-on-position?rq=1" target="_blank" rel="noopener">问题链接</a></li></ul><p><strong>问题：</strong></p><p>I’m Looking for a way to split the column based on comma delimited data. Below is my dataset</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">id  col1  col21   5,6   7,8<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>I want to get the result</p><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">id col1 col21  5    71  6    8<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>答案：</strong></p><p>You can use <code>posexplode()</code> to create position index columns for your split arrays. Then, select only those rows where the position indices are equal.</p><pre><code>SELECT id, col3, col4  FROM test  lateral VIEW posexplode(split(col1,&#39;\002&#39;)) col1 AS pos3, col3  lateral VIEW posexplode(split(col2,&#39;\002&#39;)) col2 AS pos4, col4  WHERE pos3 = pos4;</code></pre><p>Output:</p><pre><code>id col3 col41  5    71  6    8</code></pre><p>==补充==</p><ul><li><a href="https://mp.weixin.qq.com/s/pyhKhXYzWMkdd3efyT5XPQ" target="_blank" rel="noopener">参考</a></li></ul><ol><li>单列explode</li></ol><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">select class,student_namefromdefault.classinfolateral view explode(split(student,',')) t as student_name<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>单列posexplode</li></ol><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">select class,student_index + 1 as student_index,student_namefromdefault.classinfolateral view posexplode(split(student,',')) t as student_index,student_name<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>多列explode</li></ol><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">select class,student_name,student_scorefromdefault.classinfolateral view posexplode(split(student,',')) sn as student_index_sn,student_namelateral view posexplode(split(score,',')) sc as student_index_sc,student_scorewhere student_index_sn = student_index_sc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4、-Hive或spark中执行sql字符常量包含-时会报错"><a href="#4、-Hive或spark中执行sql字符常量包含-时会报错" class="headerlink" title="4、 Hive或spark中执行sql字符常量包含;时会报错"></a>4、 Hive或spark中执行sql字符常量包含<code>;</code>时会报错</h2><p>比如</p><blockquote><p>select instr(‘abc;abc’, ‘;’);</p></blockquote><p>报错</p><blockquote><p>NoViableAltException(-1@[147:1: selectExpression : ( expression | tableAllColumns );])</p></blockquote><p><strong>修改：</strong>需要将<code>;</code>改为<code>ascii</code></p><blockquote><p>select instr(‘abc\073abc’, ‘\073’);</p></blockquote><h2 id="5、如何在-Apache-Hive-中解析-Json-数组"><a href="#5、如何在-Apache-Hive-中解析-Json-数组" class="headerlink" title="5、如何在 Apache Hive 中解析 Json 数组"></a>5、如何在 Apache Hive 中解析 Json 数组</h2><h3 id="问题1：从json字符串中解析一个字段-get-json-object"><a href="#问题1：从json字符串中解析一个字段-get-json-object" class="headerlink" title="问题1：从json字符串中解析一个字段-get_json_object"></a>问题1：从<strong>json字符串</strong>中解析一个字段-get_json_object</h3><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">hive>  SELECT get_json_object('{"website":"www.iteblog.com","name":"过往记忆"}', '$.website');OKwww.iteblog.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="问题2：从json字符串中解析多个字段-json-tuple"><a href="#问题2：从json字符串中解析多个字段-json-tuple" class="headerlink" title="问题2：从json字符串中解析多个字段-json_tuple"></a>问题2：从<strong>json字符串</strong>中解析多个字段-json_tuple</h3><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">hive> SELECT json_tuple('{"website":"www.iteblog.com","name":"过往记忆"}', 'website', 'name');OKwww.iteblog.com 过往记忆<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="问题3：从json数组中解析某一个字段-get-json-object"><a href="#问题3：从json数组中解析某一个字段-get-json-object" class="headerlink" title="问题3：从json数组中解析某一个字段-get_json_object"></a>问题3：从<strong>json数组</strong>中解析某一个字段-get_json_object</h3><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">hive> SELECT get_json_object('[{"website":"www.iteblog.com","name":"过往记忆"}, {"website":"carbondata.iteblog.com","name":"carbondata 中文文档"}]', '$[0].website');OKwww.iteblog.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>注意：</strong>这里与参考链接<a href="https://www.iteblog.com/archives/2362.html" target="_blank" rel="noopener">如何在 Apache Hive 中解析 Json 数组</a>中不同的是，因为我用的是<code>sparkSql</code>，所以我用的是<code>$[0].website</code>，而参考链接使用的<code>$.[0].website</code>，我按照<strong>参考链接给的方法在Hive也select不出答案（可能是hive版本不同吧）</strong></p><h3 id="问题4：从json数组中解析多个字段-先explode再get-json-object或json-tuple"><a href="#问题4：从json数组中解析多个字段-先explode再get-json-object或json-tuple" class="headerlink" title="问题4：从json数组中解析多个字段-先explode再get_json_object或json_tuple"></a>问题4：从<strong>json数组</strong>中解析多个字段-先explode再get_json_object或json_tuple</h3><ul><li>explode将<strong>json数组</strong>用一行拆分成多行</li><li>然后再对其进行<strong>json字符串</strong>解析</li></ul><p>详情请参考<a href="https://www.iteblog.com/archives/2362.html" target="_blank" rel="noopener">如何在 Apache Hive 中解析 Json 数组</a></p><h2 id="6、删除表但是没删除hdfs数据，重建表并关联hdfs数据"><a href="#6、删除表但是没删除hdfs数据，重建表并关联hdfs数据" class="headerlink" title="6、删除表但是没删除hdfs数据，重建表并关联hdfs数据"></a>6、删除表但是没删除hdfs数据，重建表并关联hdfs数据</h2><p><a href="https://stackoverflow.com/questions/40043986/hdinsight-hive-msck-repair-table-table-name-throwing-error" target="_blank" rel="noopener">参考1</a></p><p><a href="https://blog.csdn.net/BabyFish13/article/details/79169496" target="_blank" rel="noopener">参考2</a></p><h2 id="7、HIVE将表划分测试集与训练集的方法"><a href="#7、HIVE将表划分测试集与训练集的方法" class="headerlink" title="7、HIVE将表划分测试集与训练集的方法"></a>7、HIVE将表划分测试集与训练集的方法</h2><ul><li><p><a href="https://www.thinbug.com/q/23548892" target="_blank" rel="noopener">将Hive表拆分为测试集和训练集</a></p></li><li><p><a href="https://www.iteblog.com/archives/1996.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1996.html</a></p></li></ul><pre class="line-numbers language-lang-hiveql"><code class="language-lang-hiveql">from ( select *, (rand() * 100 <= x) as is_test_set from my_table) tinsert overwrite directory '/test_set' select * where is_test_set = trueinsert overwrite directory '/training_set' select * where is_test_set = false;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="8、HIVE日期操作"><a href="#8、HIVE日期操作" class="headerlink" title="8、HIVE日期操作"></a>8、HIVE日期操作</h2><h3 id="获取时间戳"><a href="#获取时间戳" class="headerlink" title="获取时间戳"></a>获取时间戳</h3><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">select unix_timestamp('2015-04-30 13:51:20');select unix_timestamp('2015-04-30');<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="转格式"><a href="#转格式" class="headerlink" title="转格式"></a>转格式</h3><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">select from_unixtime(1323308943,'yyyyMMdd');select from_unixtime(unix_timestamp('2015-04-30', 'yyyy-MM-dd'), 'yyyyMMdd');<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="9、HIVE中distinct和group-by的区别"><a href="#9、HIVE中distinct和group-by的区别" class="headerlink" title="9、HIVE中distinct和group by的区别"></a>9、HIVE中<code>distinct</code>和<code>group by</code>的区别</h2><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">前者是去重，后者是分组reduce作业个数不同，distinct会在一个reduce中去重<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="10、count-和-count-1-和count-列名-区别"><a href="#10、count-和-count-1-和count-列名-区别" class="headerlink" title="10、count(*) 和 count(1)和count(列名)区别"></a>10、count(*) 和 count(1)和count(列名)区别</h2><pre class="line-numbers language-lang-markdown"><code class="language-lang-markdown">* 执行效果上 ：  count(\*)包括了所有的列，相当于行数，在统计结果的时候， 不会忽略列值为NULL  count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候， 不会忽略列值为NULL  count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数， 即某个字段值为NULL时，不统计。* 执行效率上：  列名为主键，count(列名)会比count(1)快  列名不为主键，count(1)会比count(列名)快  如果表多个列并且没有主键，则 count(1)的执行效率优于 count(\*) 如果有主键，则 select count(主键)的执行效率是最优的  如果表只有一个字段，则 select count(\*) 最优。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="11、求累计值"><a href="#11、求累计值" class="headerlink" title="11、求累计值"></a>11、求累计值</h2><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><div class="table-container"><table><thead><tr><th>ds</th><th>cash</th><th>累加值</th></tr></thead><tbody><tr><td>20200101</td><td>10</td><td>10</td></tr><tr><td>20200102</td><td>20</td><td>30</td></tr><tr><td></td><td></td></tr></tbody></table></div><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">SELECTds,sum(cash),sum(sum(cash)) over(distribute by substr(ds, 1, 6) sort by ds rows between UNBOUNDED PRECEDING AND CURRENT ROW) as cumulativefrom table_nameWHERE ds BETWEEN '20200901' AND '20201020'group by dsorder by ds<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="12、四舍五入"><a href="#12、四舍五入" class="headerlink" title="12、四舍五入"></a>12、四舍五入</h2><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">selectround(cast(prob as double)),round(cast(prob as double), 2),floor(cast(prob as double)), --向下取整ceil(cast(prob as double)), --向上取整<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="13、设置任务优先级"><a href="#13、设置任务优先级" class="headerlink" title="13、设置任务优先级"></a>13、设置任务优先级</h2><pre class="line-numbers language-lang-hive"><code class="language-lang-hive">SET mapreduce.job.priority=VERY_HIGH;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><a href="https://blog.csdn.net/yisun123456/article/details/82857150" target="_blank" rel="noopener">参考链接：yarn设置优先级</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算广告之理论2</title>
      <link href="/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/"/>
      <url>/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/image-20210621155717047.png" alt="image-20210621155717047"></p><h1 id="一、名词解释"><a href="#一、名词解释" class="headerlink" title="一、名词解释"></a>一、名词解释</h1><p><img src="/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/rtb_china.jpg" alt="图：rtb_china"></p><h2 id="广告主"><a href="#广告主" class="headerlink" title="广告主"></a>广告主</h2><p>有广告需求的客户公司，也称为需求方</p><h2 id="媒体"><a href="#媒体" class="headerlink" title="媒体"></a>媒体</h2><p>可以是网站或者<code>app</code>，即有多余的广告位可以出售，也称为供给方</p><h2 id="RTB：（Real-Time-Bidding，实时竞价）"><a href="#RTB：（Real-Time-Bidding，实时竞价）" class="headerlink" title="RTB：（Real Time Bidding，实时竞价）"></a>RTB：（<code>Real Time Bidding</code>，实时竞价）</h2><p>综合利用算法、大数据技术在网站或移动应用上对在线的流量实时评估价值，然后出价的竞价技术</p><h2 id="DSP（需求方平台）"><a href="#DSP（需求方平台）" class="headerlink" title="DSP（需求方平台）"></a>DSP（需求方平台）</h2><p>为各广告主或者代理商提供实时竞价投放平台，可以在该平台上管理广告活动及其投放策略，并且通过技术和算法优化投放效果，从中获得收益</p><h2 id="SSP（供应方平台）"><a href="#SSP（供应方平台）" class="headerlink" title="SSP（供应方平台）"></a>SSP（供应方平台）</h2><p>为各媒体提供一致、集成的广告位库存管理环境</p><h2 id="DMP（数据管理平台）"><a href="#DMP（数据管理平台）" class="headerlink" title="DMP（数据管理平台）"></a>DMP（数据管理平台）</h2><p>整合各方数据并提供数据分析，数据管理、数据调用等，用来指导广告主进行广告优化和投放决策</p><h2 id="ADX（Ad-Exchange，广告交易平台）"><a href="#ADX（Ad-Exchange，广告交易平台）" class="headerlink" title="ADX（Ad Exchange，广告交易平台）"></a>ADX（Ad Exchange，广告交易平台）</h2><p>同时接入了大量的<code>DSP</code>和<code>SSP</code>，给双方提供一个“交易场所”，将<code>SSP</code>方的广告展示需求以拍卖的方式卖给<code>DSP</code>方。可以类比于股票交易中的证券大厅角色</p><h2 id="CTR（点击率）"><a href="#CTR（点击率）" class="headerlink" title="CTR（点击率）"></a>CTR（点击率）</h2><p>广告点击与广告展现的比例，这是广告主、各大<code>DSP</code>厂商都非常重视的一个数字，从技术角度看，这一数字可以影响到广告的排序、出价等环节，从业务来看，这是运营人员的考核指标之一</p><ul><li><strong>CTR=点击量/展现量 *100%</strong></li></ul><h2 id="CVR（转化率）"><a href="#CVR（转化率）" class="headerlink" title="CVR（转化率）"></a>CVR（转化率）</h2><p>转化（主要由广告主定义，可以是一次下单或是一次下载<code>app</code>等）次数与到达次数的比例</p><ul><li><strong>CVR= 转化量/点击量 *100%</strong></li></ul><h2 id="eCPM（千次展示期望收入）"><a href="#eCPM（千次展示期望收入）" class="headerlink" title="eCPM（千次展示期望收入）"></a>eCPM（千次展示期望收入）</h2><p>点击率 * 点击价值，这个数字在计算广告中是核心指标，涉及到对召回广告的排序策略，以及最终出价策略</p><h2 id="CPM（Cost-per-mile）"><a href="#CPM（Cost-per-mile）" class="headerlink" title="CPM（Cost per mile）"></a>CPM（Cost per mile）</h2><p>每千次展现收费，最常见的广告模式，即不考虑点击次数、转化次数，只要广告在网站上被展现给一千个人看到就收费，是大型网站变现的最有效的方式。对广告主来说，适合于注重推广品牌的广告，力求最快最广的触及大众</p><ul><li><strong>CPM=广告费/展现量 *1000</strong></li></ul><h2 id="CPC（Cost-per-click）"><a href="#CPC（Cost-per-click）" class="headerlink" title="CPC（Cost per click）"></a>CPC（Cost per click）</h2><p>每次点击收费，无论广告被展现了多少次，只要没有产生点击就不收费。对于广告主来说选择 <code>CPC</code> 模式可以有助于提升点击量、发现潜在用户，进而可以真正做到精准营销；对于广大<code>DSP</code>厂商来说，这种收费模式也是获取利润的最有效来源之一</p><ul><li><strong>CPC=广告费/点击量</strong></li></ul><h2 id="CPA（Cost-per-action）"><a href="#CPA（Cost-per-action）" class="headerlink" title="CPA（Cost per action）"></a>CPA（Cost per action）</h2><p>每次动作收费，此处的动作一般定义为转化，可以是注册、咨询、交易、下载、留言等等，按照转化的数量来收费。对于广告主来说，这是性价比较高的一种收费方式，但是对于DPS和媒体方来说，想要把这种收费做好，却是有相当的难度。因此，目前也只有大厂或者技术实力深厚的<code>DSP</code>厂商才有能力接这种单子</p><ul><li><strong>CPA=广告费/转化量</strong></li></ul><h2 id="LTV（Life-Time-Value）"><a href="#LTV（Life-Time-Value）" class="headerlink" title="LTV（Life Time Value）"></a>LTV（Life Time Value）</h2><p>客户终生价值。是公司从用户所有的互动中所得到的全部经济收益的总和</p><ul><li>LTV的计算涉及到顾客保持率、顾客消费率、变动成本、获得成本、贴现率等信息的正确取得。</li><li>顾客保留率（retention rate，RR）= 本年度的顾客总数 / 上年度的顾客总数；</li><li>顾客消费率（spending rate，SR）= 顾客总消费额 / 顾客总数；</li><li>变动成本（variable cost，VC）= 产品成本 + 服务<a href="https://www.52by.com/article_tag/guanli" target="_blank" rel="noopener">管理</a>费用 + 信用卡成本等；</li><li>获得成本（acquisition cost，AC）= 本年度广告、<a href="https://www.52by.com/article_tag/cuxiao" target="_blank" rel="noopener">促销</a>费用 / 本年度顾客总数；</li><li>净利润（gross profit，GP）= 总收入 – 总成本；</li><li>贴现率（discount rate，DR）= [1 +（风险系数*银行利率）]n ；</li><li>利润净现值（net present value profit，NPV）= GP / DR ；</li><li>累积NPV = 特定时间内每年NPV 的总和；</li><li><strong>顾客终身价值（LTV）= 累积NPV / 顾客总数</strong></li></ul><h2 id="采购交易平台-Trading-desk"><a href="#采购交易平台-Trading-desk" class="headerlink" title="采购交易平台(Trading desk)"></a>采购交易平台(Trading desk)</h2><p>程序化独立媒体交易平台，功能类似于 DSP，通过对接多个 DSP来进行广告的优化投放，它能够将各类购买方式统一管理和预算分配，对品牌广告的有效管理甄别垃圾媒体。</p><h2 id="程序化创意平台-programmatic-creative"><a href="#程序化创意平台-programmatic-creative" class="headerlink" title="程序化创意平台(programmatic creative)"></a>程序化创意平台(programmatic creative)</h2><p>动态实时创意优化平台，它帮助广告主评估和优化广告创意，能更好的在提升在实时竞价广告投放中的效果。</p><h2 id="广告验证-ad-verification"><a href="#广告验证-ad-verification" class="headerlink" title="广告验证(ad verification)"></a>广告验证(ad verification)</h2><p>安全有效的购买广告的手段，被许多广告代理视作广告策略中必不可少的一个组成部分。</p><h1 id="二、竞价流程"><a href="#二、竞价流程" class="headerlink" title="二、竞价流程"></a>二、竞价流程</h1><p><img src="/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/bid_process.jpg" alt="图：bid_process"></p><p><img src="/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/bid_process1.png" alt="图：bid_process-简单版"></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol><li>用户通过浏览器访问网站</li><li>浏览器发送广告请求给网站服务器，即媒体</li><li>媒体将该曝光展示通过SSP发送给Adx</li><li>Adx组织一次竞价，将本次的竞价请求通知给与之对接的所有DSP方，并传输用户ID、用户IP以及广告位信息等等</li><li>各家DSP监听服务收到Adx发来的竞价请求后，根据发送来的当前用户的信息、上下文信息、代理的广告主信息对照投放需求，并与自家的DMP数据管理平台或用户数据库来评估该请求，并通过DSP的竞价引擎判断是否参与竞价，若参与则给出竞标价格</li><li>Adx收到所有出价响应或截止时间后（通常规定是100ms）根据出价排序，通知出价最高的DSP胜出，同时将胜者的竞价信息告知SSP</li><li>胜者DSP会收到Adx发送的竞价消息（WinNotice），表示该次展现已经竞价成功。DSP此时将广告物料传送至SSP，SSP 发送广告曝光请求给广告服务器</li><li>广告服务器将广告投送到用户浏览的页面</li></ol><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>有两点需要说明：</p><ol><li>以上流程需在100毫秒之内完成，也就是一眨眼的功夫都不到，这样才能让访问网站的用户马上看到广告。</li><li>虽然竞价最后的胜者是出价最高的DSP，但其实际支付的价格是出价第二高的报价再加上一个最小值，这就是著名的广义二阶拍卖(GSP)。这种定价模式的好处主要是为了避免各家DSP在多次竞价中，下一次的出价只需比上一次的胜出价格高一点点即可，从而导致出价越来越低。在GSP模式下，每个胜者只需要支付次高出价的金额，这样各家DSP也就没有动力相互压价。GSP是一种稳定的竞价方式，可操作性很强，现阶段几乎所有的互联网广告平台都使用这一种竞价方式。</li></ol><h1 id="三、技术细节"><a href="#三、技术细节" class="headerlink" title="三、技术细节"></a>三、技术细节</h1><h2 id="典型广告系统架构"><a href="#典型广告系统架构" class="headerlink" title="典型广告系统架构"></a>典型广告系统架构</h2><p><img src="/2019/10/12/ji-suan-guang-gao-zhi-li-lun-2/structure.jpg" alt="图：structure"></p><p><strong>（1）受众定向平台：</strong>利用数据挖掘、机器学习等技术进行受众定向、点击率预估等工作。具体可以细分为以下几个模块：</p><ul><li><strong>会话日志生成（Session log generation）</strong>：从各个数据平台收集日志，并最终根据用户ID为主键汇集成一份统一存储格式的日志，为后续的数据分析、算法构建提供数据来源。</li><li><strong>行为定向（Audience targeting）</strong>：从会话日志中提取数据，利用数据挖掘、机器学习等算法建模，刻画出用户的行为模式、兴趣点等，最终为用户打上结构化标签，以供广告定向投放使用。</li><li><strong>点击率建模（Click modeling）</strong>：从会话日志中提取预定义的特征，训练点击率模型，加载到缓存中供线上投放系统决策时使用。</li><li><strong>商业智能模块（Business Intelligence）</strong>：包括ETL（Extract-Transform-Load）过程、Dashboard、cube。BI系统可以为决策者提供直观且即时的数据分析，而算法对决策者来说相当于黑盒，因此，设计一个良好的BI系统可以为决策提供有力的帮助。</li></ul><p><strong>（2）高并发的投送系统</strong>：即在线的广告投放机（Ad server），主要任务是与各模块交互，将它们串联起来完成在线的广告投放决策。特点是高并发，要做到10 ms级别的实时决策，百亿次/天的广告投放系统。具体可细分为以下几个模块：</p><ul><li><strong>广告检索（Ad retrieval）</strong>：也称为广告召回，即根据用户属性以及页面上下文属性从广告索引（Ad index）中查找符合条件的广告候选。</li><li><strong>广告排序（Ad ranking）</strong>：主要功能是对广告检索模块送来的广告候选集计算eCPM，并按照值的大小倒排。eCPM的计算依赖于受众定向平台离线计算好的点击率。由于最终投放出的广告都是来自于排序的结果，因此这一模块也是至关重要，成为各种算法模型和策略大展身手的地方。</li><li><strong>收益管理（Yield management）</strong>：将局部广告排序的结果进一步优化，以做到全局最优收益。</li></ul><p><strong>（3）数据高速公路：</strong>联通在线与离线。作用是准实时地将日志推送到其它平台上，一是快速地反馈到线上系统中，二是给BI人员快速看结果。它还可能收集其它平台的日志，比如搜索广告会收集搜索日志。</p><p><strong>（4）流式计算平台：</strong>主要功能是对在线数据的处理，做到准实时的挖掘和反馈，解决那些离线计算平台无法快速响应的计算问题。具体又可分为：</p><ul><li><strong>实时受众定向（Real-time targeting）</strong>和<strong>实时点击反馈（Real-time click feedback）</strong>：对用户的实时行为进行计算，如实时更新点击率模型、动态调整用户标签，进而更好的适应线上环境。</li><li><strong>计费（Billing）</strong>：该模块掌管着广告系统的“钱袋子”，运行的准确性和稳定性直接影响了系统的收益。在广告投放过程中，经常会遇到投放预算用完的情况，这时计费模块必须及时反应，采取例如通知索引系统暂时将广告下线的办法，避免带来损失。</li><li><strong>反作弊（Anti-spam）</strong>：利用算法和人工规则等实时判断流量来源中是否有作弊流量，并将这部分流量从后续的计价和统计中去掉，是广告业务非常重要的部分。</li></ul><h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><p><strong>（1）<code>Hadoop</code>：</strong>详情请见</p><p><strong>（2）<code>Spark</code>：</strong>详情请见</p><p><strong>（3）<code>Lucene</code>：</strong>详情请见</p><p><strong>（4）<code>Storm</code>：</strong>详情请见</p><p><strong>（5）<code>ZooKeeper</code>：</strong>详情请见</p><h2 id="核心算法"><a href="#核心算法" class="headerlink" title="核心算法"></a>核心算法</h2><p>详情请见<a href="https://zhuanlan.zhihu.com/p/60544294" target="_blank" rel="noopener">参考文章</a>核心算法篇</p><h1 id="四、-参考书籍-amp-论文"><a href="#四、-参考书籍-amp-论文" class="headerlink" title="四、 参考书籍&amp;论文"></a>四、 参考书籍&amp;论文</h1><ul><li>《计算广告》 刘鹏 / 王超</li><li><a href="https://zhuanlan.zhihu.com/p/60544294" target="_blank" rel="noopener">计算广告系统算法与架构综述</a></li><li><a href="https://dolantinlist.github.io/2018/05/21/RTB广告竞价系统的算法介绍/" target="_blank" rel="noopener">RTB广告竞价系统的算法介绍</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算广告 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资源整合之学习清单</title>
      <link href="/2019/10/10/zi-yuan-zheng-he-zhi-xue-xi-qing-dan/"/>
      <url>/2019/10/10/zi-yuan-zheng-he-zhi-xue-xi-qing-dan/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/10/zi-yuan-zheng-he-zhi-xue-xi-qing-dan/image-20210621143735727.png" alt="image-20210621143735727"></p><h1 id="一、实用工具"><a href="#一、实用工具" class="headerlink" title="一、实用工具"></a>一、实用工具</h1><h2 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h2><h3 id="C"><a href="#C" class="headerlink" title="C"></a>C</h3><h3 id="C-1"><a href="#C-1" class="headerlink" title="C"></a>C</h3><h3 id="C-2"><a href="#C-2" class="headerlink" title="C ++"></a>C ++</h3><h3 id="JAVA"><a href="#JAVA" class="headerlink" title="JAVA"></a>JAVA</h3><p>（1）教程</p><ul><li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744" target="_blank" rel="noopener">廖大神</a></li></ul><p>（2）笔记</p><h3 id="PYTHON"><a href="#PYTHON" class="headerlink" title="PYTHON"></a>PYTHON</h3><h2 id="大数据工具"><a href="#大数据工具" class="headerlink" title="大数据工具"></a>大数据工具</h2><h3 id="查询引擎"><a href="#查询引擎" class="headerlink" title="查询引擎"></a>查询引擎</h3><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><h4 id="PIG"><a href="#PIG" class="headerlink" title="PIG"></a>PIG</h4><h4 id="HIVE"><a href="#HIVE" class="headerlink" title="HIVE"></a>HIVE</h4><h4 id="PRESTO"><a href="#PRESTO" class="headerlink" title="PRESTO"></a>PRESTO</h4><h4 id="IMPALA"><a href="#IMPALA" class="headerlink" title="IMPALA"></a>IMPALA</h4><h4 id="SPARK-SQL"><a href="#SPARK-SQL" class="headerlink" title="SPARK SQL"></a>SPARK SQL</h4><h3 id="计算工具"><a href="#计算工具" class="headerlink" title="计算工具"></a>计算工具</h3><h4 id="流式计算"><a href="#流式计算" class="headerlink" title="流式计算"></a>流式计算</h4><h5 id="APACHE-FLINK"><a href="#APACHE-FLINK" class="headerlink" title="APACHE FLINK"></a>APACHE FLINK</h5><h5 id="TWITTER-STORM"><a href="#TWITTER-STORM" class="headerlink" title="TWITTER STORM"></a>TWITTER STORM</h5><h5 id="SPARK-STREAMING"><a href="#SPARK-STREAMING" class="headerlink" title="SPARK STREAMING"></a>SPARK STREAMING</h5><h4 id="迭代计算"><a href="#迭代计算" class="headerlink" title="迭代计算"></a>迭代计算</h4><h5 id="TWISTER"><a href="#TWISTER" class="headerlink" title="TWISTER"></a>TWISTER</h5><h5 id="HALOOP"><a href="#HALOOP" class="headerlink" title="HALOOP"></a>HALOOP</h5><h5 id="APACHE-HAMA"><a href="#APACHE-HAMA" class="headerlink" title="APACHE HAMA"></a>APACHE HAMA</h5><h5 id="SPARK-GRAPHX"><a href="#SPARK-GRAPHX" class="headerlink" title="SPARK GRAPHX"></a>SPARK GRAPHX</h5><h5 id="APACHE-GIRAPH"><a href="#APACHE-GIRAPH" class="headerlink" title="APACHE GIRAPH"></a>APACHE GIRAPH</h5><h4 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a>离线计算</h4><h5 id="APACHE-FLINK-1"><a href="#APACHE-FLINK-1" class="headerlink" title="APACHE FLINK"></a>APACHE FLINK</h5><h5 id="BERKELEY-SPARK"><a href="#BERKELEY-SPARK" class="headerlink" title="BERKELEY SPARK"></a>BERKELEY SPARK</h5><h5 id="HADOOP-MAPREDUCE"><a href="#HADOOP-MAPREDUCE" class="headerlink" title="HADOOP MAPREDUCE"></a>HADOOP MAPREDUCE</h5><h3 id="存储工具"><a href="#存储工具" class="headerlink" title="存储工具"></a>存储工具</h3><h4 id="KEY-VALUE存储"><a href="#KEY-VALUE存储" class="headerlink" title="KEY-VALUE存储"></a>KEY-VALUE存储</h4><h5 id="REDIS"><a href="#REDIS" class="headerlink" title="REDIS"></a>REDIS</h5><h5 id="HBASE"><a href="#HBASE" class="headerlink" title="HBASE"></a>HBASE</h5><h4 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h4><h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><h5 id="MONGODB"><a href="#MONGODB" class="headerlink" title="MONGODB"></a>MONGODB</h5><h3 id="资源管理"><a href="#资源管理" class="headerlink" title="资源管理"></a>资源管理</h3><h4 id="ZOOKEEPER"><a href="#ZOOKEEPER" class="headerlink" title="ZOOKEEPER"></a>ZOOKEEPER</h4><h4 id="HADOOP-YARN"><a href="#HADOOP-YARN" class="headerlink" title="HADOOP YARN"></a>HADOOP YARN</h4><h4 id="TWITTER-MESOS"><a href="#TWITTER-MESOS" class="headerlink" title="TWITTER MESOS"></a>TWITTER MESOS</h4><h3 id="日志收集系统"><a href="#日志收集系统" class="headerlink" title="日志收集系统"></a>日志收集系统</h3><h4 id="KIBANA"><a href="#KIBANA" class="headerlink" title="KIBANA"></a>KIBANA</h4><h4 id="LOGSTASH"><a href="#LOGSTASH" class="headerlink" title="LOGSTASH"></a>LOGSTASH</h4><h4 id="ELASTIC-SEARCH"><a href="#ELASTIC-SEARCH" class="headerlink" title="ELASTIC SEARCH"></a>ELASTIC SEARCH</h4><h4 id="CLOUDERA-FLUME"><a href="#CLOUDERA-FLUME" class="headerlink" title="CLOUDERA FLUME"></a>CLOUDERA FLUME</h4><h3 id="消息系统"><a href="#消息系统" class="headerlink" title="消息系统"></a>消息系统</h3><h4 id="APACHE-KAFKA"><a href="#APACHE-KAFKA" class="headerlink" title="APACHE KAFKA"></a>APACHE KAFKA</h4><h3 id="版本控制工具"><a href="#版本控制工具" class="headerlink" title="版本控制工具"></a>版本控制工具</h3><h4 id="GIT"><a href="#GIT" class="headerlink" title="GIT"></a>GIT</h4><h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><h4 id="SHELL"><a href="#SHELL" class="headerlink" title="SHELL"></a>SHELL</h4><h1 id="二、算法学习"><a href="#二、算法学习" class="headerlink" title="二、算法学习"></a>二、算法学习</h1><h2 id="推荐算法"><a href="#推荐算法" class="headerlink" title="推荐算法"></a>推荐算法</h2><h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><h1 id="三、推荐书籍"><a href="#三、推荐书籍" class="headerlink" title="三、推荐书籍"></a>三、推荐书籍</h1>]]></content>
      
      
      <categories>
          
          <category> 资源整合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习清单 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
