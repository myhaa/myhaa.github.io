---
title: æ·±åº¦å­¦ä¹ ä¹‹äººè„¸æ£€æµ‹
author: Myhaa
top: false
cover: false
toc: true
mathjax: true
categories: æ·±åº¦å­¦ä¹ 
tags:
  - äººè„¸æ£€æµ‹
date: 2022-02-14 10:45:18
img:
coverImg:
password:
summary: å›¾ç‰‡æˆ–è€…è§†é¢‘äººè„¸æ£€æµ‹åŠå±æ€§åˆ†æ
---

# äººè„¸æ£€æµ‹

* [github topics](https://github.com/topics/face-detection?o=desc&s=stars)
* [face_recognition](https://github.com/ageitgey/face_recognition): æœ¬é¡¹ç›®æ˜¯ä¸–ç•Œä¸Šæœ€ç®€æ´çš„äººè„¸è¯†åˆ«åº“ï¼Œä½ å¯ä»¥ä½¿ç”¨Pythonå’Œå‘½ä»¤è¡Œå·¥å…·æå–ã€è¯†åˆ«ã€æ“ä½œäººè„¸ã€‚æœ¬é¡¹ç›®çš„äººè„¸è¯†åˆ«æ˜¯åŸºäºä¸šå†…é¢†å…ˆçš„C++å¼€æºåº“ dlibä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨Labeled Faces in the Wildäººè„¸æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼Œæœ‰é«˜è¾¾99.38%çš„å‡†ç¡®ç‡ã€‚ä½†å¯¹å°å­©å’Œäºšæ´²äººè„¸çš„è¯†åˆ«å‡†ç¡®ç‡å°šå¾…æå‡ã€‚
* [deepface](https://github.com/serengil/deepface): <font color='red'>ä¸€å¼ å›¾ç‰‡ä¸€å¼ è„¸</font> Deepface is a lightweight face recognition and facial attribute analysis (age, gender, emotion and race) framework for python. It is a hybrid face recognition framework wrapping state-of-the-art models: VGG-Face, Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace and Dlib.
* [yolov5](https://github.com/ultralytics/yolov5): YOLOv5 ğŸš€ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.

# äººè„¸æ£€æµ‹å’Œå±æ€§åˆ†æByç™¾åº¦äº‘

* [ç™¾åº¦äº‘å…¥å£](https://cloud.baidu.com/product/face/detect)


```python
import requests
import base64
from PIL import Image
import matplotlib.pyplot as plt

image_path = './datasets/image/face_detection.jpg'
```

## è·å–access_tokenå‡½æ•°


```python
def func_face_get_baidu_access_token():
    """
    è·å–baiduçš„access_token
    :return access_token
    """
    # encoding:utf-8
    api_key = 'you_api_key'
    secret_key = 'you_secret_key'
    
    # client_id ä¸ºå®˜ç½‘è·å–çš„AKï¼Œ client_secret ä¸ºå®˜ç½‘è·å–çš„SK
    # host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=ã€å®˜ç½‘è·å–çš„AKã€‘&client_secret=ã€å®˜ç½‘è·å–çš„SKã€‘'
    host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=%s&client_secret=%s' % (api_key, secret_key)
    response = requests.get(host)
    access_token = ''
    if response:
        res = response.json()
        print(res)
        access_token = res.get('access_token', '')
    return access_token
```


```python
access_token = func_face_get_baidu_access_token()
print(access_token)
```

    {'refresh_token': '25.6fc065b464989fce6cbd41d4e6ae1b0a.315360000.1959759303.282335-25143866', 'expires_in': 2592000, 'session_key': '9mzdA5Pj7ZKeP5jo0GEKduVWm7i9fwTHWSpG06S/723YxM+caN9pBehP4cuPHZ3duhWbbqJ27F7p1oAVdqi/11rl5aG07g==', 'access_token': '24.896a37769e0b9eb3ee4548f4c8259ec7.2592000.1646991303.282335-25143866', 'scope': 'public vis-classify_dishes vis-classify_car brain_all_scope vis-classify_animal vis-classify_plant brain_object_detect brain_realtime_logo brain_dish_detect brain_car_detect brain_animal_classify brain_plant_classify brain_ingredient brain_advanced_general_classify brain_custom_dish vis-faceverify_FACE_V3 brain_poi_recognize brain_vehicle_detect brain_redwine brain_currency brain_vehicle_damage brain_multi_ object_detect wise_adapt lebo_resource_base lightservice_public hetu_basic lightcms_map_poi kaidian_kaidian ApsMisTest_Testæƒé™ vis-classify_flower lpq_å¼€æ”¾ cop_helloScope ApsMis_fangdi_permission smartapp_snsapi_base smartapp_mapp_dev_manage iop_autocar oauth_tp_app smartapp_smart_game_openapi oauth_sessionkey smartapp_swanid_verify smartapp_opensource_openapi smartapp_opensource_recapi fake_face_detect_å¼€æ”¾Scope vis-ocr_è™šæ‹Ÿäººç‰©åŠ©ç† idl-video_è™šæ‹Ÿäººç‰©åŠ©ç† smartapp_component smartapp_search_plugin avatar_video_test b2b_tp_openapi b2b_tp_openapi_online', 'session_secret': '35e5528d67d5a4f50022fa47ad1c590e'}
    24.896a37769e0b9eb3ee4548f4c8259ec7.2592000.1646991303.282335-25143866


## è·å–è¯†åˆ«ç»“æœå‡½æ•°


```python
def func_face_get_face_detection_result(image_path, access_token):
    """
    äººè„¸æ£€æµ‹
    :param image_path: å›¾ç‰‡åœ°å€
    :param access_token: é‰´æƒ
    :return res: æ£€æµ‹ç»“æœ
    """
    request_url = "https://aip.baidubce.com/rest/2.0/face/v3/detect"

    # äºŒè¿›åˆ¶æ–¹å¼æ‰“å¼€å›¾ç‰‡æ–‡ä»¶
    f = open(image_path, 'rb')
    img = base64.b64encode(f.read())

    params = {"image":img, "image_type": "BASE64", "max_face_num": 5, "face_field": "age,gender"}
    # params = "{\"image\":\"027d8308a2ec665acb1bdf63e513bcb9\",\"image_type\":\"FACE_TOKEN\",\"face_field\":\"faceshape,facetype\"}"
    
    request_url = request_url + "?access_token=" + access_token
    headers = {'content-type': 'application/json'}
    response = requests.post(request_url, data=params, headers=headers)
    res = {}
    if response:
        res = response.json()
    response.close()
    return res
```


```python
print(func_face_get_face_detection_result(image_path, access_token=access_token))
```

    {'error_code': 0, 'error_msg': 'SUCCESS', 'log_id': 2103952089, 'timestamp': 1644399303, 'cached': 0, 'result': {'face_num': 3, 'face_list': [{'face_token': '8e645a3f8b54fbf6b139e021c55dc7f0', 'location': {'left': 561.41, 'top': 221.47, 'width': 65, 'height': 68, 'rotation': 1}, 'face_probability': 1, 'angle': {'yaw': 77, 'pitch': 15, 'roll': -11.89}, 'age': 24, 'gender': {'type': 'male', 'probability': 1}}, {'face_token': '6db2d69550c91ed03b2b408ca15dd591', 'location': {'left': 756.48, 'top': 251.15, 'width': 65, 'height': 68, 'rotation': 14}, 'face_probability': 1, 'angle': {'yaw': 76.54, 'pitch': 18.85, 'roll': -1.31}, 'age': 22, 'gender': {'type': 'female', 'probability': 0.98}}, {'face_token': 'e66641016cc327aa7e7fb8543ee7116a', 'location': {'left': 400.68, 'top': 249.63, 'width': 62, 'height': 58, 'rotation': -6}, 'face_probability': 1, 'angle': {'yaw': -33.15, 'pitch': 12.93, 'roll': -0.94}, 'age': 22, 'gender': {'type': 'female', 'probability': 1}}]}}



```python
image = Image.open(image_path)
plt.imshow(image)
plt.show()
```


â€‹    ![output_9_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/output_9_0.png)

â€‹    


## æ‰¹é‡è¯†åˆ«ç»“æœå‡½æ•°


```python
# def func_face_batch_detection():
#     """
#     æ‰¹é‡è¯†åˆ«
#     """
#     image_path_list = sorted(os.listdir(image_file_path), key=lambda x: int(x.split('.')[0]))
#     print(image_path_list)
#     print('\n')
    
#     with open(image_face_detection_result_path, mode='a+', encoding='utf-8') as f_w:
#         for image_name in image_path_list:
#             image_id = image_name.split('.')[0]
#             image_path = os.path.join(image_file_path, image_name)
#             print('image_id: {}, image_path: {}'.format(image_id, image_path))
#             try:
#                 image_res = func_face_get_face_detection_result(image_path, access_token)
#             except Exception as e:
#                 print(type(e).__name__)
#                 image_res = {}
#             print('image_res: {}'.format(image_res))
#             f_w.write(image_id + '\t' + json.dumps(image_res) + '\n')
#             print('\n')
#             time.sleep(5)
```


```python
# func_face_batch_detection()
```

## è¯†åˆ«ç»“æœå¯è§†åŒ–


```python
# with open(image_face_detection_result_path, mode='r', encoding='utf-8') as f:
#     for line in f:
#         line = line.strip()
#         image_id, image_res = line.split('\t')
#         image_res = json.loads(image_res)
#         image_path = os.path.join(image_file_path, image_id+'.jpg')
#         print('image_path: {} \n'.format(image_path))
#         # if image_id == '5':
#         #     break
        
#         # # å›¾ç‰‡å¯è§†åŒ–
#         try:
#             image = np.array(plt.imread(image_path))
#             # print(image_id, image.shape)
#             # plt.figure(figsize=(9, 9))
#             # plt.imshow(image)

#             result = image_res.get('result', {})
#             face_list = result.get('face_list', [])
#             location_list = []
#             for i in face_list:
#                 location_list.append(i.get('location', {}))
#             # print(location_list)
#             # print(len(location_list))

#             # ç”»å‡ºæ£€æµ‹åˆ°çš„æ–‡æœ¬æ¡†
#             plt.figure(figsize=(9, 9))
#             plt.imshow(image)
#             for box in location_list:
#                 top = box.get('top', 0)  # è¡¨ç¤ºå®šä½ä½ç½®çš„é•¿æ–¹å½¢å·¦ä¸Šé¡¶ç‚¹çš„å‚ç›´åæ ‡
#                 left = box.get('left', 0)  # è¡¨ç¤ºå®šä½ä½ç½®çš„é•¿æ–¹å½¢å·¦ä¸Šé¡¶ç‚¹çš„æ°´å¹³åæ ‡
#                 width = box.get('width', 0)  # è¡¨ç¤ºå®šä½ä½ç½®çš„é•¿æ–¹å½¢çš„å®½åº¦
#                 height = box.get('height', 0)  # è¡¨ç¤ºå®šä½ä½ç½®çš„é•¿æ–¹å½¢çš„é«˜åº¦
#                 rotation = box.get('rotation', 0)  # äººè„¸æ¡†ç›¸å¯¹äºç«–ç›´æ–¹å‘çš„é¡ºæ—¶é’ˆæ—‹è½¬è§’ï¼Œ[-180,180]
#                 plt.plot([left, left+width], [top, top], 'r', linewidth=1.5)
#                 plt.plot([left+width, left+width], [top, top+height], 'r', linewidth=1.5)
#                 plt.plot([left+width, left], [top+height, top+height], 'r', linewidth=1.5)
#                 plt.plot([left, left], [top+height, top], 'r', linewidth=1.5)
#             plt.savefig(os.path.join(image_face_detection_dir_result_path, image_id+'.jpg'))
#             plt.show()
#         except Exception as e:
#             print(type(e).__name__)
#             command = 'cp %s %s' % (os.path.join(image_file_path, image_id+'.jpg'), os.path.join(image_face_detection_dir_result_path, image_id+'.jpg'))
#             if os.system(command) != 0:
#                 print('cp error!')
#         # break
```

# äººè„¸æ£€æµ‹By [face_recognition](https://github.com/ageitgey/face_recognition)

## ä»å›¾ç‰‡ä¸­è¯†åˆ«äººè„¸ä½ç½®


```python
from PIL import Image
import face_recognition

%matplotlib inline
```


```python
image_path = './datasets/image/biden.jpg'
image = face_recognition.load_image_file(image_path)
```


```python
%%time
# face_locations = face_recognition.face_locations(image)
face_locations = face_recognition.face_locations(image, number_of_times_to_upsample=0, model='cnn')

print('found {} face(s) in this photograph.'.format(len(face_locations)))
```

    found 1 face(s) in this photograph.
    CPU times: user 1.06 s, sys: 786 ms, total: 1.85 s
    Wall time: 8.09 s



```python
for face_location in face_locations:
    top, right, bottom, left = face_location
    print('a face is located at pixel location Top:{}, Left:{}, Bottom:{}, Right:{}'.format(top, left, bottom, right))
    
    face_image = image[top:bottom, left:right]
    pil_image = Image.fromarray(face_image)
    pil_image.show()
```

    a face is located at pixel location Top:235, Left:428, Bottom:518, Right:712




![output_20_1](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/output_20_1.png)
    


## ä»è§†é¢‘ä¸­è¯†åˆ«å‡ºäººè„¸ä½ç½®å’Œå…¶åå­—


```python
import cv2
import face_recognition
```


```python
# æ‰“å¼€è§†é¢‘
# input_video_path = './datasets/video/hamilton_clip.mp4'
# input_video_path = './datasets/video/song.mp4'
input_video_path = './datasets/video/laowang.mp4'
output_video_path = './datasets/video/%s_output.avi' % input_video_path.rsplit('/', 1)[1].split('.')[0]
output_video_path
```




    './datasets/video/laowang_output.avi'




```python
input_video = cv2.VideoCapture(input_video_path)
frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))  # è§†é¢‘å¸§æ•°
frame_count
```




    1456




```python
frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è§†é¢‘é«˜åº¦
frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))  # è§†é¢‘å®½åº¦
frame_rate = input_video.get(cv2.CAP_PROP_FPS)  # å¸§é€Ÿç‡
frame_height, frame_width, frame_rate
```




    (1280, 720, 25.0)




```python
# åˆ›å»ºè¾“å‡ºç”µå½±æ–‡ä»¶ï¼ˆç¡®ä¿åˆ†è¾¨ç‡/å¸§é€Ÿç‡åŒ¹é…è¾“å…¥è§†é¢‘ï¼ï¼‰
# VideoWriter_fourccä¸ºè§†é¢‘ç¼–è§£ç å™¨
# fourcc = cv2.VideoWriter_fourcc(*'XVID')
fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # ,è¯¥å‚æ•°æ˜¯MPEG-4ç¼–ç ç±»å‹ï¼Œæ–‡ä»¶ååç¼€ä¸º.avi
# 29.97ä¸ºå¸§æ’­æ”¾é€Ÿç‡ï¼Œï¼ˆ640ï¼Œ360ï¼‰ä¸ºè§†é¢‘å¸§å¤§å°
# output_video = cv2.VideoWriter(output_video_path, fourcc, 29.97, (640, 360))
output_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))
```


```python
# åŠ è½½ä¸€äº›ç¤ºä¾‹å›¾ç‰‡å¹¶å­¦ä¹ å¦‚ä½•è¯†åˆ«å®ƒä»¬
lmm_image = face_recognition.load_image_file('./datasets/image/lin-manuel-miranda.png')
lmm_face_encodings = face_recognition.face_encodings(lmm_image)[0]
```


```python
al_image = face_recognition.load_image_file('./datasets/image/alex-lacamoire.png')
al_face_encodings = face_recognition.face_encodings(al_image)[0]
```


```python
known_faces = [lmm_face_encodings, al_face_encodings]
known_names = ['Lin-Manuel Miranda', 'Alex Lacamoire']
```


```python
# åˆå§‹åŒ–ä¸€äº›å˜é‡
face_locations = []
face_encodings = []
face_names = []
frame_number = 0
```


```python
%%time
while True:
    # Grab a single frame of video
    ret, frame = input_video.read()
    frame_number += 1
    
    # quit when the input video file ends
    if not ret:
        break
    
    # å°†å›¾åƒä»BGRé¢œè‰²(OpenCVä½¿ç”¨çš„)è½¬æ¢ä¸ºRGBé¢œè‰²(äººè„¸è¯†åˆ«ä½¿ç”¨çš„)
    rgb_frame = frame[:, :, ::-1]
    
    # æ‰¾å‡ºå½“å‰è§†é¢‘å¸§ä¸­æ‰€æœ‰çš„äººè„¸å’Œäººè„¸ç¼–ç 
    face_locations = face_recognition.face_locations(rgb_frame)
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
    
    face_names = []
    for face_encoding in face_encodings:
        # çœ‹çœ‹è¿™å¼ è„¸å’Œå·²çŸ¥çš„è„¸æ˜¯å¦åŒ¹é…
        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)
        
        name = 'unknown'
        if match[0]:
            name = known_names[0]
        elif match[1]:
            name = known_names[1]
        face_names.append(name)
    
    # label the results
    for (top, right, bottom, left), name in zip(face_locations, face_names):
        # draw a box around the face
        # (0,0,255)å¯¹åº”é¢œè‰²(BGR)ï¼Œ2å¯¹åº”çº¿ç²—ç»†
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        font = cv2.FONT_HERSHEY_DUPLEX  # æ­£å¸¸å¤§å°æ— è¡¬çº¿å­—ä½“
        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)
    
    if frame_number % 100 == 0:
        print('writing frame {} / {}'.format(frame_number, frame_count))
    output_video.write(frame)
```

    writing frame 100 / 1456
    writing frame 200 / 1456
    writing frame 300 / 1456
    writing frame 400 / 1456
    writing frame 500 / 1456
    writing frame 600 / 1456
    writing frame 700 / 1456
    writing frame 800 / 1456
    writing frame 900 / 1456
    writing frame 1000 / 1456
    writing frame 1100 / 1456
    writing frame 1200 / 1456
    writing frame 1300 / 1456
    writing frame 1400 / 1456
    CPU times: user 13min 21s, sys: 2.47 s, total: 13min 23s
    Wall time: 13min 13s



```python
# all done
input_video.release()  # é‡Šæ”¾è§†é¢‘æµ
cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰çª—å£
```

## ä»æ‰¹é‡frameä¸­è¯†åˆ«äººè„¸ä½ç½®


```python
# import cv2
# import face_recognition

# input_video_path = './datasets/video/hamilton_clip.mp4'
# input_video = cv2.VideoCapture(input_video_path)

# frames = []
# frame_number = 0
# batch_size = 64

# while input_video.isOpened():
#     ret, frame = input_video.read()
    
#     if not ret:
#         break
        
#     frame = frame[:, :, ::-1]
    
#     frame_number += 1
#     frames.append(frame)
    
#     if len(frames) == batch_size:
#         batch_of_face_locations = face_recognition.batch_face_locations(frames, number_of_times_to_upsample=1, batch_size=batch_size)
    
#         for frame_number_in_batch, face_locations in enumerate(batch_of_face_locations):
#             number_of_faces_in_frame = len(face_locations)
            
#             if number_of_faces_in_frame > 0:
#                 frame_idx = frame_number - batch_size + frame_number_in_batch
#                 print('found {} face(s) in frame #{}.'.format(number_of_faces_in_frame, frame_idx))

#                 for face_location in face_locations:
#                     top, right, bottom, left = face_location
#                     print('- a face is located at pixel location top:{}, left:{}, bottom:{}, right:{}'.format(top, left, bottom, right))

#         frames = []
```

## ä»å›¾ç‰‡ä¸­è¯†åˆ«äººè„¸ä½ç½®å¹¶ç”»å‡ºæ¡†


```python
import face_recognition
from PIL import Image, ImageDraw
import numpy as np
```


```python
obama_image = face_recognition.load_image_file('./datasets/image/obama.jpg')
obama_face_encoding = face_recognition.face_encodings(obama_image)[0]
# è¿™é‡Œæœ‰å¯èƒ½ä¼šå‡ºç°æŠ¥é”™ï¼Œå¯èƒ½æ˜¯ä½ tensorflowçš„è¿›ç¨‹å ç”¨äº†GPUï¼Œå¯¼è‡´è¿™è¾¹è°ƒç”¨ä¸äº†ï¼Œ
# å°†æ‰€æœ‰kernelé‡ç½®å†è¿è¡Œè¯¥ä»£ç å°±å¥½äº†
```


```python
biden_image = face_recognition.load_image_file('./datasets/image/biden.jpg')
biden_face_encoding = face_recognition.face_encodings(biden_image)[0]
```


```python
known_face_encodings = [
    obama_face_encoding,
    biden_face_encoding
]

known_face_names = [
    "Barack Obama",
    "Joe Biden"
]
```


```python
unknown_image = face_recognition.load_image_file('./datasets/image/two_people.jpg')
face_locations = face_recognition.face_locations(unknown_image, number_of_times_to_upsample=1, model='cnn')
face_encodings = face_recognition.face_encodings(unknown_image, known_face_locations=face_locations, num_jitters=1, model='small')
```


```python
pil_image = Image.fromarray(unknown_image)
draw = ImageDraw.Draw(pil_image)
```


```python
for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
    matches = face_recognition.compare_faces(known_face_encodings=known_face_encodings, face_encoding_to_check=face_encoding, tolerance=0.6)
    
    name = 'unknown'
    
    face_distances = face_recognition.face_distance(face_encodings=known_face_encodings, face_to_compare=face_encoding)
    best_match_index = np.argmin(face_distances)
    if matches[best_match_index]:
        name = known_face_names[best_match_index]
    
    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))
    
    text_width, text_height = draw.textsize(name)
    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))
    
del draw

pil_image.show()

# pil_image.save(output_image_path)
```


![output_42_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/output_42_0.png)
    


# äººè„¸æ€§åˆ«å¹´é¾„è¯†åˆ«

* [age-gender-estimation](https://github.com/yu4u/age-gender-estimation)
* [age-and-gender](https://github.com/mowshon/age-and-gender)
* [Age and Gender Prediction From Face Images Using Attentional Convolutional Network](https://arxiv.org/abs/2010.03791)

## è¯†åˆ«ç¤ºä¾‹ By  [age-and-gender](https://github.com/mowshon/age-and-gender)


```python
from age_and_gender import *
from PIL import Image, ImageDraw, ImageFont
```


```python
landmarks_model_path = './datasets/model/age_and_gender/shape_predictor_5_face_landmarks.dat'
gender_model_path = './datasets/model/age_and_gender/dnn_gender_classifier_v1.dat'
age_model_path = './datasets/model/age_and_gender/dnn_age_predictor_v1.dat'

input_image_path = './datasets/image/face_detection.jpg'
input_image_path = './datasets/image/two_people.jpg'
```


```python
age_gender_model = AgeAndGender()
age_gender_model.load_shape_predictor(landmarks_model_path)
age_gender_model.load_dnn_age_predictor(age_model_path)
age_gender_model.load_dnn_gender_classifier(gender_model_path)
```


```python
image = Image.open(input_image_path).convert('RGB')
rec_result = age_gender_model.predict(image)
rec_result
```




    [{'gender': {'value': 'male', 'confidence': 99},
      'age': {'value': 55, 'confidence': 60},
      'face': [244, 62, 394, 211]},
     {'gender': {'value': 'male', 'confidence': 99},
      'age': {'value': 70, 'confidence': 72},
      'face': [792, 95, 941, 244]}]




```python
draw = ImageDraw.Draw(image)
for info in rec_result:
    gender = info.get('gender', {}).get('value', 'unknown')
    gender_confidence = info.get('gender', {}).get('confidence', 100)
    age = info.get('age', {}).get('value', 999)
    age_confidence = info.get('age', {}).get('confidence', 100)
    face_location = info.get('face', [])
    
    left, top, right, bottom = face_location[0], face_location[1], face_location[2], face_location[3]
    
    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))
    draw.text((left - 10, bottom + 10), f"{gender} (~{gender_confidence}%)\n{age} (~{age_confidence}%).", fill=(255, 255, 255, 255), align='center')

del draw
image.show()
```


â€‹    
![output_49_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/output_49_0.png)
â€‹    


## è¯†åˆ«ç¤ºä¾‹ By [face_recognition](https://github.com/ageitgey/face_recognition) å’Œ [age-and-gender](https://github.com/mowshon/age-and-gender)

### å›¾ç‰‡


```python
from age_and_gender import *
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import face_recognition
```


```python
landmarks_model_path = './datasets/model/age_and_gender/shape_predictor_5_face_landmarks.dat'
gender_model_path = './datasets/model/age_and_gender/dnn_gender_classifier_v1.dat'
age_model_path = './datasets/model/age_and_gender/dnn_age_predictor_v1.dat'

input_image_path = './datasets/image/face_detection.jpg'
input_image_path = './datasets/image/two_people.jpg'
```


```python
age_gender_model = AgeAndGender()
age_gender_model.load_shape_predictor(landmarks_model_path)
age_gender_model.load_dnn_age_predictor(age_model_path)
age_gender_model.load_dnn_gender_classifier(gender_model_path)
```


```python
image = Image.open(input_image_path).convert('RGB')
face_locations = face_recognition.face_locations(np.asarray(image), model='hog')
```


```python
rec_result = age_gender_model.predict(image, face_locations)
rec_result
```




    [{'gender': {'value': 'male', 'confidence': 99},
      'age': {'value': 71, 'confidence': 61},
      'face': [778, 57, 964, 242]},
     {'gender': {'value': 'male', 'confidence': 99},
      'age': {'value': 55, 'confidence': 51},
      'face': [253, 47, 408, 202]}]




```python
draw = ImageDraw.Draw(image)
for info in rec_result:
    gender = info.get('gender', {}).get('value', 'unknown')
    gender_confidence = info.get('gender', {}).get('confidence', 100)
    age = info.get('age', {}).get('value', 999)
    age_confidence = info.get('age', {}).get('confidence', 100)
    face_location = info.get('face', [])
    
    left, top, right, bottom = face_location[0], face_location[1], face_location[2], face_location[3]
    
    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))
    draw.text((left - 10, bottom + 10), f"{gender} (~{gender_confidence}%)\n{age} (~{age_confidence}%).", fill=(255, 255, 255, 255), align='center')

del draw
image.show()
```


![output_57_0](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/output_57_0.png)
    


### è§†é¢‘


```python
import cv2
from age_and_gender import *
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import face_recognition
```


```python
landmarks_model_path = './datasets/model/age_and_gender/shape_predictor_5_face_landmarks.dat'
gender_model_path = './datasets/model/age_and_gender/dnn_gender_classifier_v1.dat'
age_model_path = './datasets/model/age_and_gender/dnn_age_predictor_v1.dat'

# input_video_path = './datasets/video/hamilton_clip.mp4'
# input_video_path = './datasets/video/song.mp4'
input_video_path = './datasets/video/laowang.mp4'
output_video_path = './datasets/video/%s_output.avi' % input_video_path.rsplit('/', 1)[1].split('.')[0]
output_video_path
```




    './datasets/video/laowang_output.avi'




```python
# æ‰“å¼€æ€§åˆ«å¹´é¾„æ¨¡å‹
age_gender_model = AgeAndGender()
age_gender_model.load_shape_predictor(landmarks_model_path)
age_gender_model.load_dnn_age_predictor(age_model_path)
age_gender_model.load_dnn_gender_classifier(gender_model_path)
```


```python
# æ‰“å¼€è§†é¢‘
input_video = cv2.VideoCapture(input_video_path)
frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))  # è§†é¢‘å¸§æ•°
frame_count
```




    1456




```python
frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è§†é¢‘é«˜åº¦
frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))  # è§†é¢‘å®½åº¦
frame_rate = input_video.get(cv2.CAP_PROP_FPS)  # å¸§é€Ÿç‡
frame_height, frame_width, frame_rate
```




    (1280, 720, 25.0)




```python
# åˆ›å»ºè¾“å‡ºç”µå½±æ–‡ä»¶ï¼ˆç¡®ä¿åˆ†è¾¨ç‡/å¸§é€Ÿç‡åŒ¹é…è¾“å…¥è§†é¢‘ï¼ï¼‰
# VideoWriter_fourccä¸ºè§†é¢‘ç¼–è§£ç å™¨
# fourcc = cv2.VideoWriter_fourcc(*'XVID')
fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # ,è¯¥å‚æ•°æ˜¯MPEG-4ç¼–ç ç±»å‹ï¼Œæ–‡ä»¶ååç¼€ä¸º.avi
# 29.97ä¸ºå¸§æ’­æ”¾é€Ÿç‡ï¼Œï¼ˆ640ï¼Œ360ï¼‰ä¸ºè§†é¢‘å¸§å¤§å°
# output_video = cv2.VideoWriter(output_video_path, fourcc, 29.97, (640, 360))
output_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))
```


```python
# åˆå§‹åŒ–ä¸€äº›å˜é‡
frame_number = 0
```


```python
%%time
while True:
    # Grab a single frame of video
    ret, frame = input_video.read()
    frame_number += 1
    
    # quit when the input video file ends
    if not ret:
        break
    
    # å°†å›¾åƒä»BGRé¢œè‰²(OpenCVä½¿ç”¨çš„)è½¬æ¢ä¸ºRGBé¢œè‰²(äººè„¸è¯†åˆ«ä½¿ç”¨çš„)
    rgb_frame = frame[:, :, ::-1]
    
    # æ‰¾å‡ºå½“å‰è§†é¢‘å¸§ä¸­æ‰€æœ‰çš„äººè„¸å’Œäººè„¸ç¼–ç 
    face_locations = face_recognition.face_locations(rgb_frame, model='cnn')
    rec_result = age_gender_model.predict(rgb_frame, face_locations)
    
    for info in rec_result:
        gender = info.get('gender', {}).get('value', 'unknown')
        gender_confidence = info.get('gender', {}).get('confidence', 100)
        age = info.get('age', {}).get('value', 999)
        age_confidence = info.get('age', {}).get('confidence', 100)
        face_location = info.get('face', [])
        
        left, top, right, bottom = face_location[0], face_location[1], face_location[2], face_location[3]
        # draw a box around the face
        # (0,0,255)å¯¹åº”é¢œè‰²(BGR)ï¼Œ2å¯¹åº”çº¿ç²—ç»†
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        font = cv2.FONT_HERSHEY_DUPLEX  # æ­£å¸¸å¤§å°æ— è¡¬çº¿å­—ä½“
        name = f"{gender} (~{gender_confidence}%)\n {age} (~{age_confidence}%)."
        cv2.putText(frame, name, (left - 6, bottom + 10), font, 0.5, (255, 255, 255), 1)
    
    if frame_number % 100 == 0:
        print('writing frame {} / {}'.format(frame_number, frame_count))
    output_video.write(frame)
```

    writing frame 100 / 1456
    writing frame 200 / 1456
    writing frame 300 / 1456
    writing frame 400 / 1456
    writing frame 500 / 1456
    writing frame 600 / 1456
    writing frame 700 / 1456
    writing frame 800 / 1456
    writing frame 900 / 1456
    writing frame 1000 / 1456
    writing frame 1100 / 1456
    writing frame 1200 / 1456
    writing frame 1300 / 1456
    writing frame 1400 / 1456
    CPU times: user 7min 54s, sys: 25.1 s, total: 8min 19s
    Wall time: 8min 8s



```python
# all done
input_video.release()  # é‡Šæ”¾è§†é¢‘æµ
cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰çª—å£
```

## è¯†åˆ«ç¤ºä¾‹ By [deepface](https://github.com/serengil/deepface) <font color='red'>æ¨è</font>

### è§†é¢‘

* å…ˆface_recoginition
* å†deepface


```python
import cv2
from age_and_gender import *
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import face_recognition
from deepface import DeepFace
```


```python
# input_video_path = './datasets/video/hamilton_clip.mp4'
# input_video_path = './datasets/video/song.mp4'
input_video_path = './datasets/video/laowang.mp4'
output_video_path = './datasets/video/%s_output.avi' % input_video_path.rsplit('/', 1)[1].split('.')[0]
output_video_path
```




    './datasets/video/laowang_output.avi'




```python
# æ‰“å¼€è§†é¢‘
input_video = cv2.VideoCapture(input_video_path)
frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))  # è§†é¢‘å¸§æ•°
frame_count
```




    1456




```python
frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è§†é¢‘é«˜åº¦
frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))  # è§†é¢‘å®½åº¦
frame_rate = input_video.get(cv2.CAP_PROP_FPS)  # å¸§é€Ÿç‡
frame_height, frame_width, frame_rate
```




    (1280, 720, 25.0)




```python
# åˆ›å»ºè¾“å‡ºç”µå½±æ–‡ä»¶ï¼ˆç¡®ä¿åˆ†è¾¨ç‡/å¸§é€Ÿç‡åŒ¹é…è¾“å…¥è§†é¢‘ï¼ï¼‰
# VideoWriter_fourccä¸ºè§†é¢‘ç¼–è§£ç å™¨
# fourcc = cv2.VideoWriter_fourcc(*'XVID')
fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # ,è¯¥å‚æ•°æ˜¯MPEG-4ç¼–ç ç±»å‹ï¼Œæ–‡ä»¶ååç¼€ä¸º.avi
# 29.97ä¸ºå¸§æ’­æ”¾é€Ÿç‡ï¼Œï¼ˆ640ï¼Œ360ï¼‰ä¸ºè§†é¢‘å¸§å¤§å°
# output_video = cv2.VideoWriter(output_video_path, fourcc, 29.97, (640, 360))
output_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))
```


```python
# åˆå§‹åŒ–ä¸€äº›å˜é‡
frame_number = -1
```


```python
%%time
while True:
    # Grab a single frame of video
    ret, frame = input_video.read()
    frame_number += 1
    
    # quit when the input video file ends
    if not ret:
        break
    
    # å°†å›¾åƒä»BGRé¢œè‰²(OpenCVä½¿ç”¨çš„)è½¬æ¢ä¸ºRGBé¢œè‰²(äººè„¸è¯†åˆ«ä½¿ç”¨çš„)
    rgb_frame = frame[:, :, ::-1]
    
    # æ‰¾å‡ºå½“å‰è§†é¢‘å¸§ä¸­æ‰€æœ‰çš„äººè„¸å’Œäººè„¸ç¼–ç 
    face_locations = face_recognition.face_locations(rgb_frame, model='cnn')
    
    for top, right, bottom, left in face_locations:
        image = rgb_frame[left:right, top:bottom]
        try:
            obj = DeepFace.analyze(img_path = image,
                                   actions = ['age', 'gender'],#, 'race', 'emotion'], 
                                   enforce_detection=False,
                                   detector_backend='dlib',
                                   prog_bar=False)
        except:
            obj = {}
        age = obj.get('age', 'unknown')
        gender = obj.get('gender', 'unknown')
        # draw a box around the face
        # (0,0,255)å¯¹åº”é¢œè‰²(BGR)ï¼Œ2å¯¹åº”çº¿ç²—ç»†
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        font = cv2.FONT_HERSHEY_DUPLEX  # æ­£å¸¸å¤§å°æ— è¡¬çº¿å­—ä½“
        name = f"gender: {gender}, age: {age}."
        cv2.putText(frame, name, (left - 6, bottom + 10), font, 0.5, (255, 255, 255), 1)
    
    if frame_number % 100 == 0:
        print('writing frame {} / {}'.format(frame_number, frame_count))
    output_video.write(frame)
```

    writing frame 0 / 1456
    writing frame 100 / 1456
    writing frame 200 / 1456
    writing frame 300 / 1456
    writing frame 400 / 1456
    writing frame 500 / 1456
    writing frame 600 / 1456
    writing frame 700 / 1456
    writing frame 800 / 1456
    writing frame 900 / 1456
    writing frame 1000 / 1456
    writing frame 1100 / 1456
    writing frame 1200 / 1456
    writing frame 1300 / 1456
    writing frame 1400 / 1456
    CPU times: user 10min 32s, sys: 2min 19s, total: 12min 51s
    Wall time: 11min 21s


### è§†é¢‘

* ä¸ç”¨å…ˆface_recoginition
* ç›´æ¥æ£€æµ‹+age_genderä¸€èµ·ä¸Š


```python
# import cv2
# from age_and_gender import *
# from PIL import Image, ImageDraw, ImageFont
# import numpy as np
# import face_recognition
# from deepface import DeepFace
```


```python
# # input_video_path = './datasets/video/hamilton_clip.mp4'
# # input_video_path = './datasets/video/song.mp4'
# input_video_path = './datasets/video/laowang.mp4'
# output_video_path = './datasets/video/%s_output.avi' % input_video_path.rsplit('/', 1)[1].split('.')[0]
# output_video_path
```


```python
# # æ‰“å¼€è§†é¢‘
# input_video = cv2.VideoCapture(input_video_path)
# frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))  # è§†é¢‘å¸§æ•°
# frame_count
```


```python
# frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è§†é¢‘é«˜åº¦
# frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))  # è§†é¢‘å®½åº¦
# frame_rate = input_video.get(cv2.CAP_PROP_FPS)  # å¸§é€Ÿç‡
# frame_height, frame_width, frame_rate
```


```python
# # åˆ›å»ºè¾“å‡ºç”µå½±æ–‡ä»¶ï¼ˆç¡®ä¿åˆ†è¾¨ç‡/å¸§é€Ÿç‡åŒ¹é…è¾“å…¥è§†é¢‘ï¼ï¼‰
# # VideoWriter_fourccä¸ºè§†é¢‘ç¼–è§£ç å™¨
# # fourcc = cv2.VideoWriter_fourcc(*'XVID')
# fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # ,è¯¥å‚æ•°æ˜¯MPEG-4ç¼–ç ç±»å‹ï¼Œæ–‡ä»¶ååç¼€ä¸º.avi
# # 29.97ä¸ºå¸§æ’­æ”¾é€Ÿç‡ï¼Œï¼ˆ640ï¼Œ360ï¼‰ä¸ºè§†é¢‘å¸§å¤§å°
# # output_video = cv2.VideoWriter(output_video_path, fourcc, 29.97, (640, 360))
# output_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))
```


```python
# # åˆå§‹åŒ–ä¸€äº›å˜é‡
# frame_number = -1
```


```python
# %%time
# while True:
#     # Grab a single frame of video
#     ret, frame = input_video.read()
#     frame_number += 1
    
#     # quit when the input video file ends
#     if not ret:
#         break
    
#     # å°†å›¾åƒä»BGRé¢œè‰²(OpenCVä½¿ç”¨çš„)è½¬æ¢ä¸ºRGBé¢œè‰²(äººè„¸è¯†åˆ«ä½¿ç”¨çš„)
#     rgb_frame = frame[:, :, ::-1]
    
#     # æ‰¾å‡ºå½“å‰è§†é¢‘å¸§ä¸­æ‰€æœ‰çš„äººè„¸å’Œäººè„¸ç¼–ç 
#     obj = DeepFace.analyze(img_path = rgb_frame, actions = ['age', 'gender'], enforce_detection=False, detector_backend='dlib', prog_bar=False)
    
#     age = obj.get('age', 'unknown')
#     gender = obj.get('gender', 'unknown')
#     region = obj.get('region', {})
#     if region:
#         x = region.get('x', 0)
#         y = region.get('y', 0)
#         w = region.get('w', 0)
#         h = region.get('h', 0)

#         # draw a box around the face
#         # (0,0,255)å¯¹åº”é¢œè‰²(BGR)ï¼Œ2å¯¹åº”çº¿ç²—ç»†
#         cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)
#         font = cv2.FONT_HERSHEY_DUPLEX  # æ­£å¸¸å¤§å°æ— è¡¬çº¿å­—ä½“
#         name = f"gender: {gender}, age: {age}."
#         cv2.putText(frame, name, (x - 6, y + h + 10), font, 0.5, (255, 255, 255), 1)
    
#     if frame_number % 100 == 0:
#         print('writing frame {} / {}'.format(frame_number, frame_count))
#     output_video.write(frame)
```


```python

```

# äººæ•°è¯†åˆ«


```python
import cv2
import face_recognition
```


```python
# æ‰“å¼€è§†é¢‘
input_video_path = './datasets/video/hamilton_clip.mp4'
# input_video_path = './datasets/video/song.mp4'
# input_video_path = './datasets/video/laowang.mp4'
```


```python
input_video = cv2.VideoCapture(input_video_path)
frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))  # è§†é¢‘å¸§æ•°
frame_count
```




    2356




```python
frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è§†é¢‘é«˜åº¦
frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))  # è§†é¢‘å®½åº¦
frame_rate = input_video.get(cv2.CAP_PROP_FPS)  # å¸§é€Ÿç‡
frame_height, frame_width, frame_rate
```




    (360, 640, 29.97)




```python
known_faces = []
```


```python
# åˆå§‹åŒ–ä¸€äº›å˜é‡
face_locations = []
face_encodings = []
frame_number = 0
face_count = 0
```


```python
%%time
while True:
    # Grab a single frame of video
    ret, frame = input_video.read()
    frame_number += 1
    
    # quit when the input video file ends
    if not ret:
        break
    
    # å°†å›¾åƒä»BGRé¢œè‰²(OpenCVä½¿ç”¨çš„)è½¬æ¢ä¸ºRGBé¢œè‰²(äººè„¸è¯†åˆ«ä½¿ç”¨çš„)
    rgb_frame = frame[:, :, ::-1]
    
    # æ‰¾å‡ºå½“å‰è§†é¢‘å¸§ä¸­æ‰€æœ‰çš„äººè„¸å’Œäººè„¸ç¼–ç 
    face_locations = face_recognition.face_locations(rgb_frame, model='cnn')
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
    
    for face_encoding in face_encodings:
        # çœ‹çœ‹è¿™å¼ è„¸å’Œå·²çŸ¥çš„è„¸æ˜¯å¦åŒ¹é…
        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.6)
        
        if any(match) :
            pass
        else:
            face_count += 1
        known_faces.append(face_encoding)
    
    if frame_number % 100 == 0:
        # print(match)
        print('processing frame {} / {}'.format(frame_number, frame_count))
```

    processing frame 100 / 2356
    processing frame 200 / 2356
    processing frame 300 / 2356
    processing frame 400 / 2356
    processing frame 500 / 2356
    processing frame 600 / 2356
    processing frame 700 / 2356
    processing frame 800 / 2356
    processing frame 900 / 2356
    processing frame 1000 / 2356
    processing frame 1100 / 2356
    processing frame 1200 / 2356
    processing frame 1300 / 2356
    processing frame 1400 / 2356
    processing frame 1500 / 2356
    processing frame 1600 / 2356
    processing frame 1700 / 2356
    processing frame 1800 / 2356
    processing frame 1900 / 2356
    processing frame 2000 / 2356
    processing frame 2100 / 2356
    processing frame 2200 / 2356
    processing frame 2300 / 2356
    CPU times: user 1min 57s, sys: 17 s, total: 2min 14s
    Wall time: 2min 11s



```python
face_count
```




    2




```python
from IPython.display import Video
```


```python
Video(input_video_path)
```




<video src="./datasets/video/hamilton_clip.mp4" controls  >
      Your browser does not support the <code>video</code> element.
    </video>



# æ•´åˆï¼šäººæ•°+æ€§åˆ«+å¹´é¾„è¯†åˆ«


```python
import cv2
from age_and_gender import *
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import face_recognition
```


```python
landmarks_model_path = './datasets/model/age_and_gender/shape_predictor_5_face_landmarks.dat'
gender_model_path = './datasets/model/age_and_gender/dnn_gender_classifier_v1.dat'
age_model_path = './datasets/model/age_and_gender/dnn_age_predictor_v1.dat'

input_video_path = './datasets/video/hamilton_clip.mp4'
# input_video_path = './datasets/video/song.mp4'
# input_video_path = './datasets/video/laowang.mp4'
output_video_path = './datasets/video/%s_output.avi' % input_video_path.rsplit('/', 1)[1].split('.')[0]
output_video_path
```




    './datasets/video/hamilton_clip_output.avi'




```python
# æ‰“å¼€æ€§åˆ«å¹´é¾„æ¨¡å‹
age_gender_model = AgeAndGender()
age_gender_model.load_shape_predictor(landmarks_model_path)
age_gender_model.load_dnn_age_predictor(age_model_path)
age_gender_model.load_dnn_gender_classifier(gender_model_path)
```


```python
# æ‰“å¼€è§†é¢‘
input_video = cv2.VideoCapture(input_video_path)
frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))  # è§†é¢‘å¸§æ•°
frame_count
```




    2356




```python
frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è§†é¢‘é«˜åº¦
frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))  # è§†é¢‘å®½åº¦
frame_rate = input_video.get(cv2.CAP_PROP_FPS)  # å¸§é€Ÿç‡
frame_height, frame_width, frame_rate
```




    (360, 640, 29.97)




```python
# åˆ›å»ºè¾“å‡ºç”µå½±æ–‡ä»¶ï¼ˆç¡®ä¿åˆ†è¾¨ç‡/å¸§é€Ÿç‡åŒ¹é…è¾“å…¥è§†é¢‘ï¼ï¼‰
# VideoWriter_fourccä¸ºè§†é¢‘ç¼–è§£ç å™¨
# fourcc = cv2.VideoWriter_fourcc(*'XVID')
fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # ,è¯¥å‚æ•°æ˜¯MPEG-4ç¼–ç ç±»å‹ï¼Œæ–‡ä»¶ååç¼€ä¸º.avi
# 29.97ä¸ºå¸§æ’­æ”¾é€Ÿç‡ï¼Œï¼ˆ640ï¼Œ360ï¼‰ä¸ºè§†é¢‘å¸§å¤§å°
# output_video = cv2.VideoWriter(output_video_path, fourcc, 29.97, (640, 360))
output_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))
```


```python
# åˆå§‹åŒ–ä¸€äº›å˜é‡
frame_number = 0
known_faces = []
face_count = 0
```


```python
%%time
while True:
    # Grab a single frame of video
    ret, frame = input_video.read()
    frame_number += 1
    
    # quit when the input video file ends
    if not ret:
        break
    
    # å°†å›¾åƒä»BGRé¢œè‰²(OpenCVä½¿ç”¨çš„)è½¬æ¢ä¸ºRGBé¢œè‰²(äººè„¸è¯†åˆ«ä½¿ç”¨çš„)
    rgb_frame = frame[:, :, ::-1]
    
    # æ‰¾å‡ºå½“å‰è§†é¢‘å¸§ä¸­æ‰€æœ‰çš„äººè„¸å’Œäººè„¸ç¼–ç 
    face_locations = face_recognition.face_locations(rgb_frame, model='cnn')
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
    
    # äººæ•°è¯†åˆ«
    for face_encoding in face_encodings:
        # çœ‹çœ‹è¿™å¼ è„¸å’Œå·²çŸ¥çš„è„¸æ˜¯å¦åŒ¹é…
        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.6)
        
        if any(match) :
            pass
        else:
            face_count += 1
        known_faces.append(face_encoding)
    
    # å¹´é¾„æ€§åˆ«è¯†åˆ«
    rec_result = age_gender_model.predict(rgb_frame, face_locations)
    
    # ç”»å‡ºæ¡†åŠå±æ€§
    for info in rec_result:
        gender = info.get('gender', {}).get('value', 'unknown')
        gender_confidence = info.get('gender', {}).get('confidence', 100)
        age = info.get('age', {}).get('value', 999)
        age_confidence = info.get('age', {}).get('confidence', 100)
        face_location = info.get('face', [])
        
        left, top, right, bottom = face_location[0], face_location[1], face_location[2], face_location[3]
        # draw a box around the face
        # (0,0,255)å¯¹åº”é¢œè‰²(BGR)ï¼Œ2å¯¹åº”çº¿ç²—ç»†
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        font = cv2.FONT_HERSHEY_DUPLEX  # æ­£å¸¸å¤§å°æ— è¡¬çº¿å­—ä½“
        name = f"gender: {gender} (~{gender_confidence}%)."
        cv2.putText(frame, name, (left - 6, bottom + 50), font, 0.5, (255, 255, 255), 1)
        name = f"age: {age} (~{age_confidence}%)."
        cv2.putText(frame, name, (left - 6, bottom + 30), font, 0.5, (255, 255, 255), 1)
        name = f"face_count: {face_count}."
        cv2.putText(frame, name, (left - 6, bottom + 10), font, 0.5, (255, 255, 255), 1)
    
    if frame_number % 100 == 0:
        print('writing frame {} / {}'.format(frame_number, frame_count))
    output_video.write(frame)
```

    writing frame 100 / 2356
    writing frame 200 / 2356
    writing frame 300 / 2356
    writing frame 400 / 2356
    writing frame 500 / 2356
    writing frame 600 / 2356
    writing frame 700 / 2356
    writing frame 800 / 2356
    writing frame 900 / 2356
    writing frame 1000 / 2356
    writing frame 1100 / 2356
    writing frame 1200 / 2356
    writing frame 1300 / 2356
    writing frame 1400 / 2356
    writing frame 1500 / 2356
    writing frame 1600 / 2356
    writing frame 1700 / 2356
    writing frame 1800 / 2356
    writing frame 1900 / 2356
    writing frame 2000 / 2356
    writing frame 2100 / 2356
    writing frame 2200 / 2356
    writing frame 2300 / 2356
    CPU times: user 7min 34s, sys: 19.3 s, total: 7min 53s
    Wall time: 7min 49s



```python
# all done
input_video.release()  # é‡Šæ”¾è§†é¢‘æµ
cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰çª—å£
```


```python

```
